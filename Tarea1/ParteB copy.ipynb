{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar Modulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import plotly.express as px\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                labels 1  2  3  4  5 \n",
       "0               normal               \n",
       "1               normal               \n",
       "2               normal               \n",
       "3               normal               \n",
       "4               normal               \n",
       "..                 ... .. .. .. .. ..\n",
       "159  collision_in_tool               \n",
       "160  collision_in_tool               \n",
       "161  collision_in_tool               \n",
       "162  collision_in_tool               \n",
       "163  collision_in_tool               \n",
       "\n",
       "[164 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ -2.,  -1.,  81.,   0.,  -5.,   0.],\n",
       "        [ -2.,  -1.,  79.,   0.,  -4.,   0.],\n",
       "        [ -2.,  -1.,  79.,   0.,  -4.,   0.],\n",
       "        ...,\n",
       "        [ -2.,  -1.,  78.,   0.,  -5.,   0.],\n",
       "        [ -3.,  -1.,  80.,   1.,  -4.,   1.],\n",
       "        [ -2.,  -1.,  79.,   0.,  -4.,   0.]],\n",
       "\n",
       "       [[  6.,  -1.,  79.,  -2.,   4.,  -3.],\n",
       "        [ 42.,  -3.,  80.,   5.,  53.,   3.],\n",
       "        [ -5.,   4.,  74., -15., -10.,  -1.],\n",
       "        ...,\n",
       "        [ -1.,  -5.,  80.,   6.,  -6.,   0.],\n",
       "        [ -4.,   5.,  78., -14.,  -9.,  -4.],\n",
       "        [ -4.,   1.,  80.,  -3., -12.,   5.]],\n",
       "\n",
       "       [[ -2.,  -6.,  85.,  14.,  -5.,   2.],\n",
       "        [  0.,   2.,  74.,  -7.,   1.,   0.],\n",
       "        [ -4.,  -5.,  76.,   7., -11.,   4.],\n",
       "        ...,\n",
       "        [  0.,  -9.,  87.,  13.,  -5.,   2.],\n",
       "        [ -5.,   5.,  67., -17., -16.,   7.],\n",
       "        [ -6., -10.,  86.,  16., -14.,  -1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-12.,  17.,   3., -19., -10.,  -4.],\n",
       "        [-12.,  12.,  11., -13., -16.,  -4.],\n",
       "        [ -8.,   3.,   6.,   2., -11.,  -4.],\n",
       "        ...,\n",
       "        [  0.,   1.,   3.,   1.,   1.,  -3.],\n",
       "        [  1.,   2.,   9.,   0.,   3.,  -4.],\n",
       "        [  1.,   2.,   9.,   0.,   3.,  -4.]],\n",
       "\n",
       "       [[-41.,  21.,  -5., -23., -59.,  -4.],\n",
       "        [-32.,  21.,  -6., -25., -45.,  -4.],\n",
       "        [-21.,  12.,  -6., -14., -31.,  -4.],\n",
       "        ...,\n",
       "        [ -4.,   4.,   3.,   0.,  -1.,  -3.],\n",
       "        [ -4.,   3.,   2.,   2.,  -3.,  -3.],\n",
       "        [ -2.,   3.,   5.,   0.,  -2.,  -3.]],\n",
       "\n",
       "       [[  9., -10., -11.,  17.,   7.,  -4.],\n",
       "        [  5.,   0.,   4.,   0.,   7.,  -4.],\n",
       "        [ -3.,   6.,  -2.,  -8.,  -8.,  -4.],\n",
       "        ...,\n",
       "        [ -1.,   1.,  -3.,  -3.,  -2.,  -3.],\n",
       "        [  0.,  -1.,  -5.,  -1.,   1.,  -3.],\n",
       "        [ -1.,   1.,   4.,   0.,  -1.,  -3.]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.3f}\".format\n",
    "\n",
    "data = np.loadtxt(\"lp5.csv\", delimiter=\",\", dtype=str)\n",
    "data_copy = data #va de 0 a 2623 o sea 2624 datos\n",
    "cont = 16\n",
    "labels = []\n",
    "numbers = []\n",
    "features= np.zeros((164,15,6))\n",
    "\n",
    "for i in range(len(data_copy)):\n",
    "    if(cont == 16):\n",
    "        labels.append(data_copy[i])\n",
    "    if(cont<16):\n",
    "        numbers.append(data_copy[i])\n",
    "    cont -= 1\n",
    "    if(cont == 0):\n",
    "        cont = 16\n",
    "        \n",
    "cont = 0\n",
    "for i in range(164):\n",
    "    for j in range(15):\n",
    "        for z in range(6):\n",
    "            features[i][j][z] = numbers[cont][z]\n",
    "        cont += 1\n",
    "        \n",
    "labels_df = pd.DataFrame(labels, columns=['labels','1','2','3','4','5'])\n",
    "#labels_df = labels_df.loc[:,['labels']]\n",
    "display(labels_df)\n",
    "display(features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manejo de Datos y Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dave4\\AppData\\Local\\Temp\\ipykernel_19480\\1747463221.py:8: FutureWarning:\n",
      "\n",
      "In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.77660263, 0.78156702, 0.79084826, 0.77055903, 0.7684006 ,\n",
       "         0.77422836],\n",
       "        [0.77466005, 0.78739478, 0.78717893, 0.75156486, 0.7638679 ,\n",
       "         0.77206993],\n",
       "        [0.77401252, 0.78804231, 0.79041658, 0.74465789, 0.76106195,\n",
       "         0.77012735],\n",
       "        ...,\n",
       "        [0.78005612, 0.7804878 , 0.78437298, 0.78135118, 0.78113533,\n",
       "         0.78027196],\n",
       "        [0.78027196, 0.78091949, 0.77962443, 0.78091949, 0.78027196,\n",
       "         0.78027196],\n",
       "        [0.78027196, 0.78070365, 0.78286208, 0.78156702, 0.78091949,\n",
       "         0.78005612]],\n",
       "\n",
       "       [[0.78027196, 0.77940859, 0.79905029, 0.78372545, 0.77962443,\n",
       "         0.78113533],\n",
       "        [0.78070365, 0.78113533, 0.79667602, 0.77919275, 0.78091949,\n",
       "         0.78070365],\n",
       "        [0.77984028, 0.77962443, 0.79710771, 0.78221455, 0.77832938,\n",
       "         0.78156702],\n",
       "        ...,\n",
       "        [0.78070365, 0.77876106, 0.79948198, 0.78350961, 0.77962443,\n",
       "         0.78113533],\n",
       "        [0.77962443, 0.78178286, 0.79516512, 0.77703432, 0.77725016,\n",
       "         0.78221455],\n",
       "        [0.77940859, 0.77854522, 0.79926613, 0.78415713, 0.77768185,\n",
       "         0.7804878 ]],\n",
       "\n",
       "       [[0.78502051, 0.77725016, 0.78264623, 0.78631556, 0.78696309,\n",
       "         0.7804878 ],\n",
       "        [0.78588388, 0.78005612, 0.78717893, 0.78243039, 0.78890568,\n",
       "         0.77984028],\n",
       "        [0.78653141, 0.77876106, 0.78027196, 0.78480466, 0.78933736,\n",
       "         0.78027196],\n",
       "        ...,\n",
       "        [0.78005612, 0.78135118, 0.77962443, 0.7804878 , 0.77962443,\n",
       "         0.77962443],\n",
       "        [0.78005612, 0.78091949, 0.7789769 , 0.78091949, 0.78027196,\n",
       "         0.77962443],\n",
       "        [0.7804878 , 0.78091949, 0.78005612, 0.78156702, 0.78070365,\n",
       "         0.77962443]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.78005612, 0.78070365, 0.79689186, 0.77919275, 0.77660263,\n",
       "         0.77984028],\n",
       "        [0.77962443, 0.77832938, 0.79883445, 0.78307792, 0.77617095,\n",
       "         0.77811353],\n",
       "        [0.78005612, 0.78264623, 0.79516512, 0.77487589, 0.77681848,\n",
       "         0.7789769 ],\n",
       "        ...,\n",
       "        [0.7804878 , 0.77832938, 0.79969782, 0.78307792, 0.777466  ,\n",
       "         0.77854522],\n",
       "        [0.77940859, 0.77984028, 0.79710771, 0.77962443, 0.77573926,\n",
       "         0.77984028],\n",
       "        [0.77940859, 0.78243039, 0.79710771, 0.77530758, 0.77617095,\n",
       "         0.77940859]],\n",
       "\n",
       "       [[0.77077488, 0.79494928, 0.77617095, 0.75588172, 0.77832938,\n",
       "         0.7804878 ],\n",
       "        [0.7653788 , 0.78135118, 0.78243039, 0.777466  , 0.7638679 ,\n",
       "         0.78027196],\n",
       "        [0.76365206, 0.78264623, 0.78350961, 0.78005612, 0.75674509,\n",
       "         0.78027196],\n",
       "        ...,\n",
       "        [0.77984028, 0.78113533, 0.78243039, 0.78113533, 0.78005612,\n",
       "         0.78005612],\n",
       "        [0.78027196, 0.78113533, 0.78350961, 0.78091949, 0.78070365,\n",
       "         0.78027196],\n",
       "        [0.7804878 , 0.78091949, 0.78480466, 0.78221455, 0.78156702,\n",
       "         0.78005612]],\n",
       "\n",
       "       [[0.77789769, 0.77789769, 0.7744442 , 0.78696309, 0.78653141,\n",
       "         0.77789769],\n",
       "        [0.77832938, 0.77811353, 0.77422836, 0.78631556, 0.78868983,\n",
       "         0.7789769 ],\n",
       "        [0.77832938, 0.77789769, 0.77466005, 0.78696309, 0.78804231,\n",
       "         0.77768185],\n",
       "        ...,\n",
       "        [0.67882581, 0.70472696, 0.09842435, 0.7759551 , 0.85732787,\n",
       "         0.74940643],\n",
       "        [0.69933089, 0.73796676, 0.34448521, 0.77099072, 0.81674941,\n",
       "         0.77530758],\n",
       "        [0.72199439, 0.73170732, 0.7155191 , 0.81804446, 0.69156054,\n",
       "         0.77012735]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1  2  3  4  5\n",
       "84   0  0  1  0  0\n",
       "2    1  0  0  0  0\n",
       "94   0  1  0  0  0\n",
       "45   0  0  1  0  0\n",
       "42   0  0  1  0  0\n",
       "..  .. .. .. .. ..\n",
       "71   0  0  1  0  0\n",
       "106  0  0  0  1  0\n",
       "14   1  0  0  0  0\n",
       "92   0  1  0  0  0\n",
       "102  0  0  0  0  1\n",
       "\n",
       "[131 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes_dict = {'normal':'1 0 0 0 0', #1\n",
    "        'collision_in_tool':'0 1 0 0 0', #2\n",
    "        'collision_in_part':'0 0 1 0 0', #3\n",
    "        'bottom_collision':'0 0 0 1 0', #4\n",
    "        'bottom_obstruction':'0 0 0 0 1'} #5\n",
    "\n",
    "labels_df = labels_df.replace({'labels':classes_dict})\n",
    "labels_df[['1', '2', '3', '4', '5']] = labels_df['labels'].str.split(' ', 4, expand= True)\n",
    "labels_df = labels_df.loc[:,['1', '2', '3', '4', '5']]\n",
    "labels_df[['1', '2', '3', '4', '5']]=labels_df[['1', '2', '3', '4', '5']].astype(str).astype(int)\n",
    "\n",
    "def norm(x):\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "    range = x_max - x_min  #min max entre 0 y 1\n",
    "    return((x-x_min)/(range))\n",
    "\n",
    "norm_features = norm(features)\n",
    "#split para entrenamiento y validacion\n",
    "train_features, test_features, train_labels, test_labels  = train_test_split(norm_features, labels_df, test_size=0.2, random_state= 42)\n",
    "display(train_features)\n",
    "display(train_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(learning_rate):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(8, (3,3), activation='relu', input_shape = (15,6,1), padding='same'), # #de feature maps, dimensiones del kernel, kernel entre más pequeño mejor y se prefiere un número impar\n",
    "        tf.keras.layers.Conv2D(8, (3,3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D((2,2), strides= 2, padding= 'valid'), # dimensiones del pooling\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Conv2D(16, (3,3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.Conv2D(16, (3,3), activation='relu', padding='same'),# #de feature maps, dimensiones del kernel, kernel entre más pequeño mejor y se prefiere un número impar\n",
    "        tf.keras.layers.MaxPooling2D((2,2), strides= 2, padding='valid'), # dimensiones del pooling\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(24, activation='relu'),\n",
    "        tf.keras.layers.Dense(5, activation='softmax')])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "# input shape es 15 filas, 6 columnas y 1 canal de color\n",
    "#model = model = my_model(0.01)\n",
    "#model.summary()\n",
    "\n",
    "# initialize tuner to run the model.\n",
    "# using the Hyperband search algorithm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, features, labels, epochs, batch_size):\n",
    "    history = model.fit(\n",
    "        x = features,\n",
    "        y = labels,\n",
    "        epochs= epochs,\n",
    "        batch_size= batch_size,\n",
    "        validation_split= 0.25\n",
    "    )\n",
    "    hist= pd.DataFrame(history.history) #se guardan los valores de errores y metricas en un diccionario\n",
    "    hist['epoch'] = history.epoch #los epochs se deben añadir aparte\n",
    "    return hist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizaciones\n",
    "### Pasos para visualizar los filtros:\n",
    "##### 1. Iterar por todas las capas del modelo usando model.layers\n",
    "##### 2. Si la capa actual es de convolucion se extraen los pesos y sesgos usando get_weights()\n",
    "##### 3. Se normalizan los pesos de los filtros entre 0 y 1\n",
    "##### 4. Se plotean los filtros para cada capa convolutional y todos los canales de color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_curves(history):\n",
    "    hist = history\n",
    "    labels = {\"loss\":\"Training Loss\", \"val_loss\":\"Validation Loss\"}\n",
    "    hist.rename(columns = labels, inplace = True)\n",
    "    \n",
    "    fig = px.line(hist, x='epoch', y=['Training Loss', 'Validation Loss'],\n",
    "                title='Gráficas de Pérdida de Entrenamiento y Evaluación',\n",
    "                labels={\"epoch\": \"Epoch\", \"value\":\"Binary Cross Entropy\", \"variable\":\"Curvas de Pérdida\"},\n",
    "                color_discrete_map={ # replaces default color mapping by value\n",
    "                \"Training Loss\": \"#46039f\", \"Validation Loss\": \"#fb9f3a\"})\n",
    "    fig.update_layout(template='plotly_white')\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se corren las funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dave4\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\backend.py:5703: UserWarning:\n",
      "\n",
      "\"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 1s 3ms/step - loss: 0.5301 - accuracy: 0.3163 - val_loss: 0.4858 - val_accuracy: 0.2727\n",
      "Epoch 2/250\n",
      "31/98 [========>.....................] - ETA: 0s - loss: 0.4876 - accuracy: 0.3871"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dave4\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\backend.py:5703: UserWarning:\n",
      "\n",
      "\"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 0s 2ms/step - loss: 0.5016 - accuracy: 0.3265 - val_loss: 0.4979 - val_accuracy: 0.2727\n",
      "Epoch 3/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4971 - accuracy: 0.2959 - val_loss: 0.4944 - val_accuracy: 0.2727\n",
      "Epoch 4/250\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 0.4941 - accuracy: 0.3163 - val_loss: 0.4956 - val_accuracy: 0.2727\n",
      "Epoch 5/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4935 - accuracy: 0.3163 - val_loss: 0.4992 - val_accuracy: 0.2727\n",
      "Epoch 6/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4947 - accuracy: 0.3163 - val_loss: 0.4906 - val_accuracy: 0.2727\n",
      "Epoch 7/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4911 - accuracy: 0.3163 - val_loss: 0.4917 - val_accuracy: 0.2727\n",
      "Epoch 8/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4925 - accuracy: 0.3163 - val_loss: 0.4932 - val_accuracy: 0.2727\n",
      "Epoch 9/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4922 - accuracy: 0.3163 - val_loss: 0.4932 - val_accuracy: 0.2727\n",
      "Epoch 10/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4934 - accuracy: 0.2857 - val_loss: 0.4921 - val_accuracy: 0.2727\n",
      "Epoch 11/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4922 - accuracy: 0.3163 - val_loss: 0.4922 - val_accuracy: 0.2727\n",
      "Epoch 12/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4916 - accuracy: 0.3163 - val_loss: 0.4929 - val_accuracy: 0.2727\n",
      "Epoch 13/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4905 - accuracy: 0.3163 - val_loss: 0.4942 - val_accuracy: 0.2727\n",
      "Epoch 14/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4922 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 15/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4894 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 16/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4917 - accuracy: 0.3163 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 17/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4901 - accuracy: 0.3163 - val_loss: 0.4929 - val_accuracy: 0.2727\n",
      "Epoch 18/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4909 - accuracy: 0.3163 - val_loss: 0.4939 - val_accuracy: 0.2727\n",
      "Epoch 19/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4915 - accuracy: 0.3163 - val_loss: 0.4941 - val_accuracy: 0.2727\n",
      "Epoch 20/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4904 - accuracy: 0.3163 - val_loss: 0.4932 - val_accuracy: 0.2727\n",
      "Epoch 21/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4894 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 22/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4904 - accuracy: 0.3163 - val_loss: 0.4907 - val_accuracy: 0.2727\n",
      "Epoch 23/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4915 - accuracy: 0.2653 - val_loss: 0.4940 - val_accuracy: 0.2727\n",
      "Epoch 24/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4918 - accuracy: 0.3163 - val_loss: 0.4940 - val_accuracy: 0.2727\n",
      "Epoch 25/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4901 - accuracy: 0.3163 - val_loss: 0.4913 - val_accuracy: 0.2727\n",
      "Epoch 26/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4911 - accuracy: 0.3163 - val_loss: 0.4924 - val_accuracy: 0.2727\n",
      "Epoch 27/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4901 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 28/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4900 - accuracy: 0.3163 - val_loss: 0.4905 - val_accuracy: 0.2727\n",
      "Epoch 29/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4914 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 30/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4897 - accuracy: 0.3163 - val_loss: 0.4917 - val_accuracy: 0.2727\n",
      "Epoch 31/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4900 - accuracy: 0.3163 - val_loss: 0.4910 - val_accuracy: 0.2727\n",
      "Epoch 32/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4898 - accuracy: 0.3163 - val_loss: 0.4933 - val_accuracy: 0.2727\n",
      "Epoch 33/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.3163 - val_loss: 0.4961 - val_accuracy: 0.2727\n",
      "Epoch 34/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4898 - accuracy: 0.3163 - val_loss: 0.4942 - val_accuracy: 0.2727\n",
      "Epoch 35/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.3163 - val_loss: 0.4946 - val_accuracy: 0.2727\n",
      "Epoch 36/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4895 - accuracy: 0.3163 - val_loss: 0.4921 - val_accuracy: 0.2727\n",
      "Epoch 37/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4899 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 38/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.3163 - val_loss: 0.4911 - val_accuracy: 0.2727\n",
      "Epoch 39/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4909 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 40/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4910 - accuracy: 0.2755 - val_loss: 0.4916 - val_accuracy: 0.2727\n",
      "Epoch 41/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4891 - accuracy: 0.3163 - val_loss: 0.4939 - val_accuracy: 0.2727\n",
      "Epoch 42/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4905 - accuracy: 0.3163 - val_loss: 0.4936 - val_accuracy: 0.2727\n",
      "Epoch 43/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4896 - accuracy: 0.3163 - val_loss: 0.4954 - val_accuracy: 0.2727\n",
      "Epoch 44/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4890 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 45/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4902 - accuracy: 0.3163 - val_loss: 0.4913 - val_accuracy: 0.2727\n",
      "Epoch 46/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4907 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 47/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4890 - accuracy: 0.3163 - val_loss: 0.4922 - val_accuracy: 0.2727\n",
      "Epoch 48/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.3163 - val_loss: 0.4938 - val_accuracy: 0.2727\n",
      "Epoch 49/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.3163 - val_loss: 0.4910 - val_accuracy: 0.2727\n",
      "Epoch 50/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.3163 - val_loss: 0.4936 - val_accuracy: 0.2727\n",
      "Epoch 51/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4896 - accuracy: 0.2959 - val_loss: 0.4941 - val_accuracy: 0.2727\n",
      "Epoch 52/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4903 - accuracy: 0.3163 - val_loss: 0.4916 - val_accuracy: 0.2727\n",
      "Epoch 53/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4894 - accuracy: 0.3163 - val_loss: 0.4943 - val_accuracy: 0.2727\n",
      "Epoch 54/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4899 - accuracy: 0.3163 - val_loss: 0.4941 - val_accuracy: 0.2727\n",
      "Epoch 55/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.3163 - val_loss: 0.4929 - val_accuracy: 0.2727\n",
      "Epoch 56/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4893 - accuracy: 0.3163 - val_loss: 0.4929 - val_accuracy: 0.2727\n",
      "Epoch 57/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 58/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4893 - accuracy: 0.3163 - val_loss: 0.4924 - val_accuracy: 0.2727\n",
      "Epoch 59/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4901 - accuracy: 0.3163 - val_loss: 0.4935 - val_accuracy: 0.2727\n",
      "Epoch 60/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4886 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 61/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4896 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 62/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 63/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4887 - accuracy: 0.3163 - val_loss: 0.4951 - val_accuracy: 0.2727\n",
      "Epoch 64/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 65/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.3163 - val_loss: 0.4934 - val_accuracy: 0.2727\n",
      "Epoch 66/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4891 - accuracy: 0.3163 - val_loss: 0.4922 - val_accuracy: 0.2727\n",
      "Epoch 67/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4887 - accuracy: 0.3163 - val_loss: 0.4942 - val_accuracy: 0.2727\n",
      "Epoch 68/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4890 - accuracy: 0.3163 - val_loss: 0.4910 - val_accuracy: 0.2727\n",
      "Epoch 69/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4888 - accuracy: 0.3163 - val_loss: 0.4916 - val_accuracy: 0.2727\n",
      "Epoch 70/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4888 - accuracy: 0.3163 - val_loss: 0.4931 - val_accuracy: 0.2727\n",
      "Epoch 71/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4886 - accuracy: 0.3163 - val_loss: 0.4931 - val_accuracy: 0.2727\n",
      "Epoch 72/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4890 - accuracy: 0.3163 - val_loss: 0.4920 - val_accuracy: 0.2727\n",
      "Epoch 73/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4927 - val_accuracy: 0.2727\n",
      "Epoch 74/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4898 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 75/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4895 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 76/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.3163 - val_loss: 0.4922 - val_accuracy: 0.2727\n",
      "Epoch 77/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4887 - accuracy: 0.3163 - val_loss: 0.4922 - val_accuracy: 0.2727\n",
      "Epoch 78/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 79/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.3163 - val_loss: 0.4913 - val_accuracy: 0.2727\n",
      "Epoch 80/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4895 - accuracy: 0.3163 - val_loss: 0.4934 - val_accuracy: 0.2727\n",
      "Epoch 81/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 82/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4894 - accuracy: 0.3163 - val_loss: 0.4931 - val_accuracy: 0.2727\n",
      "Epoch 83/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4893 - accuracy: 0.3163 - val_loss: 0.4922 - val_accuracy: 0.2727\n",
      "Epoch 84/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4887 - accuracy: 0.3163 - val_loss: 0.4912 - val_accuracy: 0.2727\n",
      "Epoch 85/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4887 - accuracy: 0.3163 - val_loss: 0.4920 - val_accuracy: 0.2727\n",
      "Epoch 86/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.3163 - val_loss: 0.4920 - val_accuracy: 0.2727\n",
      "Epoch 87/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4890 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 88/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4893 - accuracy: 0.3163 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 89/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4891 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 90/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4895 - accuracy: 0.3163 - val_loss: 0.4929 - val_accuracy: 0.2727\n",
      "Epoch 91/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4897 - accuracy: 0.3163 - val_loss: 0.4913 - val_accuracy: 0.2727\n",
      "Epoch 92/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.3163 - val_loss: 0.4932 - val_accuracy: 0.2727\n",
      "Epoch 93/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.3163 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 94/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 95/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4887 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 96/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.3163 - val_loss: 0.4932 - val_accuracy: 0.2727\n",
      "Epoch 97/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4888 - accuracy: 0.3163 - val_loss: 0.4917 - val_accuracy: 0.2727\n",
      "Epoch 98/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.3163 - val_loss: 0.4931 - val_accuracy: 0.2727\n",
      "Epoch 99/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.3163 - val_loss: 0.4914 - val_accuracy: 0.2727\n",
      "Epoch 100/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.3163 - val_loss: 0.4910 - val_accuracy: 0.2727\n",
      "Epoch 101/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4886 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 102/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.3163 - val_loss: 0.4938 - val_accuracy: 0.2727\n",
      "Epoch 103/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.3163 - val_loss: 0.4920 - val_accuracy: 0.2727\n",
      "Epoch 104/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 105/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4888 - accuracy: 0.3163 - val_loss: 0.4924 - val_accuracy: 0.2727\n",
      "Epoch 106/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.3163 - val_loss: 0.4914 - val_accuracy: 0.2727\n",
      "Epoch 107/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.3163 - val_loss: 0.4935 - val_accuracy: 0.2727\n",
      "Epoch 108/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4875 - accuracy: 0.3163 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 109/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4933 - val_accuracy: 0.2727\n",
      "Epoch 110/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 111/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4916 - val_accuracy: 0.2727\n",
      "Epoch 112/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 113/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.3163 - val_loss: 0.4929 - val_accuracy: 0.2727\n",
      "Epoch 114/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4917 - val_accuracy: 0.2727\n",
      "Epoch 115/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4929 - val_accuracy: 0.2727\n",
      "Epoch 116/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.3163 - val_loss: 0.4934 - val_accuracy: 0.2727\n",
      "Epoch 117/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 118/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4888 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 119/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 120/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.3163 - val_loss: 0.4921 - val_accuracy: 0.2727\n",
      "Epoch 121/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4931 - val_accuracy: 0.2727\n",
      "Epoch 122/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4924 - val_accuracy: 0.2727\n",
      "Epoch 123/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 124/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.3163 - val_loss: 0.4933 - val_accuracy: 0.2727\n",
      "Epoch 125/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 126/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 127/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.3163 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 128/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 129/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 130/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4921 - val_accuracy: 0.2727\n",
      "Epoch 131/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4887 - accuracy: 0.3163 - val_loss: 0.4921 - val_accuracy: 0.2727\n",
      "Epoch 132/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.3163 - val_loss: 0.4922 - val_accuracy: 0.2727\n",
      "Epoch 133/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4933 - val_accuracy: 0.2727\n",
      "Epoch 134/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4888 - accuracy: 0.3163 - val_loss: 0.4931 - val_accuracy: 0.2727\n",
      "Epoch 135/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 136/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 137/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4887 - accuracy: 0.3163 - val_loss: 0.4933 - val_accuracy: 0.2727\n",
      "Epoch 138/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 139/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4917 - val_accuracy: 0.2727\n",
      "Epoch 140/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.3163 - val_loss: 0.4933 - val_accuracy: 0.2727\n",
      "Epoch 141/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 142/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.3163 - val_loss: 0.4922 - val_accuracy: 0.2727\n",
      "Epoch 143/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4924 - val_accuracy: 0.2727\n",
      "Epoch 144/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 145/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.3163 - val_loss: 0.4917 - val_accuracy: 0.2727\n",
      "Epoch 146/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.3163 - val_loss: 0.4927 - val_accuracy: 0.2727\n",
      "Epoch 147/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4886 - accuracy: 0.3163 - val_loss: 0.4920 - val_accuracy: 0.2727\n",
      "Epoch 148/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4872 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 149/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 150/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 151/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4927 - val_accuracy: 0.2727\n",
      "Epoch 152/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 153/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 154/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4938 - val_accuracy: 0.2727\n",
      "Epoch 155/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.3163 - val_loss: 0.4941 - val_accuracy: 0.2727\n",
      "Epoch 156/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4888 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 157/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4935 - val_accuracy: 0.2727\n",
      "Epoch 158/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 159/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.3163 - val_loss: 0.4927 - val_accuracy: 0.2727\n",
      "Epoch 160/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 161/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4888 - accuracy: 0.3163 - val_loss: 0.4922 - val_accuracy: 0.2727\n",
      "Epoch 162/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 163/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 164/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4917 - val_accuracy: 0.2727\n",
      "Epoch 165/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 166/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 167/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.3163 - val_loss: 0.4917 - val_accuracy: 0.2727\n",
      "Epoch 168/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4920 - val_accuracy: 0.2727\n",
      "Epoch 169/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.3163 - val_loss: 0.4917 - val_accuracy: 0.2727\n",
      "Epoch 170/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 171/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 172/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 173/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4921 - val_accuracy: 0.2727\n",
      "Epoch 174/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 175/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.3163 - val_loss: 0.4920 - val_accuracy: 0.2727\n",
      "Epoch 176/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.3163 - val_loss: 0.4929 - val_accuracy: 0.2727\n",
      "Epoch 177/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 178/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4875 - accuracy: 0.3163 - val_loss: 0.4927 - val_accuracy: 0.2727\n",
      "Epoch 179/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.3163 - val_loss: 0.4933 - val_accuracy: 0.2727\n",
      "Epoch 180/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 181/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4929 - val_accuracy: 0.2727\n",
      "Epoch 182/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.3163 - val_loss: 0.4933 - val_accuracy: 0.2727\n",
      "Epoch 183/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4875 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 184/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 185/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 186/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4939 - val_accuracy: 0.2727\n",
      "Epoch 187/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 188/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4875 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 189/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4924 - val_accuracy: 0.2727\n",
      "Epoch 190/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.3163 - val_loss: 0.4917 - val_accuracy: 0.2727\n",
      "Epoch 191/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 192/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 193/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4872 - accuracy: 0.3163 - val_loss: 0.4927 - val_accuracy: 0.2727\n",
      "Epoch 194/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4912 - val_accuracy: 0.2727\n",
      "Epoch 195/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 196/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4871 - accuracy: 0.3163 - val_loss: 0.4921 - val_accuracy: 0.2727\n",
      "Epoch 197/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 198/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 199/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4886 - accuracy: 0.3163 - val_loss: 0.4914 - val_accuracy: 0.2727\n",
      "Epoch 200/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 201/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.3163 - val_loss: 0.4940 - val_accuracy: 0.2727\n",
      "Epoch 202/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.3163 - val_loss: 0.4914 - val_accuracy: 0.2727\n",
      "Epoch 203/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4916 - val_accuracy: 0.2727\n",
      "Epoch 204/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4914 - val_accuracy: 0.2727\n",
      "Epoch 205/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.3163 - val_loss: 0.4915 - val_accuracy: 0.2727\n",
      "Epoch 206/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4871 - accuracy: 0.3163 - val_loss: 0.4922 - val_accuracy: 0.2727\n",
      "Epoch 207/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 208/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 209/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 210/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4920 - val_accuracy: 0.2727\n",
      "Epoch 211/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4910 - val_accuracy: 0.2727\n",
      "Epoch 212/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4872 - accuracy: 0.3163 - val_loss: 0.4922 - val_accuracy: 0.2727\n",
      "Epoch 213/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.3163 - val_loss: 0.4913 - val_accuracy: 0.2727\n",
      "Epoch 214/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4920 - val_accuracy: 0.2727\n",
      "Epoch 215/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.3163 - val_loss: 0.4924 - val_accuracy: 0.2727\n",
      "Epoch 216/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4929 - val_accuracy: 0.2727\n",
      "Epoch 217/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4875 - accuracy: 0.3163 - val_loss: 0.4917 - val_accuracy: 0.2727\n",
      "Epoch 218/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.3163 - val_loss: 0.4916 - val_accuracy: 0.2727\n",
      "Epoch 219/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4888 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 220/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4920 - val_accuracy: 0.2727\n",
      "Epoch 221/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4871 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 222/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 223/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4922 - val_accuracy: 0.2727\n",
      "Epoch 224/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.3163 - val_loss: 0.4913 - val_accuracy: 0.2727\n",
      "Epoch 225/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4875 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 226/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4927 - val_accuracy: 0.2727\n",
      "Epoch 227/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 228/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 229/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.3163 - val_loss: 0.4920 - val_accuracy: 0.2727\n",
      "Epoch 230/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 231/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 232/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4870 - accuracy: 0.3163 - val_loss: 0.4927 - val_accuracy: 0.2727\n",
      "Epoch 233/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.3163 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 234/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4932 - val_accuracy: 0.2727\n",
      "Epoch 235/250\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 0.4881 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 236/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4870 - accuracy: 0.3163 - val_loss: 0.4924 - val_accuracy: 0.2727\n",
      "Epoch 237/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4875 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 238/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 239/250\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 0.4871 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 240/250\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4915 - val_accuracy: 0.2727\n",
      "Epoch 241/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.3163 - val_loss: 0.4904 - val_accuracy: 0.2727\n",
      "Epoch 242/250\n",
      "98/98 [==============================] - 0s 5ms/step - loss: 0.4872 - accuracy: 0.3163 - val_loss: 0.4917 - val_accuracy: 0.2727\n",
      "Epoch 243/250\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 0.4873 - accuracy: 0.3163 - val_loss: 0.4915 - val_accuracy: 0.2727\n",
      "Epoch 244/250\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 0.4877 - accuracy: 0.3163 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 245/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4872 - accuracy: 0.3163 - val_loss: 0.4915 - val_accuracy: 0.2727\n",
      "Epoch 246/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4870 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 247/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4934 - val_accuracy: 0.2727\n",
      "Epoch 248/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 249/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4869 - accuracy: 0.3163 - val_loss: 0.4927 - val_accuracy: 0.2727\n",
      "Epoch 250/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.3163 - val_loss: 0.4927 - val_accuracy: 0.2727\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.530</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.502</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.497</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.273</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.494</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.273</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.493</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.273</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.487</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.273</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.488</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.273</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.488</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.273</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.487</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.273</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.487</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.273</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     loss  accuracy  val_loss  val_accuracy  epoch\n",
       "0   0.530     0.316     0.486         0.273      0\n",
       "1   0.502     0.327     0.498         0.273      1\n",
       "2   0.497     0.296     0.494         0.273      2\n",
       "3   0.494     0.316     0.496         0.273      3\n",
       "4   0.493     0.316     0.499         0.273      4\n",
       "..    ...       ...       ...           ...    ...\n",
       "245 0.487     0.316     0.493         0.273    245\n",
       "246 0.488     0.316     0.493         0.273    246\n",
       "247 0.488     0.316     0.492         0.273    247\n",
       "248 0.487     0.316     0.493         0.273    248\n",
       "249 0.487     0.316     0.493         0.273    249\n",
       "\n",
       "[250 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Curvas de Pérdida=Training Loss<br>Epoch=%{x}<br>Binary Cross Entropy=%{y}<extra></extra>",
         "legendgroup": "Training Loss",
         "line": {
          "color": "#46039f",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Training Loss",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249
         ],
         "xaxis": "x",
         "y": [
          0.5301069021224976,
          0.5016279816627502,
          0.49710819125175476,
          0.4940946400165558,
          0.49349725246429443,
          0.494695782661438,
          0.4911162257194519,
          0.49249720573425293,
          0.4921519160270691,
          0.4933784306049347,
          0.4921732544898987,
          0.49156370759010315,
          0.4905393123626709,
          0.49221739172935486,
          0.48939141631126404,
          0.4916746914386749,
          0.49005740880966187,
          0.49085015058517456,
          0.491549551486969,
          0.4904156029224396,
          0.4893774688243866,
          0.49039891362190247,
          0.49151864647865295,
          0.4917812645435333,
          0.49005231261253357,
          0.49110475182533264,
          0.49008721113204956,
          0.48995858430862427,
          0.4914475381374359,
          0.4896999001502991,
          0.49002060294151306,
          0.4898416996002197,
          0.48892515897750854,
          0.48981866240501404,
          0.48916763067245483,
          0.48948851227760315,
          0.4899310767650604,
          0.48916342854499817,
          0.49087101221084595,
          0.49098101258277893,
          0.48907041549682617,
          0.49052339792251587,
          0.48955607414245605,
          0.4889887273311615,
          0.49023517966270447,
          0.4906553030014038,
          0.4889650046825409,
          0.489218533039093,
          0.48890140652656555,
          0.4884650707244873,
          0.48957115411758423,
          0.49025264382362366,
          0.4893912374973297,
          0.4898662269115448,
          0.4891911745071411,
          0.4893351197242737,
          0.4889499545097351,
          0.4893493354320526,
          0.4900975823402405,
          0.4886302053928375,
          0.48960986733436584,
          0.48836106061935425,
          0.48866337537765503,
          0.4892200827598572,
          0.4883653223514557,
          0.48913514614105225,
          0.4886939525604248,
          0.48902183771133423,
          0.48877549171447754,
          0.48884299397468567,
          0.48859813809394836,
          0.48899468779563904,
          0.4881742298603058,
          0.4898269772529602,
          0.48948076367378235,
          0.4880920648574829,
          0.4887213110923767,
          0.488180011510849,
          0.488495409488678,
          0.48949000239372253,
          0.4882700741291046,
          0.48944053053855896,
          0.48931366205215454,
          0.4887359142303467,
          0.4887301027774811,
          0.4892008304595947,
          0.4889908730983734,
          0.4893083870410919,
          0.48909610509872437,
          0.4894881844520569,
          0.4896877706050873,
          0.4878939688205719,
          0.48808553814888,
          0.48809126019477844,
          0.48866358399391174,
          0.4884333610534668,
          0.48879826068878174,
          0.48771166801452637,
          0.4885195791721344,
          0.4881124794483185,
          0.4886183440685272,
          0.4884316325187683,
          0.48826345801353455,
          0.48850780725479126,
          0.4888187646865845,
          0.48840755224227905,
          0.4885377585887909,
          0.4874666929244995,
          0.4878242313861847,
          0.4882504642009735,
          0.4881701171398163,
          0.4878373444080353,
          0.487935870885849,
          0.48822084069252014,
          0.48755648732185364,
          0.4883747398853302,
          0.4877115488052368,
          0.4888439178466797,
          0.4882338047027588,
          0.4883441627025604,
          0.4878266453742981,
          0.4882320165634155,
          0.48801857233047485,
          0.4879266321659088,
          0.48767513036727905,
          0.4877833127975464,
          0.4889110326766968,
          0.4877507984638214,
          0.48769378662109375,
          0.48800185322761536,
          0.48874741792678833,
          0.48811179399490356,
          0.4879951775074005,
          0.48882079124450684,
          0.4879360496997833,
          0.4882510304450989,
          0.48871979117393494,
          0.4879230260848999,
          0.4878278076648712,
          0.48813095688819885,
          0.4878348112106323,
          0.4884903132915497,
          0.4881551265716553,
          0.48781606554985046,
          0.48813652992248535,
          0.48794329166412354,
          0.4886460304260254,
          0.4871784448623657,
          0.487844318151474,
          0.4875858426094055,
          0.4880187511444092,
          0.48828989267349243,
          0.4872908592224121,
          0.4880456030368805,
          0.4879148304462433,
          0.4887620210647583,
          0.48797446489334106,
          0.4881325960159302,
          0.48772865533828735,
          0.4875788390636444,
          0.48875170946121216,
          0.4881160855293274,
          0.4883899986743927,
          0.4876475930213928,
          0.48792314529418945,
          0.48741045594215393,
          0.488099068403244,
          0.48760250210762024,
          0.4883773922920227,
          0.4878092408180237,
          0.48816782236099243,
          0.48769044876098633,
          0.4881701171398163,
          0.4875694513320923,
          0.48825621604919434,
          0.4873090982437134,
          0.48804473876953125,
          0.4875135123729706,
          0.48785150051116943,
          0.4877559244632721,
          0.48775070905685425,
          0.48772698640823364,
          0.4874885380268097,
          0.4882129430770874,
          0.48795270919799805,
          0.48797059059143066,
          0.48805999755859375,
          0.4875079095363617,
          0.48800358176231384,
          0.48742586374282837,
          0.48804640769958496,
          0.4879043400287628,
          0.487213671207428,
          0.48761090636253357,
          0.48799362778663635,
          0.48713362216949463,
          0.4879951775074005,
          0.48799607157707214,
          0.48855817317962646,
          0.48791608214378357,
          0.4879394769668579,
          0.48743847012519836,
          0.48775404691696167,
          0.48824450373649597,
          0.48728933930397034,
          0.48712223768234253,
          0.48781850934028625,
          0.48762616515159607,
          0.4880208671092987,
          0.48818451166152954,
          0.4879615902900696,
          0.48718392848968506,
          0.4874410927295685,
          0.48780006170272827,
          0.48774778842926025,
          0.4875990152359009,
          0.4875374734401703,
          0.4883120357990265,
          0.48878732323646545,
          0.48761191964149475,
          0.48710134625434875,
          0.48758789896965027,
          0.48762667179107666,
          0.48734259605407715,
          0.4874576926231384,
          0.4879920482635498,
          0.487972617149353,
          0.48742637038230896,
          0.48729193210601807,
          0.4875656068325043,
          0.4874090552330017,
          0.4870343804359436,
          0.48770076036453247,
          0.4876491129398346,
          0.4880574643611908,
          0.48703786730766296,
          0.4875369071960449,
          0.48768797516822815,
          0.4870956838130951,
          0.4875708222389221,
          0.48806774616241455,
          0.48717397451400757,
          0.4872807562351227,
          0.48773932456970215,
          0.48722708225250244,
          0.4869775176048279,
          0.4877530038356781,
          0.48767420649528503,
          0.4868704378604889,
          0.48731425404548645
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Curvas de Pérdida=Validation Loss<br>Epoch=%{x}<br>Binary Cross Entropy=%{y}<extra></extra>",
         "legendgroup": "Validation Loss",
         "line": {
          "color": "#fb9f3a",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Validation Loss",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249
         ],
         "xaxis": "x",
         "y": [
          0.48584383726119995,
          0.49792319536209106,
          0.4943702518939972,
          0.49563267827033997,
          0.4991798400878906,
          0.49057719111442566,
          0.4916512370109558,
          0.4931789040565491,
          0.4932027757167816,
          0.49214571714401245,
          0.49218037724494934,
          0.49293962121009827,
          0.49423113465309143,
          0.4924842417240143,
          0.49264004826545715,
          0.4927797019481659,
          0.49286091327667236,
          0.49393290281295776,
          0.4941498041152954,
          0.49322283267974854,
          0.49193623661994934,
          0.49067479372024536,
          0.49403706192970276,
          0.494029700756073,
          0.49131250381469727,
          0.4924491047859192,
          0.4925871789455414,
          0.4904814064502716,
          0.4923231601715088,
          0.4916841983795166,
          0.4910401999950409,
          0.49328330159187317,
          0.4960714876651764,
          0.4941546618938446,
          0.4945516288280487,
          0.4921218454837799,
          0.4923155903816223,
          0.49114447832107544,
          0.49232256412506104,
          0.4916245937347412,
          0.4938642382621765,
          0.4936214089393616,
          0.49538618326187134,
          0.49230262637138367,
          0.491269588470459,
          0.4929579496383667,
          0.4922194480895996,
          0.4938265383243561,
          0.49101534485816956,
          0.4935644865036011,
          0.4941118359565735,
          0.4915764629840851,
          0.49431130290031433,
          0.4940979778766632,
          0.49292048811912537,
          0.4928857088088989,
          0.4929957985877991,
          0.492435097694397,
          0.49349939823150635,
          0.4925025701522827,
          0.4926154315471649,
          0.4924500286579132,
          0.4950747489929199,
          0.4918436110019684,
          0.49342045187950134,
          0.49217844009399414,
          0.49419552087783813,
          0.4909851849079132,
          0.49156802892684937,
          0.49307170510292053,
          0.4931306540966034,
          0.4920331835746765,
          0.49266982078552246,
          0.4923270344734192,
          0.49249470233917236,
          0.4921599328517914,
          0.49222397804260254,
          0.4930112957954407,
          0.49127450585365295,
          0.4933958351612091,
          0.49303048849105835,
          0.4931447505950928,
          0.49224239587783813,
          0.49117082357406616,
          0.49195265769958496,
          0.4919866621494293,
          0.4930129647254944,
          0.4928298592567444,
          0.49258705973625183,
          0.49294546246528625,
          0.4912813901901245,
          0.4931875169277191,
          0.4927959442138672,
          0.4925806522369385,
          0.4925001859664917,
          0.4932287931442261,
          0.4916500449180603,
          0.4931342899799347,
          0.4914461672306061,
          0.49101847410202026,
          0.49256885051727295,
          0.4937973618507385,
          0.4920322000980377,
          0.49190011620521545,
          0.49237990379333496,
          0.49141645431518555,
          0.4935017228126526,
          0.4927837550640106,
          0.49326324462890625,
          0.4925423264503479,
          0.491641640663147,
          0.4917891025543213,
          0.49290990829467773,
          0.49169108271598816,
          0.49294403195381165,
          0.493350625038147,
          0.4926026165485382,
          0.49190202355384827,
          0.49187013506889343,
          0.4920983612537384,
          0.49307170510292053,
          0.49242880940437317,
          0.4923461079597473,
          0.49330195784568787,
          0.49300992488861084,
          0.4923399090766907,
          0.49276450276374817,
          0.49296286702156067,
          0.4919239282608032,
          0.4920811951160431,
          0.49206241965293884,
          0.49215057492256165,
          0.49331265687942505,
          0.49306026101112366,
          0.4926305115222931,
          0.49230730533599854,
          0.49326658248901367,
          0.49190109968185425,
          0.49173811078071594,
          0.49330201745033264,
          0.49227476119995117,
          0.49221622943878174,
          0.49237680435180664,
          0.4924996793270111,
          0.4916670322418213,
          0.4927027225494385,
          0.49203184247016907,
          0.4917662739753723,
          0.4929927587509155,
          0.4926031231880188,
          0.4926762580871582,
          0.4930170178413391,
          0.4926184415817261,
          0.4937935471534729,
          0.4941139817237854,
          0.49185776710510254,
          0.4934760630130768,
          0.49229633808135986,
          0.4926697015762329,
          0.49264103174209595,
          0.49224939942359924,
          0.49246764183044434,
          0.4930492043495178,
          0.49173516035079956,
          0.4929749369621277,
          0.49181482195854187,
          0.49168846011161804,
          0.49203208088874817,
          0.4917273223400116,
          0.49295955896377563,
          0.49247822165489197,
          0.491851806640625,
          0.492085725069046,
          0.4918704032897949,
          0.4920123219490051,
          0.4928889870643616,
          0.49180087447166443,
          0.49265623092651367,
          0.49325641989707947,
          0.491929829120636,
          0.4928525388240814,
          0.4932882785797119,
          0.49256712198257446,
          0.4928123652935028,
          0.49303141236305237,
          0.49390891194343567,
          0.49249741435050964,
          0.4926242232322693,
          0.4923803210258484,
          0.4917158782482147,
          0.492596834897995,
          0.4917808175086975,
          0.49268975853919983,
          0.49119439721107483,
          0.49256375432014465,
          0.49210137128829956,
          0.49189943075180054,
          0.49190446734428406,
          0.491404265165329,
          0.492620587348938,
          0.49398040771484375,
          0.49136120080947876,
          0.4916155934333801,
          0.49135908484458923,
          0.4915350079536438,
          0.49221286177635193,
          0.49254748225212097,
          0.49188828468322754,
          0.4923315942287445,
          0.4919884502887726,
          0.49103742837905884,
          0.49217402935028076,
          0.491283655166626,
          0.49202173948287964,
          0.49237608909606934,
          0.49286502599716187,
          0.4916520416736603,
          0.49156349897384644,
          0.4924515187740326,
          0.4920122027397156,
          0.492276668548584,
          0.4925537109375,
          0.4921620786190033,
          0.49133768677711487,
          0.4930437207221985,
          0.49267733097076416,
          0.4918200671672821,
          0.4923088848590851,
          0.4920400083065033,
          0.492801308631897,
          0.4918697774410248,
          0.49274900555610657,
          0.49279022216796875,
          0.4932001233100891,
          0.4923084080219269,
          0.4924285113811493,
          0.4926457703113556,
          0.49229559302330017,
          0.4918488562107086,
          0.49146905541419983,
          0.4903806149959564,
          0.4916825592517853,
          0.4915080666542053,
          0.49275872111320496,
          0.4914933443069458,
          0.49252602458000183,
          0.4933931231498718,
          0.4923397898674011,
          0.4927125871181488,
          0.4927108585834503
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "Curvas de Pérdida"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Gráficas de Pérdida de Entrenamiento y Evaluación"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Binary Cross Entropy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.530</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.502</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.497</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.273</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.494</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.273</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.493</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.273</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.487</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.273</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.488</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.273</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.488</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.273</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.487</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.273</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.487</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.273</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Training Loss  accuracy  Validation Loss  val_accuracy  epoch\n",
       "0            0.530     0.316            0.486         0.273      0\n",
       "1            0.502     0.327            0.498         0.273      1\n",
       "2            0.497     0.296            0.494         0.273      2\n",
       "3            0.494     0.316            0.496         0.273      3\n",
       "4            0.493     0.316            0.499         0.273      4\n",
       "..             ...       ...              ...           ...    ...\n",
       "245          0.487     0.316            0.493         0.273    245\n",
       "246          0.488     0.316            0.493         0.273    246\n",
       "247          0.488     0.316            0.492         0.273    247\n",
       "248          0.487     0.316            0.493         0.273    248\n",
       "249          0.487     0.316            0.493         0.273    249\n",
       "\n",
       "[250 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 250\n",
    "batch_size = 1\n",
    "model = my_model(learning_rate)\n",
    "history= train_model(model, train_features, train_labels, epochs, batch_size)\n",
    "display(history)\n",
    "loss_curves(history)\n",
    "#view_filters(model)\n",
    "display(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 0s/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normal</th>\n",
       "      <th>collision_in_tool</th>\n",
       "      <th>collision_in_part</th>\n",
       "      <th>bottom_collision</th>\n",
       "      <th>bottom_obstruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.239</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.239</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.239</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.239</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.239</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.239</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.239</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.239</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.239</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.239</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    normal  collision_in_tool  collision_in_part  bottom_collision  \\\n",
       "0    0.239              0.129              0.358             0.133   \n",
       "1    0.239              0.129              0.358             0.133   \n",
       "2    0.239              0.129              0.358             0.133   \n",
       "3    0.239              0.129              0.358             0.133   \n",
       "4    0.239              0.129              0.358             0.133   \n",
       "..     ...                ...                ...               ...   \n",
       "28   0.239              0.129              0.358             0.133   \n",
       "29   0.239              0.129              0.358             0.133   \n",
       "30   0.239              0.129              0.358             0.133   \n",
       "31   0.239              0.129              0.358             0.133   \n",
       "32   0.239              0.129              0.358             0.133   \n",
       "\n",
       "    bottom_obstruction  \n",
       "0                0.141  \n",
       "1                0.141  \n",
       "2                0.141  \n",
       "3                0.141  \n",
       "4                0.141  \n",
       "..                 ...  \n",
       "28               0.141  \n",
       "29               0.141  \n",
       "30               0.141  \n",
       "31               0.141  \n",
       "32               0.141  \n",
       "\n",
       "[33 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1  2  3  4  5\n",
       "135  0  0  0  1  0\n",
       "115  0  0  0  0  1\n",
       "131  0  0  0  0  1\n",
       "55   1  0  0  0  0\n",
       "95   0  1  0  0  0\n",
       "..  .. .. .. .. ..\n",
       "134  0  0  0  1  0\n",
       "160  0  1  0  0  0\n",
       "139  0  0  0  1  0\n",
       "78   0  0  1  0  0\n",
       "60   1  0  0  0  0\n",
       "\n",
       "[33 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = model.predict(test_features)\n",
    "predictions_df = pd.DataFrame(predictions, columns=['normal', 'collision_in_tool', 'collision_in_part', 'bottom_collision', 'bottom_obstruction'])\n",
    "display(predictions_df)\n",
    "display(test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
