{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar Modulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import plotly.express as px\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                labels 1  2  3  4  5 \n",
       "0               normal               \n",
       "1               normal               \n",
       "2               normal               \n",
       "3               normal               \n",
       "4               normal               \n",
       "..                 ... .. .. .. .. ..\n",
       "159  collision_in_tool               \n",
       "160  collision_in_tool               \n",
       "161  collision_in_tool               \n",
       "162  collision_in_tool               \n",
       "163  collision_in_tool               \n",
       "\n",
       "[164 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ -2.,  -1.,  81.,   0.,  -5.,   0.],\n",
       "        [ -2.,  -1.,  79.,   0.,  -4.,   0.],\n",
       "        [ -2.,  -1.,  79.,   0.,  -4.,   0.],\n",
       "        ...,\n",
       "        [ -2.,  -1.,  78.,   0.,  -5.,   0.],\n",
       "        [ -3.,  -1.,  80.,   1.,  -4.,   1.],\n",
       "        [ -2.,  -1.,  79.,   0.,  -4.,   0.]],\n",
       "\n",
       "       [[  6.,  -1.,  79.,  -2.,   4.,  -3.],\n",
       "        [ 42.,  -3.,  80.,   5.,  53.,   3.],\n",
       "        [ -5.,   4.,  74., -15., -10.,  -1.],\n",
       "        ...,\n",
       "        [ -1.,  -5.,  80.,   6.,  -6.,   0.],\n",
       "        [ -4.,   5.,  78., -14.,  -9.,  -4.],\n",
       "        [ -4.,   1.,  80.,  -3., -12.,   5.]],\n",
       "\n",
       "       [[ -2.,  -6.,  85.,  14.,  -5.,   2.],\n",
       "        [  0.,   2.,  74.,  -7.,   1.,   0.],\n",
       "        [ -4.,  -5.,  76.,   7., -11.,   4.],\n",
       "        ...,\n",
       "        [  0.,  -9.,  87.,  13.,  -5.,   2.],\n",
       "        [ -5.,   5.,  67., -17., -16.,   7.],\n",
       "        [ -6., -10.,  86.,  16., -14.,  -1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-12.,  17.,   3., -19., -10.,  -4.],\n",
       "        [-12.,  12.,  11., -13., -16.,  -4.],\n",
       "        [ -8.,   3.,   6.,   2., -11.,  -4.],\n",
       "        ...,\n",
       "        [  0.,   1.,   3.,   1.,   1.,  -3.],\n",
       "        [  1.,   2.,   9.,   0.,   3.,  -4.],\n",
       "        [  1.,   2.,   9.,   0.,   3.,  -4.]],\n",
       "\n",
       "       [[-41.,  21.,  -5., -23., -59.,  -4.],\n",
       "        [-32.,  21.,  -6., -25., -45.,  -4.],\n",
       "        [-21.,  12.,  -6., -14., -31.,  -4.],\n",
       "        ...,\n",
       "        [ -4.,   4.,   3.,   0.,  -1.,  -3.],\n",
       "        [ -4.,   3.,   2.,   2.,  -3.,  -3.],\n",
       "        [ -2.,   3.,   5.,   0.,  -2.,  -3.]],\n",
       "\n",
       "       [[  9., -10., -11.,  17.,   7.,  -4.],\n",
       "        [  5.,   0.,   4.,   0.,   7.,  -4.],\n",
       "        [ -3.,   6.,  -2.,  -8.,  -8.,  -4.],\n",
       "        ...,\n",
       "        [ -1.,   1.,  -3.,  -3.,  -2.,  -3.],\n",
       "        [  0.,  -1.,  -5.,  -1.,   1.,  -3.],\n",
       "        [ -1.,   1.,   4.,   0.,  -1.,  -3.]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.3f}\".format\n",
    "\n",
    "data = np.loadtxt(\"lp5.csv\", delimiter=\",\", dtype=str)\n",
    "data_copy = data #va de 0 a 2623 o sea 2624 datos\n",
    "cont = 16\n",
    "labels = []\n",
    "numbers = []\n",
    "features= np.zeros((164,15,6))\n",
    "\n",
    "for i in range(len(data_copy)):\n",
    "    if(cont == 16):\n",
    "        labels.append(data_copy[i])\n",
    "    if(cont<16):\n",
    "        numbers.append(data_copy[i])\n",
    "    cont -= 1\n",
    "    if(cont == 0):\n",
    "        cont = 16\n",
    "        \n",
    "cont = 0\n",
    "for i in range(164):\n",
    "    for j in range(15):\n",
    "        for z in range(6):\n",
    "            features[i][j][z] = numbers[cont][z]\n",
    "        cont += 1\n",
    "        \n",
    "labels_df = pd.DataFrame(labels, columns=['labels','1','2','3','4','5'])\n",
    "#labels_df = labels_df.loc[:,['labels']]\n",
    "display(labels_df)\n",
    "display(features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manejo de Datos y Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dave4\\AppData\\Local\\Temp\\ipykernel_15336\\1747463221.py:8: FutureWarning: In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n",
      "  labels_df[['1', '2', '3', '4', '5']] = labels_df['labels'].str.split(' ', 4, expand= True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.77660263, 0.78156702, 0.79084826, 0.77055903, 0.7684006 ,\n",
       "         0.77422836],\n",
       "        [0.77466005, 0.78739478, 0.78717893, 0.75156486, 0.7638679 ,\n",
       "         0.77206993],\n",
       "        [0.77401252, 0.78804231, 0.79041658, 0.74465789, 0.76106195,\n",
       "         0.77012735],\n",
       "        ...,\n",
       "        [0.78005612, 0.7804878 , 0.78437298, 0.78135118, 0.78113533,\n",
       "         0.78027196],\n",
       "        [0.78027196, 0.78091949, 0.77962443, 0.78091949, 0.78027196,\n",
       "         0.78027196],\n",
       "        [0.78027196, 0.78070365, 0.78286208, 0.78156702, 0.78091949,\n",
       "         0.78005612]],\n",
       "\n",
       "       [[0.78027196, 0.77940859, 0.79905029, 0.78372545, 0.77962443,\n",
       "         0.78113533],\n",
       "        [0.78070365, 0.78113533, 0.79667602, 0.77919275, 0.78091949,\n",
       "         0.78070365],\n",
       "        [0.77984028, 0.77962443, 0.79710771, 0.78221455, 0.77832938,\n",
       "         0.78156702],\n",
       "        ...,\n",
       "        [0.78070365, 0.77876106, 0.79948198, 0.78350961, 0.77962443,\n",
       "         0.78113533],\n",
       "        [0.77962443, 0.78178286, 0.79516512, 0.77703432, 0.77725016,\n",
       "         0.78221455],\n",
       "        [0.77940859, 0.77854522, 0.79926613, 0.78415713, 0.77768185,\n",
       "         0.7804878 ]],\n",
       "\n",
       "       [[0.78502051, 0.77725016, 0.78264623, 0.78631556, 0.78696309,\n",
       "         0.7804878 ],\n",
       "        [0.78588388, 0.78005612, 0.78717893, 0.78243039, 0.78890568,\n",
       "         0.77984028],\n",
       "        [0.78653141, 0.77876106, 0.78027196, 0.78480466, 0.78933736,\n",
       "         0.78027196],\n",
       "        ...,\n",
       "        [0.78005612, 0.78135118, 0.77962443, 0.7804878 , 0.77962443,\n",
       "         0.77962443],\n",
       "        [0.78005612, 0.78091949, 0.7789769 , 0.78091949, 0.78027196,\n",
       "         0.77962443],\n",
       "        [0.7804878 , 0.78091949, 0.78005612, 0.78156702, 0.78070365,\n",
       "         0.77962443]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.78005612, 0.78070365, 0.79689186, 0.77919275, 0.77660263,\n",
       "         0.77984028],\n",
       "        [0.77962443, 0.77832938, 0.79883445, 0.78307792, 0.77617095,\n",
       "         0.77811353],\n",
       "        [0.78005612, 0.78264623, 0.79516512, 0.77487589, 0.77681848,\n",
       "         0.7789769 ],\n",
       "        ...,\n",
       "        [0.7804878 , 0.77832938, 0.79969782, 0.78307792, 0.777466  ,\n",
       "         0.77854522],\n",
       "        [0.77940859, 0.77984028, 0.79710771, 0.77962443, 0.77573926,\n",
       "         0.77984028],\n",
       "        [0.77940859, 0.78243039, 0.79710771, 0.77530758, 0.77617095,\n",
       "         0.77940859]],\n",
       "\n",
       "       [[0.77077488, 0.79494928, 0.77617095, 0.75588172, 0.77832938,\n",
       "         0.7804878 ],\n",
       "        [0.7653788 , 0.78135118, 0.78243039, 0.777466  , 0.7638679 ,\n",
       "         0.78027196],\n",
       "        [0.76365206, 0.78264623, 0.78350961, 0.78005612, 0.75674509,\n",
       "         0.78027196],\n",
       "        ...,\n",
       "        [0.77984028, 0.78113533, 0.78243039, 0.78113533, 0.78005612,\n",
       "         0.78005612],\n",
       "        [0.78027196, 0.78113533, 0.78350961, 0.78091949, 0.78070365,\n",
       "         0.78027196],\n",
       "        [0.7804878 , 0.78091949, 0.78480466, 0.78221455, 0.78156702,\n",
       "         0.78005612]],\n",
       "\n",
       "       [[0.77789769, 0.77789769, 0.7744442 , 0.78696309, 0.78653141,\n",
       "         0.77789769],\n",
       "        [0.77832938, 0.77811353, 0.77422836, 0.78631556, 0.78868983,\n",
       "         0.7789769 ],\n",
       "        [0.77832938, 0.77789769, 0.77466005, 0.78696309, 0.78804231,\n",
       "         0.77768185],\n",
       "        ...,\n",
       "        [0.67882581, 0.70472696, 0.09842435, 0.7759551 , 0.85732787,\n",
       "         0.74940643],\n",
       "        [0.69933089, 0.73796676, 0.34448521, 0.77099072, 0.81674941,\n",
       "         0.77530758],\n",
       "        [0.72199439, 0.73170732, 0.7155191 , 0.81804446, 0.69156054,\n",
       "         0.77012735]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1  2  3  4  5\n",
       "84   0  0  1  0  0\n",
       "2    1  0  0  0  0\n",
       "94   0  1  0  0  0\n",
       "45   0  0  1  0  0\n",
       "42   0  0  1  0  0\n",
       "..  .. .. .. .. ..\n",
       "71   0  0  1  0  0\n",
       "106  0  0  0  1  0\n",
       "14   1  0  0  0  0\n",
       "92   0  1  0  0  0\n",
       "102  0  0  0  0  1\n",
       "\n",
       "[131 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes_dict = {'normal':'1 0 0 0 0', #1\n",
    "        'collision_in_tool':'0 1 0 0 0', #2\n",
    "        'collision_in_part':'0 0 1 0 0', #3\n",
    "        'bottom_collision':'0 0 0 1 0', #4\n",
    "        'bottom_obstruction':'0 0 0 0 1'} #5\n",
    "\n",
    "labels_df = labels_df.replace({'labels':classes_dict})\n",
    "labels_df[['1', '2', '3', '4', '5']] = labels_df['labels'].str.split(' ', 4, expand= True)\n",
    "labels_df = labels_df.loc[:,['1', '2', '3', '4', '5']]\n",
    "labels_df[['1', '2', '3', '4', '5']]=labels_df[['1', '2', '3', '4', '5']].astype(str).astype(int)\n",
    "\n",
    "def norm(x):\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "    range = x_max - x_min  #min max entre 0 y 1\n",
    "    return((x-x_min)/(range))\n",
    "\n",
    "norm_features = norm(features)\n",
    "#split para entrenamiento y validacion\n",
    "train_features, test_features, train_labels, test_labels  = train_test_split(norm_features, labels_df, test_size=0.2, random_state= 42)\n",
    "display(train_features)\n",
    "display(train_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 72 Complete [00h 00m 02s]\n",
      "val_accuracy: 0.3333333432674408\n",
      "\n",
      "Best val_accuracy So Far: 0.39393940567970276\n",
      "Total elapsed time: 00h 02m 15s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 15, 6, 8)          80        \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 15, 6, 8)          584       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 7, 3, 8)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 7, 3, 8)           0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 3, 16)          1168      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 7, 3, 16)          2320      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 3, 1, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 3, 1, 16)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 48)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                1176      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 125       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,453\n",
      "Trainable params: 5,453\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "<keras_tuner.engine.hyperparameters.hyperparameters.HyperParameters object at 0x00000201776A69E0>\n"
     ]
    }
   ],
   "source": [
    "'''def my_model(hp):\n",
    "    inputs = tf.keras.Input(shape=(15,6,1))\n",
    "    x = inputs\n",
    "    for i in range(hp.Int('conv_blocks', 2, 5, default=2)):\n",
    "        filters = hp.Int('filters_' + str(i), 8, 256, step=32) \n",
    "        \n",
    "    for i in range(2):\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel_size= hp.Choice('conv_1_kernel', values = [3,5]), padding= 'same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        \n",
    "    if hp.Choice('pooling_' + str(i), ['avg', 'max']) == 'max': # hp.Choice chooses from a list of values\n",
    "        x = tf.keras.layers.MaxPool2D()(x)\n",
    "    else:\n",
    "        x = tf.keras.layers.AvgPool2D()(x)\n",
    "\n",
    "    x = tf.keras.layers.GlobalAvgPool2D()(x) # apply GlobalAvG Pooling\n",
    "    \n",
    "    # Tune the number of units in the  Dense layer\n",
    "    # Choose an optimal value between min_value to max_value\n",
    "    x = tf.keras.layers.Dense(hp.Int('Dense units',10, 100, step=10, default=50), activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(5, activation= 'softmax')(x) # output layer \n",
    "\n",
    "    # define the model\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    # Tune the learning rate for the optimizer\n",
    "    # Choose an optimal value frommin_value to max_value\n",
    "    model.compile(optimizer= tf.keras.optimizers.Adam(hp.Float('learning_rate',1e-4, 1e-2, sampling='log')), \n",
    "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# initialize tuner to run the model.\n",
    "# using the Hyperband search algorithm\n",
    "tuner = kt.Hyperband(\n",
    "    hypermodel = my_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=30,\n",
    "    hyperband_iterations=2,\n",
    "    directory=\"Keras_tuner_dir\",\n",
    "    project_name=\"Keras_tuner_Demo\")\n",
    "\n",
    "# Run the search\n",
    "tuner.search(train_features, train_labels,\n",
    "            validation_data= (test_features,test_labels), \n",
    "            epochs=30,\n",
    "            callbacks=[tf.keras.callbacks.EarlyStopping(patience=2)])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps= tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "# get the best model\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "\n",
    "# show model summary\n",
    "best_model.summary()'''\n",
    "def my_model(hp):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(8, (3,3), activation='relu', input_shape = (15,6,1), padding='same'), # #de feature maps, dimensiones del kernel, kernel entre más pequeño mejor y se prefiere un número impar\n",
    "        tf.keras.layers.Conv2D(8, (3,3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D((2,2), strides= 2, padding= 'valid'), # dimensiones del pooling\n",
    "        tf.keras.layers.Dropout(hp.Float('dropout1',0.1, 0.5, step = 0.05)),\n",
    "        tf.keras.layers.Conv2D(16, (3,3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.Conv2D(16, (3,3), activation='relu', padding='same'),# #de feature maps, dimensiones del kernel, kernel entre más pequeño mejor y se prefiere un número impar\n",
    "        tf.keras.layers.MaxPooling2D((2,2), strides= 2, padding= 'valid'), # dimensiones del pooling\n",
    "        tf.keras.layers.Dropout(hp.Float('dropout2',0.1, 0.5, step = 0.051)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(24, activation='relu'),\n",
    "        tf.keras.layers.Dense(5, activation='softmax')])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "tuner = kt.GridSearch(\n",
    "    hypermodel = my_model,\n",
    "    objective='val_accuracy',\n",
    "    directory=\"Tuner_Conv\",\n",
    "    project_name=\"Tarea1_B\",\n",
    "    overwrite = True)\n",
    "\n",
    "tuner.search(train_features, train_labels,\n",
    "            validation_data= (test_features,test_labels), \n",
    "            epochs=50,\n",
    "            callbacks=[tf.keras.callbacks.EarlyStopping(patience=2)])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps= tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "# get the best model\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "\n",
    "# show model summary\n",
    "best_model.summary()\n",
    "print(best_hps)\n",
    "\n",
    "model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, features, labels, epochs, batch_size):\n",
    "    history = model.fit(\n",
    "        x = features,\n",
    "        y = labels,\n",
    "        epochs= epochs,\n",
    "        batch_size= batch_size,\n",
    "        validation_split= 0.25\n",
    "    )\n",
    "    hist= pd.DataFrame(history.history) #se guardan los valores de errores y metricas en un diccionario\n",
    "    hist['epoch'] = history.epoch #los epochs se deben añadir aparte\n",
    "    return hist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizaciones\n",
    "### Pasos para visualizar los filtros:\n",
    "##### 1. Iterar por todas las capas del modelo usando model.layers\n",
    "##### 2. Si la capa actual es de convolucion se extraen los pesos y sesgos usando get_weights()\n",
    "##### 3. Se normalizan los pesos de los filtros entre 0 y 1\n",
    "##### 4. Se plotean los filtros para cada capa convolutional y todos los canales de color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_curves(history):\n",
    "    hist = history\n",
    "    labels = {\"loss\":\"Training Loss\", \"val_loss\":\"Validation Loss\"}\n",
    "    hist.rename(columns = labels, inplace = True)\n",
    "    \n",
    "    fig = px.line(hist, x='epoch', y=['Training Loss', 'Validation Loss'],\n",
    "                title='Gráficas de Pérdida de Entrenamiento y Evaluación',\n",
    "                labels={\"epoch\": \"Epoch\", \"value\":\"Binary Cross Entropy\", \"variable\":\"Curvas de Pérdida\"},\n",
    "                color_discrete_map={ # replaces default color mapping by value\n",
    "                \"Training Loss\": \"#46039f\", \"Validation Loss\": \"#fb9f3a\"})\n",
    "    fig.update_layout(template='plotly_white')\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se corren las funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.2039 - accuracy: 0.7857 - val_loss: 0.3580 - val_accuracy: 0.6970\n",
      "Epoch 2/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.2060 - accuracy: 0.7857 - val_loss: 0.3183 - val_accuracy: 0.6970\n",
      "Epoch 3/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1722 - accuracy: 0.8163 - val_loss: 0.3764 - val_accuracy: 0.6364\n",
      "Epoch 4/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1851 - accuracy: 0.7755 - val_loss: 0.2816 - val_accuracy: 0.7576\n",
      "Epoch 5/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1899 - accuracy: 0.7857 - val_loss: 0.2971 - val_accuracy: 0.7576\n",
      "Epoch 6/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1843 - accuracy: 0.7857 - val_loss: 0.3122 - val_accuracy: 0.6970\n",
      "Epoch 7/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1883 - accuracy: 0.8163 - val_loss: 0.3334 - val_accuracy: 0.6970\n",
      "Epoch 8/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1639 - accuracy: 0.8367 - val_loss: 0.3707 - val_accuracy: 0.6667\n",
      "Epoch 9/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2073 - accuracy: 0.7347 - val_loss: 0.3142 - val_accuracy: 0.6970\n",
      "Epoch 10/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1680 - accuracy: 0.8163 - val_loss: 0.4336 - val_accuracy: 0.5758\n",
      "Epoch 11/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1787 - accuracy: 0.8163 - val_loss: 0.3603 - val_accuracy: 0.6970\n",
      "Epoch 12/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2101 - accuracy: 0.7551 - val_loss: 0.3284 - val_accuracy: 0.6970\n",
      "Epoch 13/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2005 - accuracy: 0.7857 - val_loss: 0.2968 - val_accuracy: 0.6667\n",
      "Epoch 14/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2240 - accuracy: 0.7449 - val_loss: 0.3368 - val_accuracy: 0.6061\n",
      "Epoch 15/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2309 - accuracy: 0.7143 - val_loss: 0.3352 - val_accuracy: 0.6667\n",
      "Epoch 16/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1942 - accuracy: 0.8265 - val_loss: 0.3205 - val_accuracy: 0.6667\n",
      "Epoch 17/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2002 - accuracy: 0.7755 - val_loss: 0.3179 - val_accuracy: 0.6364\n",
      "Epoch 18/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1805 - accuracy: 0.8265 - val_loss: 0.3176 - val_accuracy: 0.6364\n",
      "Epoch 19/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2021 - accuracy: 0.7755 - val_loss: 0.3272 - val_accuracy: 0.6970\n",
      "Epoch 20/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1986 - accuracy: 0.7755 - val_loss: 0.3519 - val_accuracy: 0.6970\n",
      "Epoch 21/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1964 - accuracy: 0.7959 - val_loss: 0.3009 - val_accuracy: 0.6364\n",
      "Epoch 22/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.2556 - accuracy: 0.7245 - val_loss: 0.3686 - val_accuracy: 0.6970\n",
      "Epoch 23/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2030 - accuracy: 0.7755 - val_loss: 0.3067 - val_accuracy: 0.5758\n",
      "Epoch 24/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1821 - accuracy: 0.7959 - val_loss: 0.3990 - val_accuracy: 0.6667\n",
      "Epoch 25/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.2106 - accuracy: 0.7347 - val_loss: 0.3592 - val_accuracy: 0.6970\n",
      "Epoch 26/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1591 - accuracy: 0.8163 - val_loss: 0.3803 - val_accuracy: 0.6667\n",
      "Epoch 27/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1990 - accuracy: 0.7755 - val_loss: 0.3250 - val_accuracy: 0.6667\n",
      "Epoch 28/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1602 - accuracy: 0.8367 - val_loss: 0.3230 - val_accuracy: 0.6667\n",
      "Epoch 29/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1729 - accuracy: 0.7959 - val_loss: 0.3534 - val_accuracy: 0.6667\n",
      "Epoch 30/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1995 - accuracy: 0.8163 - val_loss: 0.3731 - val_accuracy: 0.6667\n",
      "Epoch 31/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1787 - accuracy: 0.7959 - val_loss: 0.3354 - val_accuracy: 0.6667\n",
      "Epoch 32/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1862 - accuracy: 0.7959 - val_loss: 0.3176 - val_accuracy: 0.6667\n",
      "Epoch 33/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1785 - accuracy: 0.7755 - val_loss: 0.2990 - val_accuracy: 0.6970\n",
      "Epoch 34/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1816 - accuracy: 0.8265 - val_loss: 0.3880 - val_accuracy: 0.6364\n",
      "Epoch 35/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2279 - accuracy: 0.7653 - val_loss: 0.3570 - val_accuracy: 0.6970\n",
      "Epoch 36/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2054 - accuracy: 0.7347 - val_loss: 0.3234 - val_accuracy: 0.6667\n",
      "Epoch 37/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1865 - accuracy: 0.8163 - val_loss: 0.3459 - val_accuracy: 0.6970\n",
      "Epoch 38/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1826 - accuracy: 0.8061 - val_loss: 0.3305 - val_accuracy: 0.6970\n",
      "Epoch 39/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1546 - accuracy: 0.8571 - val_loss: 0.3300 - val_accuracy: 0.6970\n",
      "Epoch 40/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1733 - accuracy: 0.7959 - val_loss: 0.3465 - val_accuracy: 0.6364\n",
      "Epoch 41/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1784 - accuracy: 0.8061 - val_loss: 0.3224 - val_accuracy: 0.6970\n",
      "Epoch 42/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2180 - accuracy: 0.7551 - val_loss: 0.3647 - val_accuracy: 0.6667\n",
      "Epoch 43/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.7857 - val_loss: 0.3092 - val_accuracy: 0.7273\n",
      "Epoch 44/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1949 - accuracy: 0.8163 - val_loss: 0.2738 - val_accuracy: 0.6667\n",
      "Epoch 45/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2017 - accuracy: 0.7857 - val_loss: 0.3213 - val_accuracy: 0.6364\n",
      "Epoch 46/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1510 - accuracy: 0.8571 - val_loss: 0.3172 - val_accuracy: 0.7273\n",
      "Epoch 47/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1880 - accuracy: 0.8061 - val_loss: 0.3337 - val_accuracy: 0.6667\n",
      "Epoch 48/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2085 - accuracy: 0.7449 - val_loss: 0.3027 - val_accuracy: 0.6970\n",
      "Epoch 49/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2024 - accuracy: 0.7959 - val_loss: 0.3514 - val_accuracy: 0.6364\n",
      "Epoch 50/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1928 - accuracy: 0.7755 - val_loss: 0.3255 - val_accuracy: 0.6667\n",
      "Epoch 51/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1973 - accuracy: 0.7449 - val_loss: 0.2952 - val_accuracy: 0.6364\n",
      "Epoch 52/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2152 - accuracy: 0.7755 - val_loss: 0.3606 - val_accuracy: 0.6061\n",
      "Epoch 53/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2492 - accuracy: 0.7143 - val_loss: 0.2997 - val_accuracy: 0.6667\n",
      "Epoch 54/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2247 - accuracy: 0.7551 - val_loss: 0.3015 - val_accuracy: 0.7273\n",
      "Epoch 55/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2043 - accuracy: 0.7857 - val_loss: 0.3284 - val_accuracy: 0.6667\n",
      "Epoch 56/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1731 - accuracy: 0.8061 - val_loss: 0.3005 - val_accuracy: 0.7273\n",
      "Epoch 57/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1570 - accuracy: 0.8367 - val_loss: 0.3622 - val_accuracy: 0.6667\n",
      "Epoch 58/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1809 - accuracy: 0.8061 - val_loss: 0.3325 - val_accuracy: 0.6970\n",
      "Epoch 59/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2032 - accuracy: 0.7551 - val_loss: 0.2809 - val_accuracy: 0.6364\n",
      "Epoch 60/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1855 - accuracy: 0.8061 - val_loss: 0.3748 - val_accuracy: 0.6970\n",
      "Epoch 61/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2302 - accuracy: 0.7449 - val_loss: 0.3461 - val_accuracy: 0.6970\n",
      "Epoch 62/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1700 - accuracy: 0.8367 - val_loss: 0.3165 - val_accuracy: 0.7576\n",
      "Epoch 63/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2381 - accuracy: 0.7347 - val_loss: 0.3579 - val_accuracy: 0.6364\n",
      "Epoch 64/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1918 - accuracy: 0.7959 - val_loss: 0.3279 - val_accuracy: 0.6970\n",
      "Epoch 65/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1908 - accuracy: 0.7959 - val_loss: 0.3366 - val_accuracy: 0.6970\n",
      "Epoch 66/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1627 - accuracy: 0.8571 - val_loss: 0.3267 - val_accuracy: 0.7273\n",
      "Epoch 67/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2048 - accuracy: 0.7857 - val_loss: 0.3344 - val_accuracy: 0.6667\n",
      "Epoch 68/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2013 - accuracy: 0.7959 - val_loss: 0.3272 - val_accuracy: 0.6667\n",
      "Epoch 69/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1681 - accuracy: 0.8163 - val_loss: 0.3188 - val_accuracy: 0.6970\n",
      "Epoch 70/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2215 - accuracy: 0.7347 - val_loss: 0.3157 - val_accuracy: 0.6667\n",
      "Epoch 71/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1796 - accuracy: 0.8265 - val_loss: 0.3475 - val_accuracy: 0.6667\n",
      "Epoch 72/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1991 - accuracy: 0.7755 - val_loss: 0.3426 - val_accuracy: 0.6061\n",
      "Epoch 73/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2038 - accuracy: 0.7653 - val_loss: 0.3946 - val_accuracy: 0.6364\n",
      "Epoch 74/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1928 - accuracy: 0.8163 - val_loss: 0.3683 - val_accuracy: 0.6970\n",
      "Epoch 75/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1781 - accuracy: 0.8265 - val_loss: 0.3842 - val_accuracy: 0.6364\n",
      "Epoch 76/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2047 - accuracy: 0.7959 - val_loss: 0.3793 - val_accuracy: 0.6364\n",
      "Epoch 77/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1666 - accuracy: 0.8163 - val_loss: 0.3382 - val_accuracy: 0.6970\n",
      "Epoch 78/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.2289 - accuracy: 0.7755 - val_loss: 0.3537 - val_accuracy: 0.6667\n",
      "Epoch 79/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1836 - accuracy: 0.8265 - val_loss: 0.3928 - val_accuracy: 0.6364\n",
      "Epoch 80/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1704 - accuracy: 0.8061 - val_loss: 0.3540 - val_accuracy: 0.6970\n",
      "Epoch 81/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1711 - accuracy: 0.7959 - val_loss: 0.3675 - val_accuracy: 0.6970\n",
      "Epoch 82/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1795 - accuracy: 0.7959 - val_loss: 0.3477 - val_accuracy: 0.6667\n",
      "Epoch 83/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1695 - accuracy: 0.8571 - val_loss: 0.3229 - val_accuracy: 0.6667\n",
      "Epoch 84/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1799 - accuracy: 0.7959 - val_loss: 0.3519 - val_accuracy: 0.7273\n",
      "Epoch 85/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1615 - accuracy: 0.8061 - val_loss: 0.3550 - val_accuracy: 0.6667\n",
      "Epoch 86/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2046 - accuracy: 0.7755 - val_loss: 0.3388 - val_accuracy: 0.6970\n",
      "Epoch 87/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1597 - accuracy: 0.8265 - val_loss: 0.3436 - val_accuracy: 0.6667\n",
      "Epoch 88/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1739 - accuracy: 0.7959 - val_loss: 0.3594 - val_accuracy: 0.6061\n",
      "Epoch 89/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2129 - accuracy: 0.7755 - val_loss: 0.3474 - val_accuracy: 0.6667\n",
      "Epoch 90/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1486 - accuracy: 0.8469 - val_loss: 0.3623 - val_accuracy: 0.7273\n",
      "Epoch 91/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1888 - accuracy: 0.7857 - val_loss: 0.3379 - val_accuracy: 0.6667\n",
      "Epoch 92/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1948 - accuracy: 0.7551 - val_loss: 0.3634 - val_accuracy: 0.6667\n",
      "Epoch 93/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1772 - accuracy: 0.8061 - val_loss: 0.3625 - val_accuracy: 0.6970\n",
      "Epoch 94/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1965 - accuracy: 0.7857 - val_loss: 0.3624 - val_accuracy: 0.6364\n",
      "Epoch 95/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1628 - accuracy: 0.8061 - val_loss: 0.3237 - val_accuracy: 0.6061\n",
      "Epoch 96/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.3680 - accuracy: 0.5510 - val_loss: 0.3082 - val_accuracy: 0.6061\n",
      "Epoch 97/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1784 - accuracy: 0.8265 - val_loss: 0.3616 - val_accuracy: 0.6970\n",
      "Epoch 98/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1882 - accuracy: 0.7959 - val_loss: 0.3722 - val_accuracy: 0.6667\n",
      "Epoch 99/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1872 - accuracy: 0.8163 - val_loss: 0.3251 - val_accuracy: 0.6970\n",
      "Epoch 100/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1600 - accuracy: 0.8367 - val_loss: 0.3310 - val_accuracy: 0.7273\n",
      "Epoch 101/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1635 - accuracy: 0.8061 - val_loss: 0.3312 - val_accuracy: 0.6667\n",
      "Epoch 102/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1916 - accuracy: 0.7347 - val_loss: 0.3135 - val_accuracy: 0.7273\n",
      "Epoch 103/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2032 - accuracy: 0.7755 - val_loss: 0.3302 - val_accuracy: 0.6364\n",
      "Epoch 104/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1917 - accuracy: 0.7959 - val_loss: 0.3634 - val_accuracy: 0.6667\n",
      "Epoch 105/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1544 - accuracy: 0.8163 - val_loss: 0.3244 - val_accuracy: 0.6970\n",
      "Epoch 106/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1865 - accuracy: 0.7653 - val_loss: 0.4084 - val_accuracy: 0.6364\n",
      "Epoch 107/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2096 - accuracy: 0.7653 - val_loss: 0.3911 - val_accuracy: 0.6364\n",
      "Epoch 108/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1761 - accuracy: 0.8061 - val_loss: 0.3683 - val_accuracy: 0.6364\n",
      "Epoch 109/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1894 - accuracy: 0.8061 - val_loss: 0.3371 - val_accuracy: 0.6667\n",
      "Epoch 110/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1781 - accuracy: 0.8265 - val_loss: 0.3292 - val_accuracy: 0.6667\n",
      "Epoch 111/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1723 - accuracy: 0.8265 - val_loss: 0.3547 - val_accuracy: 0.6667\n",
      "Epoch 112/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1770 - accuracy: 0.8163 - val_loss: 0.2868 - val_accuracy: 0.6970\n",
      "Epoch 113/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1839 - accuracy: 0.8061 - val_loss: 0.3312 - val_accuracy: 0.6667\n",
      "Epoch 114/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1545 - accuracy: 0.8469 - val_loss: 0.3140 - val_accuracy: 0.6667\n",
      "Epoch 115/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.2448 - accuracy: 0.7347 - val_loss: 0.3524 - val_accuracy: 0.5758\n",
      "Epoch 116/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1931 - accuracy: 0.7857 - val_loss: 0.3268 - val_accuracy: 0.6667\n",
      "Epoch 117/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1917 - accuracy: 0.7857 - val_loss: 0.3082 - val_accuracy: 0.6364\n",
      "Epoch 118/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1417 - accuracy: 0.8571 - val_loss: 0.3295 - val_accuracy: 0.6667\n",
      "Epoch 119/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1827 - accuracy: 0.7857 - val_loss: 0.3803 - val_accuracy: 0.6667\n",
      "Epoch 120/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1701 - accuracy: 0.8265 - val_loss: 0.3384 - val_accuracy: 0.6970\n",
      "Epoch 121/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1826 - accuracy: 0.8163 - val_loss: 0.3878 - val_accuracy: 0.6667\n",
      "Epoch 122/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2753 - accuracy: 0.7143 - val_loss: 0.3341 - val_accuracy: 0.6970\n",
      "Epoch 123/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1697 - accuracy: 0.8163 - val_loss: 0.4093 - val_accuracy: 0.6667\n",
      "Epoch 124/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1965 - accuracy: 0.7959 - val_loss: 0.4275 - val_accuracy: 0.6970\n",
      "Epoch 125/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1584 - accuracy: 0.8469 - val_loss: 0.3769 - val_accuracy: 0.6667\n",
      "Epoch 126/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1721 - accuracy: 0.8265 - val_loss: 0.3723 - val_accuracy: 0.6667\n",
      "Epoch 127/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.3028 - accuracy: 0.7755 - val_loss: 0.3587 - val_accuracy: 0.7273\n",
      "Epoch 128/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2031 - accuracy: 0.7551 - val_loss: 0.3682 - val_accuracy: 0.5455\n",
      "Epoch 129/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1739 - accuracy: 0.8061 - val_loss: 0.3315 - val_accuracy: 0.6364\n",
      "Epoch 130/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1598 - accuracy: 0.8469 - val_loss: 0.3208 - val_accuracy: 0.6970\n",
      "Epoch 131/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1664 - accuracy: 0.8469 - val_loss: 0.3133 - val_accuracy: 0.7273\n",
      "Epoch 132/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1578 - accuracy: 0.8061 - val_loss: 0.3470 - val_accuracy: 0.6970\n",
      "Epoch 133/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.7857 - val_loss: 0.3133 - val_accuracy: 0.6970\n",
      "Epoch 134/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1923 - accuracy: 0.7857 - val_loss: 0.3215 - val_accuracy: 0.6970\n",
      "Epoch 135/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1795 - accuracy: 0.7755 - val_loss: 0.3793 - val_accuracy: 0.6364\n",
      "Epoch 136/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1668 - accuracy: 0.8367 - val_loss: 0.3848 - val_accuracy: 0.6970\n",
      "Epoch 137/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.2222 - accuracy: 0.7449 - val_loss: 0.3572 - val_accuracy: 0.6970\n",
      "Epoch 138/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1577 - accuracy: 0.8367 - val_loss: 0.3309 - val_accuracy: 0.6970\n",
      "Epoch 139/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1780 - accuracy: 0.8061 - val_loss: 0.3282 - val_accuracy: 0.6970\n",
      "Epoch 140/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1446 - accuracy: 0.8367 - val_loss: 0.3664 - val_accuracy: 0.6061\n",
      "Epoch 141/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1738 - accuracy: 0.8061 - val_loss: 0.3505 - val_accuracy: 0.6667\n",
      "Epoch 142/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1962 - accuracy: 0.7653 - val_loss: 0.3258 - val_accuracy: 0.6970\n",
      "Epoch 143/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1593 - accuracy: 0.8061 - val_loss: 0.3613 - val_accuracy: 0.6061\n",
      "Epoch 144/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1643 - accuracy: 0.8163 - val_loss: 0.3495 - val_accuracy: 0.6970\n",
      "Epoch 145/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1660 - accuracy: 0.8367 - val_loss: 0.3374 - val_accuracy: 0.6364\n",
      "Epoch 146/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1841 - accuracy: 0.8061 - val_loss: 0.3385 - val_accuracy: 0.6667\n",
      "Epoch 147/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1926 - accuracy: 0.7959 - val_loss: 0.3660 - val_accuracy: 0.6970\n",
      "Epoch 148/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2022 - accuracy: 0.7551 - val_loss: 0.3620 - val_accuracy: 0.6364\n",
      "Epoch 149/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1866 - accuracy: 0.8061 - val_loss: 0.3392 - val_accuracy: 0.6970\n",
      "Epoch 150/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1729 - accuracy: 0.8265 - val_loss: 0.3628 - val_accuracy: 0.6667\n",
      "Epoch 151/200\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.1756 - accuracy: 0.8265 - val_loss: 0.3539 - val_accuracy: 0.6970\n",
      "Epoch 152/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1697 - accuracy: 0.8265 - val_loss: 0.3653 - val_accuracy: 0.6970\n",
      "Epoch 153/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1839 - accuracy: 0.8367 - val_loss: 0.3611 - val_accuracy: 0.6667\n",
      "Epoch 154/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1924 - accuracy: 0.7755 - val_loss: 0.3594 - val_accuracy: 0.7273\n",
      "Epoch 155/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2082 - accuracy: 0.7653 - val_loss: 0.4085 - val_accuracy: 0.6667\n",
      "Epoch 156/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1764 - accuracy: 0.8061 - val_loss: 0.3504 - val_accuracy: 0.6667\n",
      "Epoch 157/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1713 - accuracy: 0.8163 - val_loss: 0.3442 - val_accuracy: 0.6667\n",
      "Epoch 158/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2201 - accuracy: 0.7755 - val_loss: 0.3367 - val_accuracy: 0.6667\n",
      "Epoch 159/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1819 - accuracy: 0.8265 - val_loss: 0.3826 - val_accuracy: 0.6364\n",
      "Epoch 160/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1715 - accuracy: 0.8265 - val_loss: 0.3373 - val_accuracy: 0.6970\n",
      "Epoch 161/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1965 - accuracy: 0.7857 - val_loss: 0.3664 - val_accuracy: 0.6061\n",
      "Epoch 162/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1722 - accuracy: 0.8163 - val_loss: 0.3704 - val_accuracy: 0.6667\n",
      "Epoch 163/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1779 - accuracy: 0.8367 - val_loss: 0.3726 - val_accuracy: 0.6364\n",
      "Epoch 164/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1708 - accuracy: 0.8265 - val_loss: 0.3655 - val_accuracy: 0.6061\n",
      "Epoch 165/200\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.1679 - accuracy: 0.7857 - val_loss: 0.3602 - val_accuracy: 0.6364\n",
      "Epoch 166/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1477 - accuracy: 0.8878 - val_loss: 0.3834 - val_accuracy: 0.6364\n",
      "Epoch 167/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2012 - accuracy: 0.7857 - val_loss: 0.3283 - val_accuracy: 0.6667\n",
      "Epoch 168/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1770 - accuracy: 0.7857 - val_loss: 0.3406 - val_accuracy: 0.6667\n",
      "Epoch 169/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1949 - accuracy: 0.7857 - val_loss: 0.3589 - val_accuracy: 0.6970\n",
      "Epoch 170/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1678 - accuracy: 0.8061 - val_loss: 0.3617 - val_accuracy: 0.6667\n",
      "Epoch 171/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1585 - accuracy: 0.8265 - val_loss: 0.3296 - val_accuracy: 0.6970\n",
      "Epoch 172/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1536 - accuracy: 0.7857 - val_loss: 0.3758 - val_accuracy: 0.6667\n",
      "Epoch 173/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1494 - accuracy: 0.8367 - val_loss: 0.3911 - val_accuracy: 0.6364\n",
      "Epoch 174/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1636 - accuracy: 0.8265 - val_loss: 0.3882 - val_accuracy: 0.6970\n",
      "Epoch 175/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1884 - accuracy: 0.8061 - val_loss: 0.3810 - val_accuracy: 0.6061\n",
      "Epoch 176/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1712 - accuracy: 0.8061 - val_loss: 0.3527 - val_accuracy: 0.6364\n",
      "Epoch 177/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1855 - accuracy: 0.8265 - val_loss: 0.3512 - val_accuracy: 0.6364\n",
      "Epoch 178/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1794 - accuracy: 0.7755 - val_loss: 0.3752 - val_accuracy: 0.6667\n",
      "Epoch 179/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1761 - accuracy: 0.7959 - val_loss: 0.3923 - val_accuracy: 0.6364\n",
      "Epoch 180/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1912 - accuracy: 0.7755 - val_loss: 0.3451 - val_accuracy: 0.6970\n",
      "Epoch 181/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1621 - accuracy: 0.8163 - val_loss: 0.3352 - val_accuracy: 0.6667\n",
      "Epoch 182/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.2057 - accuracy: 0.7959 - val_loss: 0.3818 - val_accuracy: 0.6667\n",
      "Epoch 183/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1846 - accuracy: 0.7959 - val_loss: 0.4305 - val_accuracy: 0.6364\n",
      "Epoch 184/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1822 - accuracy: 0.7959 - val_loss: 0.3585 - val_accuracy: 0.6667\n",
      "Epoch 185/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1954 - accuracy: 0.7653 - val_loss: 0.3971 - val_accuracy: 0.5758\n",
      "Epoch 186/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2077 - accuracy: 0.7857 - val_loss: 0.3649 - val_accuracy: 0.6364\n",
      "Epoch 187/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1972 - accuracy: 0.7551 - val_loss: 0.4117 - val_accuracy: 0.6970\n",
      "Epoch 188/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2455 - accuracy: 0.7449 - val_loss: 0.3509 - val_accuracy: 0.6061\n",
      "Epoch 189/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1915 - accuracy: 0.7959 - val_loss: 0.3387 - val_accuracy: 0.6970\n",
      "Epoch 190/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1959 - accuracy: 0.7755 - val_loss: 0.3399 - val_accuracy: 0.7273\n",
      "Epoch 191/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1580 - accuracy: 0.8367 - val_loss: 0.3496 - val_accuracy: 0.6667\n",
      "Epoch 192/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1624 - accuracy: 0.8061 - val_loss: 0.3714 - val_accuracy: 0.6667\n",
      "Epoch 193/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1938 - accuracy: 0.8061 - val_loss: 0.3735 - val_accuracy: 0.6970\n",
      "Epoch 194/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1696 - accuracy: 0.8367 - val_loss: 0.3792 - val_accuracy: 0.6667\n",
      "Epoch 195/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1670 - accuracy: 0.7959 - val_loss: 0.3843 - val_accuracy: 0.6970\n",
      "Epoch 196/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2438 - accuracy: 0.7449 - val_loss: 0.3775 - val_accuracy: 0.6061\n",
      "Epoch 197/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1958 - accuracy: 0.7755 - val_loss: 0.3475 - val_accuracy: 0.6364\n",
      "Epoch 198/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1749 - accuracy: 0.7959 - val_loss: 0.3710 - val_accuracy: 0.6667\n",
      "Epoch 199/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1710 - accuracy: 0.8367 - val_loss: 0.3542 - val_accuracy: 0.6970\n",
      "Epoch 200/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2031 - accuracy: 0.7857 - val_loss: 0.3287 - val_accuracy: 0.6667\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.204</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.206</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.697</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.172</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.636</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.185</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.758</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.190</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.758</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.244</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.606</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.196</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.636</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.175</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.667</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.171</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.697</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.203</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.667</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     loss  accuracy  val_loss  val_accuracy  epoch\n",
       "0   0.204     0.786     0.358         0.697      0\n",
       "1   0.206     0.786     0.318         0.697      1\n",
       "2   0.172     0.816     0.376         0.636      2\n",
       "3   0.185     0.776     0.282         0.758      3\n",
       "4   0.190     0.786     0.297         0.758      4\n",
       "..    ...       ...       ...           ...    ...\n",
       "195 0.244     0.745     0.378         0.606    195\n",
       "196 0.196     0.776     0.348         0.636    196\n",
       "197 0.175     0.796     0.371         0.667    197\n",
       "198 0.171     0.837     0.354         0.697    198\n",
       "199 0.203     0.786     0.329         0.667    199\n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Curvas de Pérdida=Training Loss<br>Epoch=%{x}<br>Binary Cross Entropy=%{y}<extra></extra>",
         "legendgroup": "Training Loss",
         "line": {
          "color": "#46039f",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Training Loss",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "xaxis": "x",
         "y": [
          0.20394456386566162,
          0.20599773526191711,
          0.17220871150493622,
          0.18505807220935822,
          0.18994294106960297,
          0.1843327134847641,
          0.18834199011325836,
          0.16392549872398376,
          0.20731449127197266,
          0.16800321638584137,
          0.17867609858512878,
          0.2100946456193924,
          0.2004956156015396,
          0.22398504614830017,
          0.23091979324817657,
          0.19416861236095428,
          0.20024529099464417,
          0.18054696917533875,
          0.20205706357955933,
          0.19863028824329376,
          0.1964241862297058,
          0.2556321620941162,
          0.20297031104564667,
          0.1820603907108307,
          0.21064166724681854,
          0.1591137796640396,
          0.19895273447036743,
          0.16015076637268066,
          0.17293477058410645,
          0.19945476949214935,
          0.17868086695671082,
          0.1862298995256424,
          0.1785043627023697,
          0.18160869181156158,
          0.22794808447360992,
          0.205417200922966,
          0.18645831942558289,
          0.18262411653995514,
          0.15459182858467102,
          0.1733422577381134,
          0.1783933788537979,
          0.21799685060977936,
          0.18631386756896973,
          0.19486533105373383,
          0.20166075229644775,
          0.15102258324623108,
          0.1879899799823761,
          0.20847658812999725,
          0.20235182344913483,
          0.19282054901123047,
          0.1973324567079544,
          0.21515072882175446,
          0.2492111176252365,
          0.22469958662986755,
          0.20431733131408691,
          0.1731427013874054,
          0.15695001184940338,
          0.1808534413576126,
          0.2032373547554016,
          0.18553860485553741,
          0.2301943004131317,
          0.17004014551639557,
          0.23810429871082306,
          0.19175027310848236,
          0.19080528616905212,
          0.16268128156661987,
          0.20481428503990173,
          0.20125235617160797,
          0.16807302832603455,
          0.22152499854564667,
          0.17961163818836212,
          0.1990790069103241,
          0.20381946861743927,
          0.1928044557571411,
          0.1781018078327179,
          0.2047065794467926,
          0.16657231748104095,
          0.22888821363449097,
          0.1835557520389557,
          0.17041905224323273,
          0.1710856556892395,
          0.1794825941324234,
          0.16949568688869476,
          0.17987507581710815,
          0.16146144270896912,
          0.2045777589082718,
          0.15966655313968658,
          0.17385616898536682,
          0.21285827457904816,
          0.14860284328460693,
          0.18875230848789215,
          0.1948392391204834,
          0.17715483903884888,
          0.1965147703886032,
          0.16277089715003967,
          0.36801809072494507,
          0.17838403582572937,
          0.18818219006061554,
          0.18715542554855347,
          0.16004543006420135,
          0.16347187757492065,
          0.19163352251052856,
          0.20319044589996338,
          0.19165337085723877,
          0.154445618391037,
          0.18652835488319397,
          0.20958112180233002,
          0.17611554265022278,
          0.18940584361553192,
          0.1780756115913391,
          0.17229190468788147,
          0.17695653438568115,
          0.18386556208133698,
          0.15453490614891052,
          0.24482400715351105,
          0.19306680560112,
          0.19169430434703827,
          0.14169085025787354,
          0.1827162653207779,
          0.17013338208198547,
          0.18262581527233124,
          0.2752523422241211,
          0.1697172075510025,
          0.1964876800775528,
          0.15838350355625153,
          0.17207036912441254,
          0.3028038740158081,
          0.2030767947435379,
          0.17388303577899933,
          0.15981997549533844,
          0.16642196476459503,
          0.15775053203105927,
          0.1863456517457962,
          0.19231314957141876,
          0.17951799929141998,
          0.16675984859466553,
          0.2221612185239792,
          0.15766362845897675,
          0.17802675068378448,
          0.1445746272802353,
          0.17379523813724518,
          0.19623711705207825,
          0.15932030975818634,
          0.16430023312568665,
          0.16599123179912567,
          0.18409734964370728,
          0.1926482915878296,
          0.20215579867362976,
          0.1866297572851181,
          0.17289726436138153,
          0.17559659481048584,
          0.16971099376678467,
          0.1838764250278473,
          0.19240355491638184,
          0.20816156268119812,
          0.17642852663993835,
          0.17128382623195648,
          0.22009921073913574,
          0.18193943798542023,
          0.1714821606874466,
          0.19645489752292633,
          0.1721583604812622,
          0.17792384326457977,
          0.1707882136106491,
          0.1679321527481079,
          0.14768792688846588,
          0.2011847347021103,
          0.17695826292037964,
          0.19487635791301727,
          0.16778501868247986,
          0.15846043825149536,
          0.1536376178264618,
          0.14935867488384247,
          0.16355006396770477,
          0.18839043378829956,
          0.17116808891296387,
          0.1854555606842041,
          0.1794324666261673,
          0.17608146369457245,
          0.19119912385940552,
          0.16212299466133118,
          0.20566527545452118,
          0.18462005257606506,
          0.18222267925739288,
          0.19540750980377197,
          0.20769107341766357,
          0.19718070328235626,
          0.24547536671161652,
          0.19154302775859833,
          0.1959252655506134,
          0.15802425146102905,
          0.16236551105976105,
          0.1937597692012787,
          0.16960228979587555,
          0.16701741516590118,
          0.2437891811132431,
          0.1957559585571289,
          0.1749201864004135,
          0.17100141942501068,
          0.20309549570083618
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Curvas de Pérdida=Validation Loss<br>Epoch=%{x}<br>Binary Cross Entropy=%{y}<extra></extra>",
         "legendgroup": "Validation Loss",
         "line": {
          "color": "#fb9f3a",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Validation Loss",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "xaxis": "x",
         "y": [
          0.35797351598739624,
          0.3182930648326874,
          0.3764013350009918,
          0.28163614869117737,
          0.29713889956474304,
          0.31216341257095337,
          0.3333609700202942,
          0.370731920003891,
          0.31424349546432495,
          0.43361979722976685,
          0.36032503843307495,
          0.3283505439758301,
          0.29678404331207275,
          0.336825430393219,
          0.3351626396179199,
          0.3204748034477234,
          0.3179144561290741,
          0.3175884485244751,
          0.3272494971752167,
          0.351923406124115,
          0.3009280264377594,
          0.3685811460018158,
          0.3067062199115753,
          0.3989821672439575,
          0.35915905237197876,
          0.38028955459594727,
          0.3250364363193512,
          0.3229970335960388,
          0.3533613383769989,
          0.37305301427841187,
          0.3354400396347046,
          0.3176066279411316,
          0.2990158498287201,
          0.3879743814468384,
          0.35703396797180176,
          0.3234178423881531,
          0.3459487855434418,
          0.3305034637451172,
          0.32995468378067017,
          0.34649455547332764,
          0.322393536567688,
          0.36472389101982117,
          0.30915966629981995,
          0.27381396293640137,
          0.3213486671447754,
          0.31717967987060547,
          0.3337324857711792,
          0.3027086853981018,
          0.351407915353775,
          0.3254549205303192,
          0.2952451705932617,
          0.3605769872665405,
          0.29972273111343384,
          0.30146241188049316,
          0.3284442722797394,
          0.30050671100616455,
          0.36224228143692017,
          0.3325499892234802,
          0.28093430399894714,
          0.3747744858264923,
          0.346097469329834,
          0.3164536952972412,
          0.3578788638114929,
          0.32785892486572266,
          0.33656764030456543,
          0.3266620337963104,
          0.3344457149505615,
          0.32721230387687683,
          0.3188108205795288,
          0.3156717121601105,
          0.34749728441238403,
          0.34264543652534485,
          0.39456483721733093,
          0.3682616949081421,
          0.38416579365730286,
          0.3792552947998047,
          0.3382025957107544,
          0.35365819931030273,
          0.39277976751327515,
          0.35396838188171387,
          0.36750632524490356,
          0.34771567583084106,
          0.32290175557136536,
          0.3519449532032013,
          0.3550053834915161,
          0.3387940227985382,
          0.3436097800731659,
          0.3594297170639038,
          0.34736335277557373,
          0.3622802197933197,
          0.3378518521785736,
          0.3634016513824463,
          0.3625326454639435,
          0.36244672536849976,
          0.3237338066101074,
          0.3081635534763336,
          0.36162319779396057,
          0.37215307354927063,
          0.3251059949398041,
          0.3310241401195526,
          0.3311749994754791,
          0.3135378658771515,
          0.3301568031311035,
          0.36343294382095337,
          0.3243977129459381,
          0.4084486961364746,
          0.39113467931747437,
          0.36826062202453613,
          0.3370986580848694,
          0.32916295528411865,
          0.3547358214855194,
          0.28679490089416504,
          0.331175833940506,
          0.3139725625514984,
          0.3523906171321869,
          0.32678934931755066,
          0.30820232629776,
          0.32948195934295654,
          0.38030779361724854,
          0.3384413719177246,
          0.3878202736377716,
          0.33407875895500183,
          0.4093177616596222,
          0.4274511933326721,
          0.3769414722919464,
          0.37233683466911316,
          0.3587234914302826,
          0.3681908845901489,
          0.331525593996048,
          0.3207882344722748,
          0.31332626938819885,
          0.347033828496933,
          0.31330135464668274,
          0.3214775621891022,
          0.3793482482433319,
          0.38477954268455505,
          0.3571934700012207,
          0.3309209942817688,
          0.3281899094581604,
          0.3664472997188568,
          0.3505350351333618,
          0.32583895325660706,
          0.36131617426872253,
          0.34950777888298035,
          0.33738264441490173,
          0.33853185176849365,
          0.36595800518989563,
          0.36203357577323914,
          0.33920523524284363,
          0.36279648542404175,
          0.35387659072875977,
          0.3653067946434021,
          0.3610798120498657,
          0.35936689376831055,
          0.408454030752182,
          0.35042551159858704,
          0.34416672587394714,
          0.3366771340370178,
          0.3826104700565338,
          0.33733516931533813,
          0.3664349317550659,
          0.3704328238964081,
          0.37263861298561096,
          0.36546164751052856,
          0.36018428206443787,
          0.38335341215133667,
          0.3283242881298065,
          0.34064480662345886,
          0.3589020371437073,
          0.36172062158584595,
          0.3296321928501129,
          0.3758341670036316,
          0.3911322355270386,
          0.3881767690181732,
          0.3809828758239746,
          0.3526588976383209,
          0.35118186473846436,
          0.3752051591873169,
          0.39233097434043884,
          0.3450908064842224,
          0.3352234959602356,
          0.3817604184150696,
          0.4304577708244324,
          0.3584727644920349,
          0.3971228301525116,
          0.36492571234703064,
          0.4116867184638977,
          0.3509488105773926,
          0.33868324756622314,
          0.33992093801498413,
          0.34960824251174927,
          0.3714350759983063,
          0.37353837490081787,
          0.3791985511779785,
          0.384269654750824,
          0.377531498670578,
          0.34753528237342834,
          0.3709612190723419,
          0.35416609048843384,
          0.3286919593811035
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "Curvas de Pérdida"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Gráficas de Pérdida de Entrenamiento y Evaluación"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Binary Cross Entropy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.204</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.206</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.697</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.172</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.636</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.185</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.758</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.190</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.758</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.244</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.606</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.196</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.636</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.175</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.667</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.171</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.697</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.203</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.667</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Training Loss  accuracy  Validation Loss  val_accuracy  epoch\n",
       "0            0.204     0.786            0.358         0.697      0\n",
       "1            0.206     0.786            0.318         0.697      1\n",
       "2            0.172     0.816            0.376         0.636      2\n",
       "3            0.185     0.776            0.282         0.758      3\n",
       "4            0.190     0.786            0.297         0.758      4\n",
       "..             ...       ...              ...           ...    ...\n",
       "195          0.244     0.745            0.378         0.606    195\n",
       "196          0.196     0.776            0.348         0.636    196\n",
       "197          0.175     0.796            0.371         0.667    197\n",
       "198          0.171     0.837            0.354         0.697    198\n",
       "199          0.203     0.786            0.329         0.667    199\n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 200\n",
    "batch_size = 3\n",
    "#model = my_model(learning_rate)\n",
    "history= train_model(model, train_features, train_labels, epochs, batch_size)\n",
    "display(history)\n",
    "loss_curves(history)\n",
    "#view_filters(model)\n",
    "display(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normal</th>\n",
       "      <th>collision_in_tool</th>\n",
       "      <th>collision_in_part</th>\n",
       "      <th>bottom_collision</th>\n",
       "      <th>bottom_obstruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.317</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.333</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.335</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.317</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.331</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.319</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.319</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.332</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    normal  collision_in_tool  collision_in_part  bottom_collision  \\\n",
       "0    0.317              0.177              0.386             0.103   \n",
       "1    0.000              0.000              0.000             0.000   \n",
       "2    0.000              0.000              0.004             0.828   \n",
       "3    0.333              0.185              0.383             0.086   \n",
       "4    0.335              0.184              0.387             0.082   \n",
       "..     ...                ...                ...               ...   \n",
       "28   0.317              0.177              0.385             0.104   \n",
       "29   0.331              0.184              0.385             0.087   \n",
       "30   0.319              0.178              0.385             0.102   \n",
       "31   0.319              0.179              0.384             0.102   \n",
       "32   0.332              0.185              0.383             0.087   \n",
       "\n",
       "    bottom_obstruction  \n",
       "0                0.017  \n",
       "1                1.000  \n",
       "2                0.167  \n",
       "3                0.013  \n",
       "4                0.012  \n",
       "..                 ...  \n",
       "28               0.017  \n",
       "29               0.013  \n",
       "30               0.016  \n",
       "31               0.016  \n",
       "32               0.013  \n",
       "\n",
       "[33 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1  2  3  4  5\n",
       "135  0  0  0  1  0\n",
       "115  0  0  0  0  1\n",
       "131  0  0  0  0  1\n",
       "55   1  0  0  0  0\n",
       "95   0  1  0  0  0\n",
       "..  .. .. .. .. ..\n",
       "134  0  0  0  1  0\n",
       "160  0  1  0  0  0\n",
       "139  0  0  0  1  0\n",
       "78   0  0  1  0  0\n",
       "60   1  0  0  0  0\n",
       "\n",
       "[33 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = model.predict(test_features)\n",
    "predictions_df = pd.DataFrame(predictions, columns=['normal', 'collision_in_tool', 'collision_in_part', 'bottom_collision', 'bottom_obstruction'])\n",
    "display(predictions_df)\n",
    "display(test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
