{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar Modulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import plotly.express as px\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                labels 1  2  3  4  5 \n",
       "0               normal               \n",
       "1               normal               \n",
       "2               normal               \n",
       "3               normal               \n",
       "4               normal               \n",
       "..                 ... .. .. .. .. ..\n",
       "159  collision_in_tool               \n",
       "160  collision_in_tool               \n",
       "161  collision_in_tool               \n",
       "162  collision_in_tool               \n",
       "163  collision_in_tool               \n",
       "\n",
       "[164 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ -2.,  -1.,  81.,   0.,  -5.,   0.],\n",
       "        [ -2.,  -1.,  79.,   0.,  -4.,   0.],\n",
       "        [ -2.,  -1.,  79.,   0.,  -4.,   0.],\n",
       "        ...,\n",
       "        [ -2.,  -1.,  78.,   0.,  -5.,   0.],\n",
       "        [ -3.,  -1.,  80.,   1.,  -4.,   1.],\n",
       "        [ -2.,  -1.,  79.,   0.,  -4.,   0.]],\n",
       "\n",
       "       [[  6.,  -1.,  79.,  -2.,   4.,  -3.],\n",
       "        [ 42.,  -3.,  80.,   5.,  53.,   3.],\n",
       "        [ -5.,   4.,  74., -15., -10.,  -1.],\n",
       "        ...,\n",
       "        [ -1.,  -5.,  80.,   6.,  -6.,   0.],\n",
       "        [ -4.,   5.,  78., -14.,  -9.,  -4.],\n",
       "        [ -4.,   1.,  80.,  -3., -12.,   5.]],\n",
       "\n",
       "       [[ -2.,  -6.,  85.,  14.,  -5.,   2.],\n",
       "        [  0.,   2.,  74.,  -7.,   1.,   0.],\n",
       "        [ -4.,  -5.,  76.,   7., -11.,   4.],\n",
       "        ...,\n",
       "        [  0.,  -9.,  87.,  13.,  -5.,   2.],\n",
       "        [ -5.,   5.,  67., -17., -16.,   7.],\n",
       "        [ -6., -10.,  86.,  16., -14.,  -1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-12.,  17.,   3., -19., -10.,  -4.],\n",
       "        [-12.,  12.,  11., -13., -16.,  -4.],\n",
       "        [ -8.,   3.,   6.,   2., -11.,  -4.],\n",
       "        ...,\n",
       "        [  0.,   1.,   3.,   1.,   1.,  -3.],\n",
       "        [  1.,   2.,   9.,   0.,   3.,  -4.],\n",
       "        [  1.,   2.,   9.,   0.,   3.,  -4.]],\n",
       "\n",
       "       [[-41.,  21.,  -5., -23., -59.,  -4.],\n",
       "        [-32.,  21.,  -6., -25., -45.,  -4.],\n",
       "        [-21.,  12.,  -6., -14., -31.,  -4.],\n",
       "        ...,\n",
       "        [ -4.,   4.,   3.,   0.,  -1.,  -3.],\n",
       "        [ -4.,   3.,   2.,   2.,  -3.,  -3.],\n",
       "        [ -2.,   3.,   5.,   0.,  -2.,  -3.]],\n",
       "\n",
       "       [[  9., -10., -11.,  17.,   7.,  -4.],\n",
       "        [  5.,   0.,   4.,   0.,   7.,  -4.],\n",
       "        [ -3.,   6.,  -2.,  -8.,  -8.,  -4.],\n",
       "        ...,\n",
       "        [ -1.,   1.,  -3.,  -3.,  -2.,  -3.],\n",
       "        [  0.,  -1.,  -5.,  -1.,   1.,  -3.],\n",
       "        [ -1.,   1.,   4.,   0.,  -1.,  -3.]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.3f}\".format\n",
    "\n",
    "data = np.loadtxt(\"lp5.csv\", delimiter=\",\", dtype=str)\n",
    "data_copy = data #va de 0 a 2623 o sea 2624 datos\n",
    "cont = 16\n",
    "labels = []\n",
    "numbers = []\n",
    "features= np.zeros((164,15,6))\n",
    "\n",
    "for i in range(len(data_copy)):\n",
    "    if(cont == 16):\n",
    "        labels.append(data_copy[i])\n",
    "    if(cont<16):\n",
    "        numbers.append(data_copy[i])\n",
    "    cont -= 1\n",
    "    if(cont == 0):\n",
    "        cont = 16\n",
    "        \n",
    "cont = 0\n",
    "for i in range(164):\n",
    "    for j in range(15):\n",
    "        for z in range(6):\n",
    "            features[i][j][z] = numbers[cont][z]\n",
    "        cont += 1\n",
    "        \n",
    "labels_df = pd.DataFrame(labels, columns=['labels','1','2','3','4','5'])\n",
    "#labels_df = labels_df.loc[:,['labels']]\n",
    "display(labels_df)\n",
    "display(features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manejo de Datos y Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dave4\\AppData\\Local\\Temp\\ipykernel_7200\\1747463221.py:8: FutureWarning:\n",
      "\n",
      "In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.77660263, 0.78156702, 0.79084826, 0.77055903, 0.7684006 ,\n",
       "         0.77422836],\n",
       "        [0.77466005, 0.78739478, 0.78717893, 0.75156486, 0.7638679 ,\n",
       "         0.77206993],\n",
       "        [0.77401252, 0.78804231, 0.79041658, 0.74465789, 0.76106195,\n",
       "         0.77012735],\n",
       "        ...,\n",
       "        [0.78005612, 0.7804878 , 0.78437298, 0.78135118, 0.78113533,\n",
       "         0.78027196],\n",
       "        [0.78027196, 0.78091949, 0.77962443, 0.78091949, 0.78027196,\n",
       "         0.78027196],\n",
       "        [0.78027196, 0.78070365, 0.78286208, 0.78156702, 0.78091949,\n",
       "         0.78005612]],\n",
       "\n",
       "       [[0.78027196, 0.77940859, 0.79905029, 0.78372545, 0.77962443,\n",
       "         0.78113533],\n",
       "        [0.78070365, 0.78113533, 0.79667602, 0.77919275, 0.78091949,\n",
       "         0.78070365],\n",
       "        [0.77984028, 0.77962443, 0.79710771, 0.78221455, 0.77832938,\n",
       "         0.78156702],\n",
       "        ...,\n",
       "        [0.78070365, 0.77876106, 0.79948198, 0.78350961, 0.77962443,\n",
       "         0.78113533],\n",
       "        [0.77962443, 0.78178286, 0.79516512, 0.77703432, 0.77725016,\n",
       "         0.78221455],\n",
       "        [0.77940859, 0.77854522, 0.79926613, 0.78415713, 0.77768185,\n",
       "         0.7804878 ]],\n",
       "\n",
       "       [[0.78502051, 0.77725016, 0.78264623, 0.78631556, 0.78696309,\n",
       "         0.7804878 ],\n",
       "        [0.78588388, 0.78005612, 0.78717893, 0.78243039, 0.78890568,\n",
       "         0.77984028],\n",
       "        [0.78653141, 0.77876106, 0.78027196, 0.78480466, 0.78933736,\n",
       "         0.78027196],\n",
       "        ...,\n",
       "        [0.78005612, 0.78135118, 0.77962443, 0.7804878 , 0.77962443,\n",
       "         0.77962443],\n",
       "        [0.78005612, 0.78091949, 0.7789769 , 0.78091949, 0.78027196,\n",
       "         0.77962443],\n",
       "        [0.7804878 , 0.78091949, 0.78005612, 0.78156702, 0.78070365,\n",
       "         0.77962443]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.78005612, 0.78070365, 0.79689186, 0.77919275, 0.77660263,\n",
       "         0.77984028],\n",
       "        [0.77962443, 0.77832938, 0.79883445, 0.78307792, 0.77617095,\n",
       "         0.77811353],\n",
       "        [0.78005612, 0.78264623, 0.79516512, 0.77487589, 0.77681848,\n",
       "         0.7789769 ],\n",
       "        ...,\n",
       "        [0.7804878 , 0.77832938, 0.79969782, 0.78307792, 0.777466  ,\n",
       "         0.77854522],\n",
       "        [0.77940859, 0.77984028, 0.79710771, 0.77962443, 0.77573926,\n",
       "         0.77984028],\n",
       "        [0.77940859, 0.78243039, 0.79710771, 0.77530758, 0.77617095,\n",
       "         0.77940859]],\n",
       "\n",
       "       [[0.77077488, 0.79494928, 0.77617095, 0.75588172, 0.77832938,\n",
       "         0.7804878 ],\n",
       "        [0.7653788 , 0.78135118, 0.78243039, 0.777466  , 0.7638679 ,\n",
       "         0.78027196],\n",
       "        [0.76365206, 0.78264623, 0.78350961, 0.78005612, 0.75674509,\n",
       "         0.78027196],\n",
       "        ...,\n",
       "        [0.77984028, 0.78113533, 0.78243039, 0.78113533, 0.78005612,\n",
       "         0.78005612],\n",
       "        [0.78027196, 0.78113533, 0.78350961, 0.78091949, 0.78070365,\n",
       "         0.78027196],\n",
       "        [0.7804878 , 0.78091949, 0.78480466, 0.78221455, 0.78156702,\n",
       "         0.78005612]],\n",
       "\n",
       "       [[0.77789769, 0.77789769, 0.7744442 , 0.78696309, 0.78653141,\n",
       "         0.77789769],\n",
       "        [0.77832938, 0.77811353, 0.77422836, 0.78631556, 0.78868983,\n",
       "         0.7789769 ],\n",
       "        [0.77832938, 0.77789769, 0.77466005, 0.78696309, 0.78804231,\n",
       "         0.77768185],\n",
       "        ...,\n",
       "        [0.67882581, 0.70472696, 0.09842435, 0.7759551 , 0.85732787,\n",
       "         0.74940643],\n",
       "        [0.69933089, 0.73796676, 0.34448521, 0.77099072, 0.81674941,\n",
       "         0.77530758],\n",
       "        [0.72199439, 0.73170732, 0.7155191 , 0.81804446, 0.69156054,\n",
       "         0.77012735]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1  2  3  4  5\n",
       "84   0  0  1  0  0\n",
       "2    1  0  0  0  0\n",
       "94   0  1  0  0  0\n",
       "45   0  0  1  0  0\n",
       "42   0  0  1  0  0\n",
       "..  .. .. .. .. ..\n",
       "71   0  0  1  0  0\n",
       "106  0  0  0  1  0\n",
       "14   1  0  0  0  0\n",
       "92   0  1  0  0  0\n",
       "102  0  0  0  0  1\n",
       "\n",
       "[131 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes_dict = {'normal':'1 0 0 0 0', #1\n",
    "        'collision_in_tool':'0 1 0 0 0', #2\n",
    "        'collision_in_part':'0 0 1 0 0', #3\n",
    "        'bottom_collision':'0 0 0 1 0', #4\n",
    "        'bottom_obstruction':'0 0 0 0 1'} #5\n",
    "\n",
    "labels_df = labels_df.replace({'labels':classes_dict})\n",
    "labels_df[['1', '2', '3', '4', '5']] = labels_df['labels'].str.split(' ', 4, expand= True)\n",
    "labels_df = labels_df.loc[:,['1', '2', '3', '4', '5']]\n",
    "labels_df[['1', '2', '3', '4', '5']]=labels_df[['1', '2', '3', '4', '5']].astype(str).astype(int)\n",
    "\n",
    "def norm(x):\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "    range = x_max - x_min  #min max entre 0 y 1\n",
    "    return((x-x_min)/(range))\n",
    "\n",
    "norm_features = norm(features)\n",
    "#split para entrenamiento y validacion\n",
    "train_features, test_features, train_labels, test_labels  = train_test_split(norm_features, labels_df, test_size=0.2, random_state= 42)\n",
    "display(train_features)\n",
    "display(train_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 200 Complete [00h 00m 02s]\n",
      "val_accuracy: 0.3333333432674408\n",
      "\n",
      "Best val_accuracy So Far: 0.3333333432674408\n",
      "Total elapsed time: 00h 06m 52s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.13\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.14\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.15\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.16\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.17\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.18\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.21\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.22\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.23\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.24\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 15, 6, 8)          80        \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 15, 6, 8)          584       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 7, 3, 8)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 7, 3, 8)           0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 3, 16)          1168      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 7, 3, 16)          2320      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 3, 1, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 3, 1, 16)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 48)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                980       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 105       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,237\n",
      "Trainable params: 5,237\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "<keras_tuner.engine.hyperparameters.hyperparameters.HyperParameters object at 0x000001DDCAAB3040>\n"
     ]
    }
   ],
   "source": [
    "'''def my_model(hp):\n",
    "    inputs = tf.keras.Input(shape=(15,6,1))\n",
    "    x = inputs\n",
    "    for i in range(hp.Int('conv_blocks', 2, 5, default=2)):\n",
    "        filters = hp.Int('filters_' + str(i), 8, 256, step=32) \n",
    "        \n",
    "    for i in range(2):\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel_size= hp.Choice('conv_1_kernel', values = [3,5]), padding= 'same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        \n",
    "    if hp.Choice('pooling_' + str(i), ['avg', 'max']) == 'max': # hp.Choice chooses from a list of values\n",
    "        x = tf.keras.layers.MaxPool2D()(x)\n",
    "    else:\n",
    "        x = tf.keras.layers.AvgPool2D()(x)\n",
    "\n",
    "    x = tf.keras.layers.GlobalAvgPool2D()(x) # apply GlobalAvG Pooling\n",
    "    \n",
    "    # Tune the number of units in the  Dense layer\n",
    "    # Choose an optimal value between min_value to max_value\n",
    "    x = tf.keras.layers.Dense(hp.Int('Dense units',10, 100, step=10, default=50), activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(5, activation= 'softmax')(x) # output layer \n",
    "\n",
    "    # define the model\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    # Tune the learning rate for the optimizer\n",
    "    # Choose an optimal value frommin_value to max_value\n",
    "    model.compile(optimizer= tf.keras.optimizers.Adam(hp.Float('learning_rate',1e-4, 1e-2, sampling='log')), \n",
    "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# initialize tuner to run the model.\n",
    "# using the Hyperband search algorithm\n",
    "tuner = kt.Hyperband(\n",
    "    hypermodel = my_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=30,\n",
    "    hyperband_iterations=2,\n",
    "    directory=\"Keras_tuner_dir\",\n",
    "    project_name=\"Keras_tuner_Demo\")\n",
    "\n",
    "# Run the search\n",
    "tuner.search(train_features, train_labels,\n",
    "            validation_data= (test_features,test_labels), \n",
    "            epochs=30,\n",
    "            callbacks=[tf.keras.callbacks.EarlyStopping(patience=2)])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps= tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "# get the best model\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "\n",
    "# show model summary\n",
    "best_model.summary()'''\n",
    "def my_model(hp):\n",
    "    filters1=hp.Int('conv_filt1', 4, 16, step=4)\n",
    "    filter2=hp.Int('conv_filt2', 8, 32, step=8)\n",
    "    kernel1 = hp.Choice('kernel1', values = [3,5])\n",
    "    kernel2 = hp.Choice('kernel2', values = [3,5])\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(filters1, kernel_size=kernel1, activation='relu', input_shape = (15,6,1), padding='same'), # #de feature maps, dimensiones del kernel, kernel entre más pequeño mejor y se prefiere un número impar\n",
    "        tf.keras.layers.Conv2D(filters1, kernel_size=kernel1, activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D((2,2), strides= 2, padding= 'valid'), # dimensiones del pooling\n",
    "        tf.keras.layers.Dropout(hp.Float('dropout1',0.25, 1.0)),\n",
    "        tf.keras.layers.Conv2D(filter2, kernel_size=kernel2, activation='relu', padding='same'),\n",
    "        tf.keras.layers.Conv2D(filter2, kernel_size=kernel2, activation='relu', padding='same'),# #de feature maps, dimensiones del kernel, kernel entre más pequeño mejor y se prefiere un número impar\n",
    "        tf.keras.layers.MaxPooling2D((2,2), strides= 2, padding= 'valid'), # dimensiones del pooling\n",
    "        tf.keras.layers.Dropout(hp.Float('dropout2',0.15, 1.0)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=hp.Int('neuronas', 10, 30, step=5), activation='relu'),\n",
    "        tf.keras.layers.Dense(5, activation='softmax')])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate',0.0005, 0.005)), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "tuner = kt.GridSearch(\n",
    "    hypermodel = my_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials= 200,\n",
    "    directory=\"Tuner_Conv\",\n",
    "    project_name=\"Tarea1_B\",\n",
    "    overwrite = True)\n",
    "\n",
    "tuner.search(train_features, train_labels,\n",
    "            validation_data= (test_features,test_labels), \n",
    "            epochs=50,\n",
    "            callbacks=[tf.keras.callbacks.EarlyStopping(patience=2)])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps= tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "# get the best model\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "\n",
    "# show model summary\n",
    "best_model.summary()\n",
    "print(best_hps)\n",
    "\n",
    "model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, features, labels, epochs, batch_size):\n",
    "    history = model.fit(\n",
    "        x = features,\n",
    "        y = labels,\n",
    "        epochs= epochs,\n",
    "        batch_size= batch_size,\n",
    "        validation_split= 0.25\n",
    "    )\n",
    "    hist= pd.DataFrame(history.history) #se guardan los valores de errores y metricas en un diccionario\n",
    "    hist['epoch'] = history.epoch #los epochs se deben añadir aparte\n",
    "    return hist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizaciones\n",
    "### Pasos para visualizar los filtros:\n",
    "##### 1. Iterar por todas las capas del modelo usando model.layers\n",
    "##### 2. Si la capa actual es de convolucion se extraen los pesos y sesgos usando get_weights()\n",
    "##### 3. Se normalizan los pesos de los filtros entre 0 y 1\n",
    "##### 4. Se plotean los filtros para cada capa convolutional y todos los canales de color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_curves(history):\n",
    "    hist = history\n",
    "    labels = {\"loss\":\"Training Loss\", \"val_loss\":\"Validation Loss\"}\n",
    "    hist.rename(columns = labels, inplace = True)\n",
    "    \n",
    "    fig = px.line(hist, x='epoch', y=['Training Loss', 'Validation Loss'],\n",
    "                title='Gráficas de Pérdida de Entrenamiento y Evaluación',\n",
    "                labels={\"epoch\": \"Epoch\", \"value\":\"Binary Cross Entropy\", \"variable\":\"Curvas de Pérdida\"},\n",
    "                color_discrete_map={ # replaces default color mapping by value\n",
    "                \"Training Loss\": \"#46039f\", \"Validation Loss\": \"#fb9f3a\"})\n",
    "    fig.update_layout(template='plotly_white')\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se corren las funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dave4\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\backend.py:5703: UserWarning:\n",
      "\n",
      "\"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 1s 3ms/step - loss: 0.5530 - accuracy: 0.2653 - val_loss: 0.4961 - val_accuracy: 0.2727\n",
      "Epoch 2/250\n",
      "40/98 [===========>..................] - ETA: 0s - loss: 0.5238 - accuracy: 0.2000    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dave4\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\backend.py:5703: UserWarning:\n",
      "\n",
      "\"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 0s 2ms/step - loss: 0.5043 - accuracy: 0.2857 - val_loss: 0.5014 - val_accuracy: 0.2727\n",
      "Epoch 3/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.3061 - val_loss: 0.4863 - val_accuracy: 0.2727\n",
      "Epoch 4/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4940 - accuracy: 0.3061 - val_loss: 0.4951 - val_accuracy: 0.2727\n",
      "Epoch 5/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4980 - accuracy: 0.2755 - val_loss: 0.4953 - val_accuracy: 0.2727\n",
      "Epoch 6/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4942 - accuracy: 0.3163 - val_loss: 0.4957 - val_accuracy: 0.2727\n",
      "Epoch 7/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4932 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 8/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4930 - accuracy: 0.3061 - val_loss: 0.4877 - val_accuracy: 0.2727\n",
      "Epoch 9/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4924 - accuracy: 0.3061 - val_loss: 0.4898 - val_accuracy: 0.2727\n",
      "Epoch 10/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4928 - accuracy: 0.2857 - val_loss: 0.4921 - val_accuracy: 0.2727\n",
      "Epoch 11/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4929 - accuracy: 0.3163 - val_loss: 0.4904 - val_accuracy: 0.2727\n",
      "Epoch 12/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.3163 - val_loss: 0.4884 - val_accuracy: 0.2727\n",
      "Epoch 13/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4801 - accuracy: 0.2755 - val_loss: 0.4605 - val_accuracy: 0.3333\n",
      "Epoch 14/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4345 - accuracy: 0.4184 - val_loss: 0.4414 - val_accuracy: 0.3333\n",
      "Epoch 15/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4119 - accuracy: 0.4286 - val_loss: 0.4817 - val_accuracy: 0.3333\n",
      "Epoch 16/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4114 - accuracy: 0.4184 - val_loss: 0.5278 - val_accuracy: 0.3333\n",
      "Epoch 17/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4175 - accuracy: 0.4490 - val_loss: 0.4323 - val_accuracy: 0.3333\n",
      "Epoch 18/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4101 - accuracy: 0.4490 - val_loss: 0.4347 - val_accuracy: 0.3333\n",
      "Epoch 19/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3989 - accuracy: 0.4490 - val_loss: 0.4308 - val_accuracy: 0.3333\n",
      "Epoch 20/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3968 - accuracy: 0.4490 - val_loss: 0.4295 - val_accuracy: 0.3333\n",
      "Epoch 21/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3947 - accuracy: 0.4490 - val_loss: 0.4303 - val_accuracy: 0.3333\n",
      "Epoch 22/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3980 - accuracy: 0.4490 - val_loss: 0.4995 - val_accuracy: 0.3333\n",
      "Epoch 23/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4128 - accuracy: 0.3980 - val_loss: 0.4280 - val_accuracy: 0.3333\n",
      "Epoch 24/250\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 0.3990 - accuracy: 0.4184 - val_loss: 0.4351 - val_accuracy: 0.3333\n",
      "Epoch 25/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3973 - accuracy: 0.4184 - val_loss: 0.4392 - val_accuracy: 0.3333\n",
      "Epoch 26/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3969 - accuracy: 0.4490 - val_loss: 0.4655 - val_accuracy: 0.3333\n",
      "Epoch 27/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4010 - accuracy: 0.4490 - val_loss: 0.4333 - val_accuracy: 0.3333\n",
      "Epoch 28/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4054 - accuracy: 0.4388 - val_loss: 0.4318 - val_accuracy: 0.3333\n",
      "Epoch 29/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4026 - accuracy: 0.4388 - val_loss: 0.4442 - val_accuracy: 0.3333\n",
      "Epoch 30/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4013 - accuracy: 0.4490 - val_loss: 0.4283 - val_accuracy: 0.3333\n",
      "Epoch 31/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4015 - accuracy: 0.4388 - val_loss: 0.4228 - val_accuracy: 0.3333\n",
      "Epoch 32/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3958 - accuracy: 0.4592 - val_loss: 0.4417 - val_accuracy: 0.3333\n",
      "Epoch 33/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4035 - accuracy: 0.4388 - val_loss: 0.4226 - val_accuracy: 0.3333\n",
      "Epoch 34/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3940 - accuracy: 0.4490 - val_loss: 0.4449 - val_accuracy: 0.3333\n",
      "Epoch 35/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3906 - accuracy: 0.4490 - val_loss: 0.4339 - val_accuracy: 0.3333\n",
      "Epoch 36/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3942 - accuracy: 0.4592 - val_loss: 0.4411 - val_accuracy: 0.3333\n",
      "Epoch 37/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3947 - accuracy: 0.4490 - val_loss: 0.4334 - val_accuracy: 0.3333\n",
      "Epoch 38/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3916 - accuracy: 0.4592 - val_loss: 0.4410 - val_accuracy: 0.3333\n",
      "Epoch 39/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3921 - accuracy: 0.4592 - val_loss: 0.4695 - val_accuracy: 0.3333\n",
      "Epoch 40/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3960 - accuracy: 0.4490 - val_loss: 0.4984 - val_accuracy: 0.3333\n",
      "Epoch 41/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3930 - accuracy: 0.4592 - val_loss: 0.4340 - val_accuracy: 0.3333\n",
      "Epoch 42/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3940 - accuracy: 0.4592 - val_loss: 0.4915 - val_accuracy: 0.3333\n",
      "Epoch 43/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3947 - accuracy: 0.4490 - val_loss: 0.4146 - val_accuracy: 0.3939\n",
      "Epoch 44/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3915 - accuracy: 0.4490 - val_loss: 0.4462 - val_accuracy: 0.3333\n",
      "Epoch 45/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3921 - accuracy: 0.4592 - val_loss: 0.4178 - val_accuracy: 0.3333\n",
      "Epoch 46/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3916 - accuracy: 0.4694 - val_loss: 0.4346 - val_accuracy: 0.3333\n",
      "Epoch 47/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3920 - accuracy: 0.4694 - val_loss: 0.4206 - val_accuracy: 0.3333\n",
      "Epoch 48/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3881 - accuracy: 0.4694 - val_loss: 0.4185 - val_accuracy: 0.3333\n",
      "Epoch 49/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3903 - accuracy: 0.4592 - val_loss: 0.6021 - val_accuracy: 0.3333\n",
      "Epoch 50/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3867 - accuracy: 0.4898 - val_loss: 0.7042 - val_accuracy: 0.3333\n",
      "Epoch 51/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4410 - accuracy: 0.4490 - val_loss: 0.4332 - val_accuracy: 0.3333\n",
      "Epoch 52/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4012 - accuracy: 0.4490 - val_loss: 0.4250 - val_accuracy: 0.3636\n",
      "Epoch 53/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3916 - accuracy: 0.4592 - val_loss: 0.4773 - val_accuracy: 0.3333\n",
      "Epoch 54/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3966 - accuracy: 0.4592 - val_loss: 0.5865 - val_accuracy: 0.3333\n",
      "Epoch 55/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4491 - accuracy: 0.3878 - val_loss: 0.4586 - val_accuracy: 0.3030\n",
      "Epoch 56/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4318 - accuracy: 0.4184 - val_loss: 0.4438 - val_accuracy: 0.3333\n",
      "Epoch 57/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4089 - accuracy: 0.4388 - val_loss: 0.4354 - val_accuracy: 0.3333\n",
      "Epoch 58/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3992 - accuracy: 0.4388 - val_loss: 0.4258 - val_accuracy: 0.3636\n",
      "Epoch 59/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3949 - accuracy: 0.4490 - val_loss: 0.4247 - val_accuracy: 0.3636\n",
      "Epoch 60/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3943 - accuracy: 0.4490 - val_loss: 0.4238 - val_accuracy: 0.3636\n",
      "Epoch 61/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3949 - accuracy: 0.4490 - val_loss: 0.4215 - val_accuracy: 0.3636\n",
      "Epoch 62/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4016 - accuracy: 0.4388 - val_loss: 0.4512 - val_accuracy: 0.3333\n",
      "Epoch 63/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3942 - accuracy: 0.4490 - val_loss: 0.4208 - val_accuracy: 0.3333\n",
      "Epoch 64/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3940 - accuracy: 0.4490 - val_loss: 0.4207 - val_accuracy: 0.3333\n",
      "Epoch 65/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3944 - accuracy: 0.4490 - val_loss: 0.4213 - val_accuracy: 0.3333\n",
      "Epoch 66/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3956 - accuracy: 0.4490 - val_loss: 0.4223 - val_accuracy: 0.3333\n",
      "Epoch 67/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3943 - accuracy: 0.4490 - val_loss: 0.4217 - val_accuracy: 0.3333\n",
      "Epoch 68/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3953 - accuracy: 0.4490 - val_loss: 0.4206 - val_accuracy: 0.3333\n",
      "Epoch 69/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3946 - accuracy: 0.4490 - val_loss: 0.4218 - val_accuracy: 0.3333\n",
      "Epoch 70/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3939 - accuracy: 0.4490 - val_loss: 0.4211 - val_accuracy: 0.3333\n",
      "Epoch 71/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3943 - accuracy: 0.4490 - val_loss: 0.4207 - val_accuracy: 0.3333\n",
      "Epoch 72/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3955 - accuracy: 0.4490 - val_loss: 0.4215 - val_accuracy: 0.3333\n",
      "Epoch 73/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4033 - accuracy: 0.4388 - val_loss: 0.5275 - val_accuracy: 0.3333\n",
      "Epoch 74/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3933 - accuracy: 0.4388 - val_loss: 0.7425 - val_accuracy: 0.3333\n",
      "Epoch 75/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4218 - accuracy: 0.4388 - val_loss: 0.4298 - val_accuracy: 0.3333\n",
      "Epoch 76/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4044 - accuracy: 0.4388 - val_loss: 0.4304 - val_accuracy: 0.3333\n",
      "Epoch 77/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4047 - accuracy: 0.4388 - val_loss: 0.4293 - val_accuracy: 0.3333\n",
      "Epoch 78/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3948 - accuracy: 0.4490 - val_loss: 0.4305 - val_accuracy: 0.3333\n",
      "Epoch 79/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3951 - accuracy: 0.4490 - val_loss: 0.4294 - val_accuracy: 0.3333\n",
      "Epoch 80/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3956 - accuracy: 0.4286 - val_loss: 0.4293 - val_accuracy: 0.3333\n",
      "Epoch 81/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.4490 - val_loss: 0.4216 - val_accuracy: 0.3333\n",
      "Epoch 82/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3953 - accuracy: 0.3980 - val_loss: 0.4423 - val_accuracy: 0.3333\n",
      "Epoch 83/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3984 - accuracy: 0.4490 - val_loss: 0.4811 - val_accuracy: 0.3333\n",
      "Epoch 84/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4021 - accuracy: 0.4490 - val_loss: 0.4075 - val_accuracy: 0.3939\n",
      "Epoch 85/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3920 - accuracy: 0.4694 - val_loss: 0.4085 - val_accuracy: 0.3939\n",
      "Epoch 86/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3886 - accuracy: 0.4694 - val_loss: 0.4063 - val_accuracy: 0.3939\n",
      "Epoch 87/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3892 - accuracy: 0.4490 - val_loss: 0.4015 - val_accuracy: 0.3939\n",
      "Epoch 88/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3784 - accuracy: 0.5000 - val_loss: 0.3999 - val_accuracy: 0.3939\n",
      "Epoch 89/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3793 - accuracy: 0.4796 - val_loss: 0.3892 - val_accuracy: 0.4242\n",
      "Epoch 90/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3768 - accuracy: 0.4694 - val_loss: 0.4067 - val_accuracy: 0.4545\n",
      "Epoch 91/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3964 - accuracy: 0.4592 - val_loss: 0.4111 - val_accuracy: 0.3939\n",
      "Epoch 92/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3872 - accuracy: 0.4388 - val_loss: 0.4628 - val_accuracy: 0.3333\n",
      "Epoch 93/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3812 - accuracy: 0.4694 - val_loss: 0.3968 - val_accuracy: 0.3939\n",
      "Epoch 94/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3801 - accuracy: 0.4898 - val_loss: 0.3982 - val_accuracy: 0.4242\n",
      "Epoch 95/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3777 - accuracy: 0.4592 - val_loss: 0.6676 - val_accuracy: 0.3333\n",
      "Epoch 96/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4619 - accuracy: 0.3776 - val_loss: 0.4428 - val_accuracy: 0.3333\n",
      "Epoch 97/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4088 - accuracy: 0.4286 - val_loss: 0.4226 - val_accuracy: 0.3636\n",
      "Epoch 98/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4073 - accuracy: 0.4286 - val_loss: 0.4341 - val_accuracy: 0.3333\n",
      "Epoch 99/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3974 - accuracy: 0.4388 - val_loss: 0.4264 - val_accuracy: 0.3333\n",
      "Epoch 100/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4123 - accuracy: 0.4592 - val_loss: 0.4072 - val_accuracy: 0.3636\n",
      "Epoch 101/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3755 - accuracy: 0.4694 - val_loss: 0.3941 - val_accuracy: 0.3636\n",
      "Epoch 102/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3867 - accuracy: 0.4898 - val_loss: 0.4137 - val_accuracy: 0.3939\n",
      "Epoch 103/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3830 - accuracy: 0.4898 - val_loss: 0.4068 - val_accuracy: 0.3939\n",
      "Epoch 104/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3894 - accuracy: 0.4286 - val_loss: 0.3987 - val_accuracy: 0.3939\n",
      "Epoch 105/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3796 - accuracy: 0.4592 - val_loss: 0.3898 - val_accuracy: 0.3939\n",
      "Epoch 106/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3820 - accuracy: 0.4796 - val_loss: 0.3898 - val_accuracy: 0.3939\n",
      "Epoch 107/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3729 - accuracy: 0.4592 - val_loss: 0.3687 - val_accuracy: 0.4545\n",
      "Epoch 108/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3711 - accuracy: 0.5204 - val_loss: 0.4151 - val_accuracy: 0.3939\n",
      "Epoch 109/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3845 - accuracy: 0.4694 - val_loss: 0.3957 - val_accuracy: 0.3939\n",
      "Epoch 110/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3837 - accuracy: 0.4694 - val_loss: 0.3704 - val_accuracy: 0.4545\n",
      "Epoch 111/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4739 - accuracy: 0.4694 - val_loss: 0.4111 - val_accuracy: 0.3939\n",
      "Epoch 112/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4009 - accuracy: 0.4388 - val_loss: 0.4312 - val_accuracy: 0.3333\n",
      "Epoch 113/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3940 - accuracy: 0.4490 - val_loss: 0.4246 - val_accuracy: 0.3333\n",
      "Epoch 114/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3955 - accuracy: 0.4490 - val_loss: 0.4246 - val_accuracy: 0.3333\n",
      "Epoch 115/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3941 - accuracy: 0.4490 - val_loss: 0.4246 - val_accuracy: 0.3333\n",
      "Epoch 116/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3941 - accuracy: 0.4490 - val_loss: 0.4250 - val_accuracy: 0.3333\n",
      "Epoch 117/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3941 - accuracy: 0.4490 - val_loss: 0.4248 - val_accuracy: 0.3333\n",
      "Epoch 118/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3956 - accuracy: 0.4490 - val_loss: 0.4245 - val_accuracy: 0.3333\n",
      "Epoch 119/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3971 - accuracy: 0.4490 - val_loss: 0.4096 - val_accuracy: 0.3939\n",
      "Epoch 120/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3932 - accuracy: 0.4490 - val_loss: 0.4114 - val_accuracy: 0.3636\n",
      "Epoch 121/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3787 - accuracy: 0.5000 - val_loss: 0.4512 - val_accuracy: 0.3333\n",
      "Epoch 122/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3809 - accuracy: 0.4592 - val_loss: 0.4151 - val_accuracy: 0.3636\n",
      "Epoch 123/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3879 - accuracy: 0.4490 - val_loss: 0.4048 - val_accuracy: 0.3939\n",
      "Epoch 124/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3898 - accuracy: 0.4694 - val_loss: 0.4303 - val_accuracy: 0.3333\n",
      "Epoch 125/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3758 - accuracy: 0.4592 - val_loss: 0.4162 - val_accuracy: 0.3636\n",
      "Epoch 126/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3776 - accuracy: 0.4796 - val_loss: 0.4070 - val_accuracy: 0.3333\n",
      "Epoch 127/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3690 - accuracy: 0.5102 - val_loss: 0.3957 - val_accuracy: 0.3939\n",
      "Epoch 128/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3763 - accuracy: 0.4388 - val_loss: 0.3999 - val_accuracy: 0.3939\n",
      "Epoch 129/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3736 - accuracy: 0.4898 - val_loss: 0.4600 - val_accuracy: 0.3939\n",
      "Epoch 130/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3613 - accuracy: 0.5000 - val_loss: 0.4499 - val_accuracy: 0.3939\n",
      "Epoch 131/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3747 - accuracy: 0.4898 - val_loss: 0.4174 - val_accuracy: 0.4242\n",
      "Epoch 132/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3717 - accuracy: 0.4694 - val_loss: 0.5579 - val_accuracy: 0.4848\n",
      "Epoch 133/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4009 - accuracy: 0.4082 - val_loss: 0.3990 - val_accuracy: 0.3939\n",
      "Epoch 134/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3779 - accuracy: 0.4592 - val_loss: 0.3603 - val_accuracy: 0.4848\n",
      "Epoch 135/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3609 - accuracy: 0.4898 - val_loss: 0.3376 - val_accuracy: 0.5152\n",
      "Epoch 136/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4096 - accuracy: 0.4082 - val_loss: 0.4496 - val_accuracy: 0.3030\n",
      "Epoch 137/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4123 - accuracy: 0.4184 - val_loss: 0.4321 - val_accuracy: 0.3333\n",
      "Epoch 138/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3957 - accuracy: 0.4490 - val_loss: 0.4126 - val_accuracy: 0.3939\n",
      "Epoch 139/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3973 - accuracy: 0.4388 - val_loss: 0.4140 - val_accuracy: 0.3939\n",
      "Epoch 140/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3937 - accuracy: 0.4490 - val_loss: 0.4130 - val_accuracy: 0.3939\n",
      "Epoch 141/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3951 - accuracy: 0.4490 - val_loss: 0.4138 - val_accuracy: 0.3939\n",
      "Epoch 142/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3941 - accuracy: 0.4490 - val_loss: 0.4141 - val_accuracy: 0.3636\n",
      "Epoch 143/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3928 - accuracy: 0.4592 - val_loss: 0.4133 - val_accuracy: 0.3636\n",
      "Epoch 144/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4006 - accuracy: 0.4388 - val_loss: 0.4348 - val_accuracy: 0.3333\n",
      "Epoch 145/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3946 - accuracy: 0.4490 - val_loss: 0.4244 - val_accuracy: 0.3333\n",
      "Epoch 146/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3955 - accuracy: 0.4490 - val_loss: 0.5224 - val_accuracy: 0.3333\n",
      "Epoch 147/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3959 - accuracy: 0.4490 - val_loss: 0.4290 - val_accuracy: 0.3333\n",
      "Epoch 148/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3943 - accuracy: 0.4490 - val_loss: 0.4147 - val_accuracy: 0.3939\n",
      "Epoch 149/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3934 - accuracy: 0.4490 - val_loss: 0.4134 - val_accuracy: 0.3939\n",
      "Epoch 150/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3936 - accuracy: 0.4490 - val_loss: 0.4128 - val_accuracy: 0.3939\n",
      "Epoch 151/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3942 - accuracy: 0.4490 - val_loss: 0.4111 - val_accuracy: 0.3939\n",
      "Epoch 152/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3940 - accuracy: 0.4490 - val_loss: 0.4126 - val_accuracy: 0.3939\n",
      "Epoch 153/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3966 - accuracy: 0.4592 - val_loss: 0.5585 - val_accuracy: 0.3333\n",
      "Epoch 154/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3789 - accuracy: 0.4796 - val_loss: 0.4090 - val_accuracy: 0.3636\n",
      "Epoch 155/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3756 - accuracy: 0.4694 - val_loss: 0.4140 - val_accuracy: 0.3636\n",
      "Epoch 156/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3656 - accuracy: 0.4184 - val_loss: 0.3413 - val_accuracy: 0.5152\n",
      "Epoch 157/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3696 - accuracy: 0.4694 - val_loss: 0.4282 - val_accuracy: 0.3333\n",
      "Epoch 158/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4063 - accuracy: 0.4388 - val_loss: 0.4087 - val_accuracy: 0.3939\n",
      "Epoch 159/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3945 - accuracy: 0.4490 - val_loss: 0.4068 - val_accuracy: 0.3939\n",
      "Epoch 160/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3833 - accuracy: 0.4592 - val_loss: 0.8873 - val_accuracy: 0.4848\n",
      "Epoch 161/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3911 - accuracy: 0.4898 - val_loss: 0.3803 - val_accuracy: 0.4545\n",
      "Epoch 162/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3735 - accuracy: 0.4898 - val_loss: 0.3794 - val_accuracy: 0.4545\n",
      "Epoch 163/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3578 - accuracy: 0.4796 - val_loss: 0.3470 - val_accuracy: 0.5152\n",
      "Epoch 164/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3536 - accuracy: 0.5102 - val_loss: 0.3343 - val_accuracy: 0.5152\n",
      "Epoch 165/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3831 - accuracy: 0.4796 - val_loss: 0.4316 - val_accuracy: 0.3333\n",
      "Epoch 166/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4192 - accuracy: 0.4286 - val_loss: 0.4436 - val_accuracy: 0.3030\n",
      "Epoch 167/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4106 - accuracy: 0.4286 - val_loss: 0.4328 - val_accuracy: 0.3333\n",
      "Epoch 168/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3971 - accuracy: 0.4490 - val_loss: 0.4124 - val_accuracy: 0.3939\n",
      "Epoch 169/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3983 - accuracy: 0.4490 - val_loss: 0.4420 - val_accuracy: 0.3030\n",
      "Epoch 170/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4081 - accuracy: 0.4286 - val_loss: 0.4351 - val_accuracy: 0.3333\n",
      "Epoch 171/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4013 - accuracy: 0.4388 - val_loss: 0.4105 - val_accuracy: 0.3939\n",
      "Epoch 172/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3818 - accuracy: 0.4694 - val_loss: 0.4000 - val_accuracy: 0.3939\n",
      "Epoch 173/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3841 - accuracy: 0.4694 - val_loss: 0.3947 - val_accuracy: 0.3939\n",
      "Epoch 174/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3926 - accuracy: 0.4490 - val_loss: 0.4074 - val_accuracy: 0.3939\n",
      "Epoch 175/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3940 - accuracy: 0.4490 - val_loss: 0.4072 - val_accuracy: 0.3939\n",
      "Epoch 176/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3942 - accuracy: 0.4490 - val_loss: 0.4074 - val_accuracy: 0.3939\n",
      "Epoch 177/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3938 - accuracy: 0.4490 - val_loss: 0.4065 - val_accuracy: 0.3939\n",
      "Epoch 178/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3941 - accuracy: 0.4490 - val_loss: 0.4070 - val_accuracy: 0.3939\n",
      "Epoch 179/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3945 - accuracy: 0.4490 - val_loss: 0.4072 - val_accuracy: 0.3939\n",
      "Epoch 180/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3938 - accuracy: 0.4490 - val_loss: 0.4067 - val_accuracy: 0.3939\n",
      "Epoch 181/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3962 - accuracy: 0.4490 - val_loss: 0.4534 - val_accuracy: 0.3636\n",
      "Epoch 182/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3887 - accuracy: 0.4592 - val_loss: 0.4247 - val_accuracy: 0.3636\n",
      "Epoch 183/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3828 - accuracy: 0.4796 - val_loss: 0.4233 - val_accuracy: 0.3636\n",
      "Epoch 184/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3836 - accuracy: 0.4592 - val_loss: 0.7885 - val_accuracy: 0.3939\n",
      "Epoch 185/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3852 - accuracy: 0.4592 - val_loss: 0.4286 - val_accuracy: 0.3333\n",
      "Epoch 186/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.6133 - accuracy: 0.4490 - val_loss: 0.4325 - val_accuracy: 0.3333\n",
      "Epoch 187/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4143 - accuracy: 0.4184 - val_loss: 0.4308 - val_accuracy: 0.3333\n",
      "Epoch 188/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4020 - accuracy: 0.4388 - val_loss: 0.4308 - val_accuracy: 0.3333\n",
      "Epoch 189/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3979 - accuracy: 0.4490 - val_loss: 0.4303 - val_accuracy: 0.3333\n",
      "Epoch 190/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3944 - accuracy: 0.4490 - val_loss: 0.4305 - val_accuracy: 0.3333\n",
      "Epoch 191/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4036 - accuracy: 0.4388 - val_loss: 0.4316 - val_accuracy: 0.3333\n",
      "Epoch 192/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3950 - accuracy: 0.4490 - val_loss: 0.4306 - val_accuracy: 0.3333\n",
      "Epoch 193/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3947 - accuracy: 0.4490 - val_loss: 0.4300 - val_accuracy: 0.3333\n",
      "Epoch 194/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3943 - accuracy: 0.4490 - val_loss: 0.4296 - val_accuracy: 0.3333\n",
      "Epoch 195/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3950 - accuracy: 0.4490 - val_loss: 0.4293 - val_accuracy: 0.3333\n",
      "Epoch 196/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3941 - accuracy: 0.4490 - val_loss: 0.4286 - val_accuracy: 0.3333\n",
      "Epoch 197/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3933 - accuracy: 0.4490 - val_loss: 0.4288 - val_accuracy: 0.3333\n",
      "Epoch 198/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3944 - accuracy: 0.4490 - val_loss: 0.4272 - val_accuracy: 0.3333\n",
      "Epoch 199/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3951 - accuracy: 0.4490 - val_loss: 0.4289 - val_accuracy: 0.3333\n",
      "Epoch 200/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3938 - accuracy: 0.4490 - val_loss: 0.4276 - val_accuracy: 0.3333\n",
      "Epoch 201/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3934 - accuracy: 0.4490 - val_loss: 0.4268 - val_accuracy: 0.3333\n",
      "Epoch 202/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3933 - accuracy: 0.4490 - val_loss: 0.4274 - val_accuracy: 0.3333\n",
      "Epoch 203/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4036 - accuracy: 0.4388 - val_loss: 0.4272 - val_accuracy: 0.3333\n",
      "Epoch 204/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3948 - accuracy: 0.4490 - val_loss: 0.4136 - val_accuracy: 0.3636\n",
      "Epoch 205/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3938 - accuracy: 0.4490 - val_loss: 0.4139 - val_accuracy: 0.3636\n",
      "Epoch 206/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3942 - accuracy: 0.4490 - val_loss: 0.4153 - val_accuracy: 0.3636\n",
      "Epoch 207/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3939 - accuracy: 0.4490 - val_loss: 0.4160 - val_accuracy: 0.3636\n",
      "Epoch 208/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3940 - accuracy: 0.4490 - val_loss: 0.4148 - val_accuracy: 0.3636\n",
      "Epoch 209/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3935 - accuracy: 0.4490 - val_loss: 0.4160 - val_accuracy: 0.3636\n",
      "Epoch 210/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3956 - accuracy: 0.4490 - val_loss: 0.4178 - val_accuracy: 0.3636\n",
      "Epoch 211/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3944 - accuracy: 0.4490 - val_loss: 0.4155 - val_accuracy: 0.3636\n",
      "Epoch 212/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3940 - accuracy: 0.4490 - val_loss: 0.4155 - val_accuracy: 0.3636\n",
      "Epoch 213/250\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 0.3936 - accuracy: 0.4490 - val_loss: 0.4160 - val_accuracy: 0.3636\n",
      "Epoch 214/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3935 - accuracy: 0.4490 - val_loss: 0.4155 - val_accuracy: 0.3636\n",
      "Epoch 215/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3944 - accuracy: 0.4490 - val_loss: 0.4150 - val_accuracy: 0.3636\n",
      "Epoch 216/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3948 - accuracy: 0.4490 - val_loss: 0.4155 - val_accuracy: 0.3636\n",
      "Epoch 217/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3939 - accuracy: 0.4490 - val_loss: 0.4179 - val_accuracy: 0.3636\n",
      "Epoch 218/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3937 - accuracy: 0.4490 - val_loss: 0.4184 - val_accuracy: 0.3636\n",
      "Epoch 219/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3940 - accuracy: 0.4490 - val_loss: 0.4175 - val_accuracy: 0.3636\n",
      "Epoch 220/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3933 - accuracy: 0.4490 - val_loss: 0.4182 - val_accuracy: 0.3636\n",
      "Epoch 221/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3933 - accuracy: 0.4490 - val_loss: 0.4181 - val_accuracy: 0.3636\n",
      "Epoch 222/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3936 - accuracy: 0.4490 - val_loss: 0.4178 - val_accuracy: 0.3636\n",
      "Epoch 223/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3936 - accuracy: 0.4490 - val_loss: 0.4180 - val_accuracy: 0.3636\n",
      "Epoch 224/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3934 - accuracy: 0.4490 - val_loss: 0.4173 - val_accuracy: 0.3636\n",
      "Epoch 225/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3935 - accuracy: 0.4490 - val_loss: 0.4310 - val_accuracy: 0.3333\n",
      "Epoch 226/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3996 - accuracy: 0.4388 - val_loss: 0.4854 - val_accuracy: 0.3333\n",
      "Epoch 227/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3973 - accuracy: 0.4592 - val_loss: 0.7721 - val_accuracy: 0.4242\n",
      "Epoch 228/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3942 - accuracy: 0.4490 - val_loss: 0.4120 - val_accuracy: 0.3939\n",
      "Epoch 229/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3937 - accuracy: 0.4490 - val_loss: 0.4134 - val_accuracy: 0.3939\n",
      "Epoch 230/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3927 - accuracy: 0.4592 - val_loss: 0.4171 - val_accuracy: 0.3636\n",
      "Epoch 231/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3875 - accuracy: 0.4592 - val_loss: 0.4274 - val_accuracy: 0.3333\n",
      "Epoch 232/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3795 - accuracy: 0.4694 - val_loss: 0.4321 - val_accuracy: 0.4242\n",
      "Epoch 233/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3869 - accuracy: 0.4694 - val_loss: 0.3987 - val_accuracy: 0.3939\n",
      "Epoch 234/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3752 - accuracy: 0.4694 - val_loss: 0.3981 - val_accuracy: 0.3939\n",
      "Epoch 235/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3724 - accuracy: 0.4898 - val_loss: 0.3723 - val_accuracy: 0.4545\n",
      "Epoch 236/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3812 - accuracy: 0.5000 - val_loss: 0.3539 - val_accuracy: 0.4848\n",
      "Epoch 237/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3611 - accuracy: 0.4796 - val_loss: 0.3750 - val_accuracy: 0.4848\n",
      "Epoch 238/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3672 - accuracy: 0.4898 - val_loss: 0.4106 - val_accuracy: 0.3939\n",
      "Epoch 239/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3649 - accuracy: 0.5000 - val_loss: 0.3599 - val_accuracy: 0.4545\n",
      "Epoch 240/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3586 - accuracy: 0.5000 - val_loss: 0.3573 - val_accuracy: 0.4545\n",
      "Epoch 241/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3564 - accuracy: 0.5000 - val_loss: 0.3388 - val_accuracy: 0.4848\n",
      "Epoch 242/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3597 - accuracy: 0.4796 - val_loss: 0.3694 - val_accuracy: 0.4545\n",
      "Epoch 243/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3599 - accuracy: 0.5000 - val_loss: 0.3386 - val_accuracy: 0.4848\n",
      "Epoch 244/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3615 - accuracy: 0.4286 - val_loss: 0.7402 - val_accuracy: 0.2727\n",
      "Epoch 245/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4140 - accuracy: 0.4286 - val_loss: 0.4090 - val_accuracy: 0.3939\n",
      "Epoch 246/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4444 - accuracy: 0.4490 - val_loss: 0.4124 - val_accuracy: 0.3636\n",
      "Epoch 247/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3781 - accuracy: 0.4796 - val_loss: 0.3967 - val_accuracy: 0.3939\n",
      "Epoch 248/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3646 - accuracy: 0.4796 - val_loss: 0.3882 - val_accuracy: 0.4545\n",
      "Epoch 249/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3563 - accuracy: 0.5000 - val_loss: 0.3595 - val_accuracy: 0.4545\n",
      "Epoch 250/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3485 - accuracy: 0.5000 - val_loss: 0.3387 - val_accuracy: 0.5152\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.553</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.504</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.513</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.273</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.494</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.273</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.498</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.273</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.444</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.364</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.378</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.394</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.365</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.455</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.356</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.455</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.348</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.515</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     loss  accuracy  val_loss  val_accuracy  epoch\n",
       "0   0.553     0.265     0.496         0.273      0\n",
       "1   0.504     0.286     0.501         0.273      1\n",
       "2   0.513     0.306     0.486         0.273      2\n",
       "3   0.494     0.306     0.495         0.273      3\n",
       "4   0.498     0.276     0.495         0.273      4\n",
       "..    ...       ...       ...           ...    ...\n",
       "245 0.444     0.449     0.412         0.364    245\n",
       "246 0.378     0.480     0.397         0.394    246\n",
       "247 0.365     0.480     0.388         0.455    247\n",
       "248 0.356     0.500     0.360         0.455    248\n",
       "249 0.348     0.500     0.339         0.515    249\n",
       "\n",
       "[250 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Curvas de Pérdida=Training Loss<br>Epoch=%{x}<br>Binary Cross Entropy=%{y}<extra></extra>",
         "legendgroup": "Training Loss",
         "line": {
          "color": "#46039f",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Training Loss",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249
         ],
         "xaxis": "x",
         "y": [
          0.5529754757881165,
          0.5042559504508972,
          0.5126374959945679,
          0.49403202533721924,
          0.49800336360931396,
          0.49421313405036926,
          0.49320435523986816,
          0.4929971992969513,
          0.49237310886383057,
          0.4927685856819153,
          0.49293941259384155,
          0.4931182861328125,
          0.4801253378391266,
          0.43448421359062195,
          0.4119327664375305,
          0.41140323877334595,
          0.41750064492225647,
          0.41007500886917114,
          0.39888131618499756,
          0.39675939083099365,
          0.39466574788093567,
          0.3979552984237671,
          0.41276997327804565,
          0.39895325899124146,
          0.39733457565307617,
          0.3968777656555176,
          0.40096473693847656,
          0.40544748306274414,
          0.4026302099227905,
          0.40131762623786926,
          0.40145623683929443,
          0.3958076536655426,
          0.40346619486808777,
          0.39397120475769043,
          0.39055848121643066,
          0.3942125737667084,
          0.3946836292743683,
          0.3916277587413788,
          0.39214709401130676,
          0.39599066972732544,
          0.39304643869400024,
          0.3940396308898926,
          0.394733726978302,
          0.39145976305007935,
          0.3920516073703766,
          0.39158642292022705,
          0.39200225472450256,
          0.3880554437637329,
          0.39031291007995605,
          0.3866627812385559,
          0.4409746825695038,
          0.40121206641197205,
          0.39156320691108704,
          0.39655444025993347,
          0.44908756017684937,
          0.43177270889282227,
          0.4089440703392029,
          0.39922717213630676,
          0.39486557245254517,
          0.394329696893692,
          0.39490753412246704,
          0.40162768959999084,
          0.39416787028312683,
          0.3939782679080963,
          0.3943597376346588,
          0.3956416845321655,
          0.39427733421325684,
          0.3952527344226837,
          0.39462900161743164,
          0.39389508962631226,
          0.3943313658237457,
          0.395541787147522,
          0.4032900333404541,
          0.3932575583457947,
          0.42176714539527893,
          0.4044428765773773,
          0.4046539068222046,
          0.3947514593601227,
          0.3950803279876709,
          0.395648330450058,
          0.3967057168483734,
          0.3952610492706299,
          0.3984084129333496,
          0.40210485458374023,
          0.3919708728790283,
          0.38862141966819763,
          0.3892322778701782,
          0.3783951997756958,
          0.37925729155540466,
          0.3767573833465576,
          0.39642253518104553,
          0.3871728479862213,
          0.381206214427948,
          0.3800791800022125,
          0.37773677706718445,
          0.4618888795375824,
          0.4088132679462433,
          0.40733036398887634,
          0.39741313457489014,
          0.41225001215934753,
          0.3755485415458679,
          0.386705607175827,
          0.3829759955406189,
          0.38942715525627136,
          0.3796156346797943,
          0.3820447623729706,
          0.37285497784614563,
          0.37107351422309875,
          0.3844532370567322,
          0.3836815357208252,
          0.47391316294670105,
          0.4009377360343933,
          0.39400166273117065,
          0.3955168128013611,
          0.3941074013710022,
          0.39408478140830994,
          0.39405515789985657,
          0.3956063687801361,
          0.39709189534187317,
          0.39321592450141907,
          0.3787066340446472,
          0.38094791769981384,
          0.3878993093967438,
          0.3898152709007263,
          0.3758103847503662,
          0.3775827884674072,
          0.3689831793308258,
          0.37628474831581116,
          0.37361133098602295,
          0.3612685203552246,
          0.3746711313724518,
          0.371694415807724,
          0.4008695185184479,
          0.3779103457927704,
          0.36092609167099,
          0.40959659218788147,
          0.4122847318649292,
          0.39567437767982483,
          0.397288978099823,
          0.3936825692653656,
          0.3951067328453064,
          0.39413657784461975,
          0.39284399151802063,
          0.40061312913894653,
          0.394610196352005,
          0.39547616243362427,
          0.39589551091194153,
          0.39425599575042725,
          0.3933793008327484,
          0.39360058307647705,
          0.39415183663368225,
          0.3940024971961975,
          0.3965781033039093,
          0.3788735866546631,
          0.37555551528930664,
          0.365622341632843,
          0.36957716941833496,
          0.4062841236591339,
          0.3944559693336487,
          0.38325628638267517,
          0.3910767734050751,
          0.37346091866493225,
          0.3578202426433563,
          0.3535749912261963,
          0.38313761353492737,
          0.419185996055603,
          0.41064873337745667,
          0.3970979154109955,
          0.39830729365348816,
          0.40810543298721313,
          0.4013066589832306,
          0.38182851672172546,
          0.38406920433044434,
          0.39260467886924744,
          0.3939526677131653,
          0.39416974782943726,
          0.39377516508102417,
          0.39408496022224426,
          0.3945481777191162,
          0.39375290274620056,
          0.3962405025959015,
          0.38868358731269836,
          0.3828013241291046,
          0.3835584819316864,
          0.3851732611656189,
          0.6132704615592957,
          0.4143485724925995,
          0.4019778370857239,
          0.3979230523109436,
          0.39443397521972656,
          0.4035574793815613,
          0.39497900009155273,
          0.3947004973888397,
          0.3943079710006714,
          0.39499926567077637,
          0.3941074013710022,
          0.3932742774486542,
          0.3943856656551361,
          0.3950519561767578,
          0.39379119873046875,
          0.3933683931827545,
          0.3932873606681824,
          0.40364643931388855,
          0.39480918645858765,
          0.39380010962486267,
          0.39418068528175354,
          0.39391273260116577,
          0.3939646780490875,
          0.3934580385684967,
          0.3956054449081421,
          0.39442747831344604,
          0.39396044611930847,
          0.3935762047767639,
          0.3935225009918213,
          0.39435502886772156,
          0.3948412835597992,
          0.39391377568244934,
          0.393707811832428,
          0.39396265149116516,
          0.3932512700557709,
          0.3932735323905945,
          0.39357656240463257,
          0.39357390999794006,
          0.39344877004623413,
          0.39351528882980347,
          0.399586021900177,
          0.3972969949245453,
          0.39420485496520996,
          0.3937124013900757,
          0.3926915228366852,
          0.3874955177307129,
          0.3794586658477783,
          0.38693949580192566,
          0.37520915269851685,
          0.3723699152469635,
          0.3812377452850342,
          0.36107954382896423,
          0.367212176322937,
          0.3649221956729889,
          0.3585944175720215,
          0.3563932180404663,
          0.35969120264053345,
          0.3599044382572174,
          0.36148595809936523,
          0.41396912932395935,
          0.44442060589790344,
          0.37809258699417114,
          0.3646377921104431,
          0.3563147485256195,
          0.34847521781921387
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Curvas de Pérdida=Validation Loss<br>Epoch=%{x}<br>Binary Cross Entropy=%{y}<extra></extra>",
         "legendgroup": "Validation Loss",
         "line": {
          "color": "#fb9f3a",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Validation Loss",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249
         ],
         "xaxis": "x",
         "y": [
          0.49606022238731384,
          0.5013682246208191,
          0.4863106608390808,
          0.49505695700645447,
          0.49526670575141907,
          0.4956916570663452,
          0.4926286041736603,
          0.4877071976661682,
          0.4898199141025543,
          0.49206942319869995,
          0.49037349224090576,
          0.4884055256843567,
          0.4604892134666443,
          0.4413769841194153,
          0.48169755935668945,
          0.5278377532958984,
          0.4322648048400879,
          0.43470507860183716,
          0.4308280646800995,
          0.42949292063713074,
          0.43033429980278015,
          0.4994715452194214,
          0.4280385971069336,
          0.4350777864456177,
          0.4392094314098358,
          0.46549418568611145,
          0.43332239985466003,
          0.4318487048149109,
          0.44415193796157837,
          0.42827820777893066,
          0.4228256940841675,
          0.4417194128036499,
          0.4225882887840271,
          0.4448526203632355,
          0.43390217423439026,
          0.44106149673461914,
          0.4333893954753876,
          0.4410444498062134,
          0.46951374411582947,
          0.49836552143096924,
          0.4339986741542816,
          0.4914572238922119,
          0.41456955671310425,
          0.44623735547065735,
          0.4178001284599304,
          0.4346088767051697,
          0.42064517736434937,
          0.41846662759780884,
          0.6020777225494385,
          0.7042387127876282,
          0.43324053287506104,
          0.42498311400413513,
          0.4773402810096741,
          0.5864928364753723,
          0.4586344361305237,
          0.4438076615333557,
          0.4353603720664978,
          0.42582470178604126,
          0.42467567324638367,
          0.42383143305778503,
          0.4215007722377777,
          0.45118191838264465,
          0.4208407402038574,
          0.42065879702568054,
          0.42131268978118896,
          0.4222867488861084,
          0.4216836094856262,
          0.42057302594184875,
          0.421808123588562,
          0.4210534691810608,
          0.4206801652908325,
          0.4214578866958618,
          0.5275196433067322,
          0.7425399422645569,
          0.42984795570373535,
          0.43038538098335266,
          0.4292829930782318,
          0.43046271800994873,
          0.4294278025627136,
          0.42929986119270325,
          0.4215956926345825,
          0.4422706067562103,
          0.48111411929130554,
          0.4075499475002289,
          0.4084661900997162,
          0.40634849667549133,
          0.4014773666858673,
          0.39987295866012573,
          0.38917043805122375,
          0.40665316581726074,
          0.4111258089542389,
          0.46276357769966125,
          0.39676567912101746,
          0.3981969952583313,
          0.6675547361373901,
          0.4428115785121918,
          0.42260706424713135,
          0.43406105041503906,
          0.42636987566947937,
          0.4071507751941681,
          0.39411431550979614,
          0.41373130679130554,
          0.40676429867744446,
          0.39869606494903564,
          0.38978907465934753,
          0.38982322812080383,
          0.3687257170677185,
          0.4150562882423401,
          0.3956618309020996,
          0.37040963768959045,
          0.41111379861831665,
          0.43118616938591003,
          0.4246227443218231,
          0.4245830476284027,
          0.4246024787425995,
          0.42495161294937134,
          0.4248492121696472,
          0.4245195984840393,
          0.40963149070739746,
          0.4114070534706116,
          0.45115000009536743,
          0.4151143729686737,
          0.4047979414463043,
          0.43027377128601074,
          0.41623249650001526,
          0.40695348381996155,
          0.39565974473953247,
          0.39989611506462097,
          0.4600166976451874,
          0.44994503259658813,
          0.4174119234085083,
          0.5579366683959961,
          0.39898988604545593,
          0.36030134558677673,
          0.33764272928237915,
          0.44955819845199585,
          0.4321337044239044,
          0.41259709000587463,
          0.4139859080314636,
          0.4129675030708313,
          0.41384702920913696,
          0.4140627086162567,
          0.41331446170806885,
          0.43483278155326843,
          0.4243568181991577,
          0.5224103927612305,
          0.4290078580379486,
          0.4147176444530487,
          0.4133557379245758,
          0.4128263294696808,
          0.4110943377017975,
          0.41264042258262634,
          0.5584617257118225,
          0.4089621305465698,
          0.414028525352478,
          0.34133848547935486,
          0.4281890094280243,
          0.4086674153804779,
          0.40679433941841125,
          0.887310266494751,
          0.38030362129211426,
          0.37938398122787476,
          0.34695056080818176,
          0.3343432545661926,
          0.4315517246723175,
          0.4436355531215668,
          0.43277812004089355,
          0.41240501403808594,
          0.441962867975235,
          0.435147762298584,
          0.41054606437683105,
          0.39997926354408264,
          0.3947492837905884,
          0.4073880612850189,
          0.4071851670742035,
          0.4073794186115265,
          0.40654319524765015,
          0.40701597929000854,
          0.40723466873168945,
          0.4066733717918396,
          0.453382283449173,
          0.4247340261936188,
          0.4233167767524719,
          0.7885133624076843,
          0.428647518157959,
          0.4324999451637268,
          0.4308026432991028,
          0.430842787027359,
          0.43032965064048767,
          0.4305340349674225,
          0.4315800666809082,
          0.4305559992790222,
          0.4300166368484497,
          0.4296353757381439,
          0.42931032180786133,
          0.42864274978637695,
          0.4287669062614441,
          0.4272422194480896,
          0.42886632680892944,
          0.4276379346847534,
          0.4268413782119751,
          0.42738229036331177,
          0.4271892309188843,
          0.4136447310447693,
          0.4138821065425873,
          0.41525912284851074,
          0.41595637798309326,
          0.4147874712944031,
          0.4160173237323761,
          0.41779205203056335,
          0.4154556691646576,
          0.41545259952545166,
          0.41601645946502686,
          0.415532648563385,
          0.4150136113166809,
          0.41554781794548035,
          0.4178805351257324,
          0.4184498190879822,
          0.4174775183200836,
          0.4182446002960205,
          0.41806501150131226,
          0.41779884696006775,
          0.4180046319961548,
          0.41730615496635437,
          0.43104496598243713,
          0.48536214232444763,
          0.7720997929573059,
          0.41201749444007874,
          0.41335898637771606,
          0.4170903265476227,
          0.4273608326911926,
          0.4320913851261139,
          0.3986595869064331,
          0.3981267511844635,
          0.3722798824310303,
          0.353915274143219,
          0.3749682605266571,
          0.41058558225631714,
          0.3598591983318329,
          0.3573097884654999,
          0.33878952264785767,
          0.3693896532058716,
          0.3386386036872864,
          0.7402289509773254,
          0.4090196490287781,
          0.41235244274139404,
          0.39669787883758545,
          0.38817957043647766,
          0.35950690507888794,
          0.3386527895927429
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "Curvas de Pérdida"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Gráficas de Pérdida de Entrenamiento y Evaluación"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Binary Cross Entropy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.553</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.504</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.513</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.273</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.494</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.273</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.498</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.273</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.444</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.364</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.378</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.394</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.365</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.455</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.356</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.455</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.348</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.515</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Training Loss  accuracy  Validation Loss  val_accuracy  epoch\n",
       "0            0.553     0.265            0.496         0.273      0\n",
       "1            0.504     0.286            0.501         0.273      1\n",
       "2            0.513     0.306            0.486         0.273      2\n",
       "3            0.494     0.306            0.495         0.273      3\n",
       "4            0.498     0.276            0.495         0.273      4\n",
       "..             ...       ...              ...           ...    ...\n",
       "245          0.444     0.449            0.412         0.364    245\n",
       "246          0.378     0.480            0.397         0.394    246\n",
       "247          0.365     0.480            0.388         0.455    247\n",
       "248          0.356     0.500            0.360         0.455    248\n",
       "249          0.348     0.500            0.339         0.515    249\n",
       "\n",
       "[250 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 200\n",
    "batch_size = 1\n",
    "#model = my_model(learning_rate)\n",
    "history= train_model(model, train_features, train_labels, epochs, batch_size)\n",
    "display(history)\n",
    "loss_curves(history)\n",
    "#view_filters(model)\n",
    "display(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normal</th>\n",
       "      <th>collision_in_tool</th>\n",
       "      <th>collision_in_part</th>\n",
       "      <th>bottom_collision</th>\n",
       "      <th>bottom_obstruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.179</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.378</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.297</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.081</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.332</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.170</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.324</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.372</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    normal  collision_in_tool  collision_in_part  bottom_collision  \\\n",
       "0    0.179              0.161              0.445             0.192   \n",
       "1    0.000              0.000              0.000             0.000   \n",
       "2    0.000              0.000              0.000             0.984   \n",
       "3    0.378              0.155              0.402             0.059   \n",
       "4    0.297              0.158              0.443             0.092   \n",
       "..     ...                ...                ...               ...   \n",
       "28   0.081              0.129              0.400             0.347   \n",
       "29   0.332              0.157              0.426             0.076   \n",
       "30   0.170              0.158              0.446             0.201   \n",
       "31   0.324              0.160              0.427             0.081   \n",
       "32   0.372              0.156              0.405             0.061   \n",
       "\n",
       "    bottom_obstruction  \n",
       "0                0.024  \n",
       "1                1.000  \n",
       "2                0.015  \n",
       "3                0.006  \n",
       "4                0.010  \n",
       "..                 ...  \n",
       "28               0.044  \n",
       "29               0.008  \n",
       "30               0.024  \n",
       "31               0.009  \n",
       "32               0.007  \n",
       "\n",
       "[33 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1  2  3  4  5\n",
       "135  0  0  0  1  0\n",
       "115  0  0  0  0  1\n",
       "131  0  0  0  0  1\n",
       "55   1  0  0  0  0\n",
       "95   0  1  0  0  0\n",
       "..  .. .. .. .. ..\n",
       "134  0  0  0  1  0\n",
       "160  0  1  0  0  0\n",
       "139  0  0  0  1  0\n",
       "78   0  0  1  0  0\n",
       "60   1  0  0  0  0\n",
       "\n",
       "[33 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = model.predict(test_features)\n",
    "predictions_df = pd.DataFrame(predictions, columns=['normal', 'collision_in_tool', 'collision_in_part', 'bottom_collision', 'bottom_obstruction'])\n",
    "display(predictions_df)\n",
    "display(test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
