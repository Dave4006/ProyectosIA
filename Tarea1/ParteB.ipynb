{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar Modulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import plotly.express as px\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                labels 1  2  3  4  5 \n",
       "0               normal               \n",
       "1               normal               \n",
       "2               normal               \n",
       "3               normal               \n",
       "4               normal               \n",
       "..                 ... .. .. .. .. ..\n",
       "159  collision_in_tool               \n",
       "160  collision_in_tool               \n",
       "161  collision_in_tool               \n",
       "162  collision_in_tool               \n",
       "163  collision_in_tool               \n",
       "\n",
       "[164 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ -2.,  -1.,  81.,   0.,  -5.,   0.],\n",
       "        [ -2.,  -1.,  79.,   0.,  -4.,   0.],\n",
       "        [ -2.,  -1.,  79.,   0.,  -4.,   0.],\n",
       "        ...,\n",
       "        [ -2.,  -1.,  78.,   0.,  -5.,   0.],\n",
       "        [ -3.,  -1.,  80.,   1.,  -4.,   1.],\n",
       "        [ -2.,  -1.,  79.,   0.,  -4.,   0.]],\n",
       "\n",
       "       [[  6.,  -1.,  79.,  -2.,   4.,  -3.],\n",
       "        [ 42.,  -3.,  80.,   5.,  53.,   3.],\n",
       "        [ -5.,   4.,  74., -15., -10.,  -1.],\n",
       "        ...,\n",
       "        [ -1.,  -5.,  80.,   6.,  -6.,   0.],\n",
       "        [ -4.,   5.,  78., -14.,  -9.,  -4.],\n",
       "        [ -4.,   1.,  80.,  -3., -12.,   5.]],\n",
       "\n",
       "       [[ -2.,  -6.,  85.,  14.,  -5.,   2.],\n",
       "        [  0.,   2.,  74.,  -7.,   1.,   0.],\n",
       "        [ -4.,  -5.,  76.,   7., -11.,   4.],\n",
       "        ...,\n",
       "        [  0.,  -9.,  87.,  13.,  -5.,   2.],\n",
       "        [ -5.,   5.,  67., -17., -16.,   7.],\n",
       "        [ -6., -10.,  86.,  16., -14.,  -1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-12.,  17.,   3., -19., -10.,  -4.],\n",
       "        [-12.,  12.,  11., -13., -16.,  -4.],\n",
       "        [ -8.,   3.,   6.,   2., -11.,  -4.],\n",
       "        ...,\n",
       "        [  0.,   1.,   3.,   1.,   1.,  -3.],\n",
       "        [  1.,   2.,   9.,   0.,   3.,  -4.],\n",
       "        [  1.,   2.,   9.,   0.,   3.,  -4.]],\n",
       "\n",
       "       [[-41.,  21.,  -5., -23., -59.,  -4.],\n",
       "        [-32.,  21.,  -6., -25., -45.,  -4.],\n",
       "        [-21.,  12.,  -6., -14., -31.,  -4.],\n",
       "        ...,\n",
       "        [ -4.,   4.,   3.,   0.,  -1.,  -3.],\n",
       "        [ -4.,   3.,   2.,   2.,  -3.,  -3.],\n",
       "        [ -2.,   3.,   5.,   0.,  -2.,  -3.]],\n",
       "\n",
       "       [[  9., -10., -11.,  17.,   7.,  -4.],\n",
       "        [  5.,   0.,   4.,   0.,   7.,  -4.],\n",
       "        [ -3.,   6.,  -2.,  -8.,  -8.,  -4.],\n",
       "        ...,\n",
       "        [ -1.,   1.,  -3.,  -3.,  -2.,  -3.],\n",
       "        [  0.,  -1.,  -5.,  -1.,   1.,  -3.],\n",
       "        [ -1.,   1.,   4.,   0.,  -1.,  -3.]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.3f}\".format\n",
    "\n",
    "data = np.loadtxt(\"lp5.csv\", delimiter=\",\", dtype=str)\n",
    "data_copy = data #va de 0 a 2623 o sea 2624 datos\n",
    "cont = 16\n",
    "labels = []\n",
    "numbers = []\n",
    "features= np.zeros((164,15,6))\n",
    "\n",
    "for i in range(len(data_copy)):\n",
    "    if(cont == 16):\n",
    "        labels.append(data_copy[i])\n",
    "    if(cont<16):\n",
    "        numbers.append(data_copy[i])\n",
    "    cont -= 1\n",
    "    if(cont == 0):\n",
    "        cont = 16\n",
    "        \n",
    "cont = 0\n",
    "for i in range(164):\n",
    "    for j in range(15):\n",
    "        for z in range(6):\n",
    "            features[i][j][z] = numbers[cont][z]\n",
    "        cont += 1\n",
    "        \n",
    "labels_df = pd.DataFrame(labels, columns=['labels','1','2','3','4','5'])\n",
    "#labels_df = labels_df.loc[:,['labels']]\n",
    "display(labels_df)\n",
    "display(features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manejo de Datos y Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dave4\\AppData\\Local\\Temp\\ipykernel_12160\\1747463221.py:8: FutureWarning:\n",
      "\n",
      "In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.77660263, 0.78156702, 0.79084826, 0.77055903, 0.7684006 ,\n",
       "         0.77422836],\n",
       "        [0.77466005, 0.78739478, 0.78717893, 0.75156486, 0.7638679 ,\n",
       "         0.77206993],\n",
       "        [0.77401252, 0.78804231, 0.79041658, 0.74465789, 0.76106195,\n",
       "         0.77012735],\n",
       "        ...,\n",
       "        [0.78005612, 0.7804878 , 0.78437298, 0.78135118, 0.78113533,\n",
       "         0.78027196],\n",
       "        [0.78027196, 0.78091949, 0.77962443, 0.78091949, 0.78027196,\n",
       "         0.78027196],\n",
       "        [0.78027196, 0.78070365, 0.78286208, 0.78156702, 0.78091949,\n",
       "         0.78005612]],\n",
       "\n",
       "       [[0.78027196, 0.77940859, 0.79905029, 0.78372545, 0.77962443,\n",
       "         0.78113533],\n",
       "        [0.78070365, 0.78113533, 0.79667602, 0.77919275, 0.78091949,\n",
       "         0.78070365],\n",
       "        [0.77984028, 0.77962443, 0.79710771, 0.78221455, 0.77832938,\n",
       "         0.78156702],\n",
       "        ...,\n",
       "        [0.78070365, 0.77876106, 0.79948198, 0.78350961, 0.77962443,\n",
       "         0.78113533],\n",
       "        [0.77962443, 0.78178286, 0.79516512, 0.77703432, 0.77725016,\n",
       "         0.78221455],\n",
       "        [0.77940859, 0.77854522, 0.79926613, 0.78415713, 0.77768185,\n",
       "         0.7804878 ]],\n",
       "\n",
       "       [[0.78502051, 0.77725016, 0.78264623, 0.78631556, 0.78696309,\n",
       "         0.7804878 ],\n",
       "        [0.78588388, 0.78005612, 0.78717893, 0.78243039, 0.78890568,\n",
       "         0.77984028],\n",
       "        [0.78653141, 0.77876106, 0.78027196, 0.78480466, 0.78933736,\n",
       "         0.78027196],\n",
       "        ...,\n",
       "        [0.78005612, 0.78135118, 0.77962443, 0.7804878 , 0.77962443,\n",
       "         0.77962443],\n",
       "        [0.78005612, 0.78091949, 0.7789769 , 0.78091949, 0.78027196,\n",
       "         0.77962443],\n",
       "        [0.7804878 , 0.78091949, 0.78005612, 0.78156702, 0.78070365,\n",
       "         0.77962443]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.78005612, 0.78070365, 0.79689186, 0.77919275, 0.77660263,\n",
       "         0.77984028],\n",
       "        [0.77962443, 0.77832938, 0.79883445, 0.78307792, 0.77617095,\n",
       "         0.77811353],\n",
       "        [0.78005612, 0.78264623, 0.79516512, 0.77487589, 0.77681848,\n",
       "         0.7789769 ],\n",
       "        ...,\n",
       "        [0.7804878 , 0.77832938, 0.79969782, 0.78307792, 0.777466  ,\n",
       "         0.77854522],\n",
       "        [0.77940859, 0.77984028, 0.79710771, 0.77962443, 0.77573926,\n",
       "         0.77984028],\n",
       "        [0.77940859, 0.78243039, 0.79710771, 0.77530758, 0.77617095,\n",
       "         0.77940859]],\n",
       "\n",
       "       [[0.77077488, 0.79494928, 0.77617095, 0.75588172, 0.77832938,\n",
       "         0.7804878 ],\n",
       "        [0.7653788 , 0.78135118, 0.78243039, 0.777466  , 0.7638679 ,\n",
       "         0.78027196],\n",
       "        [0.76365206, 0.78264623, 0.78350961, 0.78005612, 0.75674509,\n",
       "         0.78027196],\n",
       "        ...,\n",
       "        [0.77984028, 0.78113533, 0.78243039, 0.78113533, 0.78005612,\n",
       "         0.78005612],\n",
       "        [0.78027196, 0.78113533, 0.78350961, 0.78091949, 0.78070365,\n",
       "         0.78027196],\n",
       "        [0.7804878 , 0.78091949, 0.78480466, 0.78221455, 0.78156702,\n",
       "         0.78005612]],\n",
       "\n",
       "       [[0.77789769, 0.77789769, 0.7744442 , 0.78696309, 0.78653141,\n",
       "         0.77789769],\n",
       "        [0.77832938, 0.77811353, 0.77422836, 0.78631556, 0.78868983,\n",
       "         0.7789769 ],\n",
       "        [0.77832938, 0.77789769, 0.77466005, 0.78696309, 0.78804231,\n",
       "         0.77768185],\n",
       "        ...,\n",
       "        [0.67882581, 0.70472696, 0.09842435, 0.7759551 , 0.85732787,\n",
       "         0.74940643],\n",
       "        [0.69933089, 0.73796676, 0.34448521, 0.77099072, 0.81674941,\n",
       "         0.77530758],\n",
       "        [0.72199439, 0.73170732, 0.7155191 , 0.81804446, 0.69156054,\n",
       "         0.77012735]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1  2  3  4  5\n",
       "84   0  0  1  0  0\n",
       "2    1  0  0  0  0\n",
       "94   0  1  0  0  0\n",
       "45   0  0  1  0  0\n",
       "42   0  0  1  0  0\n",
       "..  .. .. .. .. ..\n",
       "71   0  0  1  0  0\n",
       "106  0  0  0  1  0\n",
       "14   1  0  0  0  0\n",
       "92   0  1  0  0  0\n",
       "102  0  0  0  0  1\n",
       "\n",
       "[131 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes_dict = {'normal':'1 0 0 0 0', #1\n",
    "        'collision_in_tool':'0 1 0 0 0', #2\n",
    "        'collision_in_part':'0 0 1 0 0', #3\n",
    "        'bottom_collision':'0 0 0 1 0', #4\n",
    "        'bottom_obstruction':'0 0 0 0 1'} #5\n",
    "\n",
    "labels_df = labels_df.replace({'labels':classes_dict})\n",
    "labels_df[['1', '2', '3', '4', '5']] = labels_df['labels'].str.split(' ', 4, expand= True)\n",
    "labels_df = labels_df.loc[:,['1', '2', '3', '4', '5']]\n",
    "labels_df[['1', '2', '3', '4', '5']]=labels_df[['1', '2', '3', '4', '5']].astype(str).astype(int)\n",
    "\n",
    "def norm(x):\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "    range = x_max - x_min  #min max entre 0 y 1\n",
    "    return((x-x_min)/(range))\n",
    "\n",
    "norm_features = norm(features)\n",
    "#split para entrenamiento y validacion\n",
    "train_features, test_features, train_labels, test_labels  = train_test_split(norm_features, labels_df, test_size=0.2, random_state= 42)\n",
    "display(train_features)\n",
    "display(train_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_70 (Conv2D)          (None, 15, 6, 16)         160       \n",
      "                                                                 \n",
      " max_pooling2d_70 (MaxPoolin  (None, 15, 6, 16)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_70 (Dropout)        (None, 15, 6, 16)         0         \n",
      "                                                                 \n",
      " conv2d_71 (Conv2D)          (None, 15, 6, 16)         2320      \n",
      "                                                                 \n",
      " max_pooling2d_71 (MaxPoolin  (None, 5, 2, 16)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_71 (Dropout)        (None, 5, 2, 16)          0         \n",
      "                                                                 \n",
      " flatten_36 (Flatten)        (None, 160)               0         \n",
      "                                                                 \n",
      " dense_96 (Dense)            (None, 80)                12880     \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 20)                1620      \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 5)                 105       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,085\n",
      "Trainable params: 17,085\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def my_model(learning_rate):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape = (15,6,1), padding='same'), # #de feature maps, dimensiones del kernel, kernel entre más pequeño mejor y se prefiere un número impar\n",
    "        tf.keras.layers.MaxPooling2D((1,1)), # dimensiones del pooling\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape = (15,6,1), padding='same'), # #de feature maps, dimensiones del kernel, kernel entre más pequeño mejor y se prefiere un número impar\n",
    "        tf.keras.layers.MaxPooling2D((3,3)), # dimensiones del pooling\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(80, activation='relu'),\n",
    "        tf.keras.layers.Dense(20, activation='relu'),\n",
    "        tf.keras.layers.Dense(5, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "# input shape es 15 filas, 6 columnas y 1 canal de color\n",
    "model = my_model(0.01)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, features, labels, epochs, batch_size):\n",
    "    history = model.fit(\n",
    "        x = features,\n",
    "        y = labels,\n",
    "        epochs= epochs,\n",
    "        batch_size= batch_size,\n",
    "        validation_split= 0.25\n",
    "    )\n",
    "    hist= pd.DataFrame(history.history) #se guardan los valores de errores y metricas en un diccionario\n",
    "    hist['epoch'] = history.epoch #los epochs se deben añadir aparte\n",
    "    return hist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizaciones\n",
    "### Pasos para visualizar los filtros:\n",
    "##### 1. Iterar por todas las capas del modelo usando model.layers\n",
    "##### 2. Si la capa actual es de convolucion se extraen los pesos y sesgos usando get_weights()\n",
    "##### 3. Se normalizan los pesos de los filtros entre 0 y 1\n",
    "##### 4. Se plotean los filtros para cada capa convolutional y todos los canales de color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualización de los filtros\n",
    "\n",
    "'''def view_filters(model):\n",
    "    for layer in model.layers:\n",
    "        if 'conv' in layer.name:\n",
    "            filters, bias= layer.get_weights()\n",
    "            print(layer.name, filters.shape)\n",
    "        #normalize filter values between  0 and 1 for visualization\n",
    "            f_min, f_max = filters.min(), filters.max()\n",
    "            filters = (filters - f_min) / (f_max - f_min)  \n",
    "            print(filters.shape[3])\n",
    "            axis_x=1\n",
    "        #plotting all the filters\n",
    "            for i in range(filters.shape[3]):\n",
    "        #for i in range(6):\n",
    "            #get the filters\n",
    "                filt=filters[:,:,:, i]\n",
    "                plotFilters(filt)\n",
    "                \n",
    "def plotFilters(conv_filter):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(5,5))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( conv_filter, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "   #plt.savefig('filters.jpg')\n",
    "    plt.show()\n",
    "## Visualización de las curvas de pérdida'''\n",
    "\n",
    "def loss_curves(history):\n",
    "    hist = history\n",
    "    labels = {\"loss\":\"Training Loss\", \"val_loss\":\"Validation Loss\"}\n",
    "    hist.rename(columns = labels, inplace = True)\n",
    "    \n",
    "    fig = px.line(hist, x='epoch', y=['Training Loss', 'Validation Loss'],\n",
    "                title='Gráficas de Pérdida de Entrenamiento y Evaluación',\n",
    "                labels={\"epoch\": \"Epoch\", \"value\":\"Binary Cross Entropy\", \"variable\":\"Curvas de Pérdida\"},\n",
    "                color_discrete_map={ # replaces default color mapping by value\n",
    "                \"Training Loss\": \"#46039f\", \"Validation Loss\": \"#fb9f3a\"})\n",
    "    fig.update_layout(template='plotly_white')\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se corren las funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dave4\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\backend.py:5676: UserWarning:\n",
      "\n",
      "\"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 10ms/step - loss: 0.6573 - accuracy: 0.1327 - val_loss: 0.6185 - val_accuracy: 0.0606\n",
      "Epoch 2/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.5594 - accuracy: 0.1429 - val_loss: 0.5353 - val_accuracy: 0.0606\n",
      "Epoch 3/400\n",
      " 1/20 [>.............................] - ETA: 0s - loss: 0.5213 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dave4\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\backend.py:5676: UserWarning:\n",
      "\n",
      "\"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 4ms/step - loss: 0.5032 - accuracy: 0.1837 - val_loss: 0.5038 - val_accuracy: 0.2727\n",
      "Epoch 4/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4899 - accuracy: 0.2653 - val_loss: 0.4958 - val_accuracy: 0.2727\n",
      "Epoch 5/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4909 - accuracy: 0.3163 - val_loss: 0.4974 - val_accuracy: 0.2727\n",
      "Epoch 6/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4907 - accuracy: 0.3061 - val_loss: 0.5004 - val_accuracy: 0.2727\n",
      "Epoch 7/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4968 - accuracy: 0.3265 - val_loss: 0.5055 - val_accuracy: 0.2727\n",
      "Epoch 8/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.4956 - accuracy: 0.2449 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 9/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.4916 - accuracy: 0.3061 - val_loss: 0.5034 - val_accuracy: 0.2727\n",
      "Epoch 10/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.4891 - accuracy: 0.3163 - val_loss: 0.4978 - val_accuracy: 0.2727\n",
      "Epoch 11/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4927 - accuracy: 0.3163 - val_loss: 0.5008 - val_accuracy: 0.2727\n",
      "Epoch 12/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4899 - accuracy: 0.3163 - val_loss: 0.4950 - val_accuracy: 0.2727\n",
      "Epoch 13/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.4853 - accuracy: 0.3163 - val_loss: 0.4972 - val_accuracy: 0.2727\n",
      "Epoch 14/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4870 - accuracy: 0.3163 - val_loss: 0.4964 - val_accuracy: 0.2727\n",
      "Epoch 15/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4890 - accuracy: 0.3163 - val_loss: 0.4966 - val_accuracy: 0.2727\n",
      "Epoch 16/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.4882 - accuracy: 0.3367 - val_loss: 0.4947 - val_accuracy: 0.2727\n",
      "Epoch 17/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4914 - accuracy: 0.3061 - val_loss: 0.4968 - val_accuracy: 0.2727\n",
      "Epoch 18/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4917 - accuracy: 0.2347 - val_loss: 0.4965 - val_accuracy: 0.2727\n",
      "Epoch 19/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4967 - accuracy: 0.3163 - val_loss: 0.4997 - val_accuracy: 0.2727\n",
      "Epoch 20/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4916 - accuracy: 0.2857 - val_loss: 0.4960 - val_accuracy: 0.2727\n",
      "Epoch 21/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4925 - accuracy: 0.2959 - val_loss: 0.4954 - val_accuracy: 0.2727\n",
      "Epoch 22/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4893 - accuracy: 0.3163 - val_loss: 0.5023 - val_accuracy: 0.2727\n",
      "Epoch 23/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4858 - accuracy: 0.3265 - val_loss: 0.4914 - val_accuracy: 0.2727\n",
      "Epoch 24/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4888 - accuracy: 0.3163 - val_loss: 0.4989 - val_accuracy: 0.2727\n",
      "Epoch 25/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4888 - accuracy: 0.2959 - val_loss: 0.4941 - val_accuracy: 0.2727\n",
      "Epoch 26/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4863 - accuracy: 0.3163 - val_loss: 0.4956 - val_accuracy: 0.2727\n",
      "Epoch 27/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.4900 - accuracy: 0.2755 - val_loss: 0.4959 - val_accuracy: 0.2727\n",
      "Epoch 28/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.4900 - accuracy: 0.3163 - val_loss: 0.4988 - val_accuracy: 0.2727\n",
      "Epoch 29/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4893 - accuracy: 0.3163 - val_loss: 0.4956 - val_accuracy: 0.2727\n",
      "Epoch 30/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4859 - accuracy: 0.3163 - val_loss: 0.4961 - val_accuracy: 0.2727\n",
      "Epoch 31/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4868 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 32/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.4895 - accuracy: 0.3163 - val_loss: 0.4984 - val_accuracy: 0.2727\n",
      "Epoch 33/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4814 - accuracy: 0.3163 - val_loss: 0.4884 - val_accuracy: 0.2727\n",
      "Epoch 34/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4780 - accuracy: 0.3163 - val_loss: 0.4964 - val_accuracy: 0.2727\n",
      "Epoch 35/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4775 - accuracy: 0.3367 - val_loss: 0.4882 - val_accuracy: 0.2727\n",
      "Epoch 36/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4748 - accuracy: 0.3367 - val_loss: 0.4874 - val_accuracy: 0.3030\n",
      "Epoch 37/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4740 - accuracy: 0.3163 - val_loss: 0.4820 - val_accuracy: 0.3333\n",
      "Epoch 38/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4563 - accuracy: 0.4082 - val_loss: 0.4702 - val_accuracy: 0.3333\n",
      "Epoch 39/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.4562 - accuracy: 0.4082 - val_loss: 0.4780 - val_accuracy: 0.3333\n",
      "Epoch 40/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4470 - accuracy: 0.4184 - val_loss: 0.4654 - val_accuracy: 0.3333\n",
      "Epoch 41/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4356 - accuracy: 0.4082 - val_loss: 0.4533 - val_accuracy: 0.3333\n",
      "Epoch 42/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4269 - accuracy: 0.3776 - val_loss: 0.4516 - val_accuracy: 0.3333\n",
      "Epoch 43/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4162 - accuracy: 0.4490 - val_loss: 0.4433 - val_accuracy: 0.3333\n",
      "Epoch 44/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4174 - accuracy: 0.4286 - val_loss: 0.4493 - val_accuracy: 0.3333\n",
      "Epoch 45/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4074 - accuracy: 0.4184 - val_loss: 0.4400 - val_accuracy: 0.3333\n",
      "Epoch 46/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4120 - accuracy: 0.4388 - val_loss: 0.4424 - val_accuracy: 0.3333\n",
      "Epoch 47/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4014 - accuracy: 0.3980 - val_loss: 0.4396 - val_accuracy: 0.3333\n",
      "Epoch 48/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4037 - accuracy: 0.4490 - val_loss: 0.4440 - val_accuracy: 0.3333\n",
      "Epoch 49/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4025 - accuracy: 0.3469 - val_loss: 0.4399 - val_accuracy: 0.3333\n",
      "Epoch 50/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3957 - accuracy: 0.4490 - val_loss: 0.4358 - val_accuracy: 0.3333\n",
      "Epoch 51/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3981 - accuracy: 0.4490 - val_loss: 0.4354 - val_accuracy: 0.3333\n",
      "Epoch 52/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.4001 - accuracy: 0.4286 - val_loss: 0.4284 - val_accuracy: 0.3333\n",
      "Epoch 53/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3961 - accuracy: 0.4388 - val_loss: 0.4363 - val_accuracy: 0.3333\n",
      "Epoch 54/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3881 - accuracy: 0.4388 - val_loss: 0.4294 - val_accuracy: 0.3333\n",
      "Epoch 55/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3899 - accuracy: 0.4286 - val_loss: 0.4306 - val_accuracy: 0.3333\n",
      "Epoch 56/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3911 - accuracy: 0.4592 - val_loss: 0.4309 - val_accuracy: 0.3333\n",
      "Epoch 57/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3922 - accuracy: 0.4490 - val_loss: 0.4183 - val_accuracy: 0.3333\n",
      "Epoch 58/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3829 - accuracy: 0.5000 - val_loss: 0.4217 - val_accuracy: 0.3333\n",
      "Epoch 59/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3882 - accuracy: 0.4490 - val_loss: 0.4223 - val_accuracy: 0.3333\n",
      "Epoch 60/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3838 - accuracy: 0.3776 - val_loss: 0.4226 - val_accuracy: 0.3333\n",
      "Epoch 61/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3899 - accuracy: 0.4592 - val_loss: 0.4252 - val_accuracy: 0.3333\n",
      "Epoch 62/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3894 - accuracy: 0.4592 - val_loss: 0.4306 - val_accuracy: 0.3333\n",
      "Epoch 63/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3877 - accuracy: 0.4286 - val_loss: 0.4150 - val_accuracy: 0.3939\n",
      "Epoch 64/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3789 - accuracy: 0.4592 - val_loss: 0.4195 - val_accuracy: 0.3333\n",
      "Epoch 65/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3772 - accuracy: 0.4490 - val_loss: 0.4093 - val_accuracy: 0.3939\n",
      "Epoch 66/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3789 - accuracy: 0.4898 - val_loss: 0.4130 - val_accuracy: 0.3636\n",
      "Epoch 67/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3779 - accuracy: 0.5000 - val_loss: 0.4127 - val_accuracy: 0.3939\n",
      "Epoch 68/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3747 - accuracy: 0.5000 - val_loss: 0.4039 - val_accuracy: 0.3939\n",
      "Epoch 69/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3701 - accuracy: 0.5204 - val_loss: 0.4140 - val_accuracy: 0.3636\n",
      "Epoch 70/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3712 - accuracy: 0.4796 - val_loss: 0.4000 - val_accuracy: 0.3939\n",
      "Epoch 71/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3705 - accuracy: 0.4694 - val_loss: 0.4020 - val_accuracy: 0.4242\n",
      "Epoch 72/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3675 - accuracy: 0.5000 - val_loss: 0.3869 - val_accuracy: 0.4848\n",
      "Epoch 73/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3671 - accuracy: 0.5102 - val_loss: 0.3873 - val_accuracy: 0.4545\n",
      "Epoch 74/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3677 - accuracy: 0.4796 - val_loss: 0.3951 - val_accuracy: 0.4545\n",
      "Epoch 75/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3578 - accuracy: 0.5204 - val_loss: 0.3922 - val_accuracy: 0.4242\n",
      "Epoch 76/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3562 - accuracy: 0.5306 - val_loss: 0.3969 - val_accuracy: 0.3939\n",
      "Epoch 77/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3518 - accuracy: 0.5306 - val_loss: 0.3822 - val_accuracy: 0.4545\n",
      "Epoch 78/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3622 - accuracy: 0.5102 - val_loss: 0.3963 - val_accuracy: 0.3939\n",
      "Epoch 79/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3604 - accuracy: 0.5306 - val_loss: 0.3861 - val_accuracy: 0.4545\n",
      "Epoch 80/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3622 - accuracy: 0.4796 - val_loss: 0.3803 - val_accuracy: 0.4848\n",
      "Epoch 81/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3506 - accuracy: 0.5102 - val_loss: 0.3708 - val_accuracy: 0.4545\n",
      "Epoch 82/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3457 - accuracy: 0.5102 - val_loss: 0.3726 - val_accuracy: 0.4848\n",
      "Epoch 83/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3519 - accuracy: 0.5102 - val_loss: 0.3742 - val_accuracy: 0.4545\n",
      "Epoch 84/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3483 - accuracy: 0.5204 - val_loss: 0.3718 - val_accuracy: 0.5152\n",
      "Epoch 85/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3563 - accuracy: 0.4898 - val_loss: 0.3690 - val_accuracy: 0.5152\n",
      "Epoch 86/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3490 - accuracy: 0.5408 - val_loss: 0.3780 - val_accuracy: 0.4545\n",
      "Epoch 87/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3471 - accuracy: 0.5204 - val_loss: 0.3808 - val_accuracy: 0.4545\n",
      "Epoch 88/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3489 - accuracy: 0.5306 - val_loss: 0.3680 - val_accuracy: 0.4848\n",
      "Epoch 89/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3528 - accuracy: 0.5000 - val_loss: 0.3651 - val_accuracy: 0.5152\n",
      "Epoch 90/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3511 - accuracy: 0.5000 - val_loss: 0.3673 - val_accuracy: 0.5152\n",
      "Epoch 91/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3520 - accuracy: 0.5102 - val_loss: 0.3615 - val_accuracy: 0.5152\n",
      "Epoch 92/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3468 - accuracy: 0.5204 - val_loss: 0.3840 - val_accuracy: 0.4545\n",
      "Epoch 93/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3317 - accuracy: 0.5510 - val_loss: 0.3601 - val_accuracy: 0.5152\n",
      "Epoch 94/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3450 - accuracy: 0.5510 - val_loss: 0.3729 - val_accuracy: 0.4545\n",
      "Epoch 95/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3579 - accuracy: 0.4796 - val_loss: 0.3724 - val_accuracy: 0.5152\n",
      "Epoch 96/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3435 - accuracy: 0.5204 - val_loss: 0.3676 - val_accuracy: 0.4545\n",
      "Epoch 97/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3461 - accuracy: 0.5204 - val_loss: 0.3702 - val_accuracy: 0.4545\n",
      "Epoch 98/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3490 - accuracy: 0.5102 - val_loss: 0.3584 - val_accuracy: 0.5152\n",
      "Epoch 99/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3407 - accuracy: 0.5204 - val_loss: 0.3613 - val_accuracy: 0.4848\n",
      "Epoch 100/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3321 - accuracy: 0.5408 - val_loss: 0.3560 - val_accuracy: 0.5152\n",
      "Epoch 101/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3429 - accuracy: 0.4898 - val_loss: 0.3578 - val_accuracy: 0.4848\n",
      "Epoch 102/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3375 - accuracy: 0.5102 - val_loss: 0.3744 - val_accuracy: 0.4848\n",
      "Epoch 103/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3370 - accuracy: 0.5102 - val_loss: 0.3641 - val_accuracy: 0.4848\n",
      "Epoch 104/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3418 - accuracy: 0.5612 - val_loss: 0.3597 - val_accuracy: 0.5152\n",
      "Epoch 105/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3362 - accuracy: 0.5408 - val_loss: 0.3508 - val_accuracy: 0.5152\n",
      "Epoch 106/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3423 - accuracy: 0.5306 - val_loss: 0.3724 - val_accuracy: 0.4848\n",
      "Epoch 107/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3370 - accuracy: 0.5510 - val_loss: 0.3556 - val_accuracy: 0.5152\n",
      "Epoch 108/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3357 - accuracy: 0.5612 - val_loss: 0.3478 - val_accuracy: 0.5152\n",
      "Epoch 109/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3393 - accuracy: 0.5102 - val_loss: 0.3754 - val_accuracy: 0.4545\n",
      "Epoch 110/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3402 - accuracy: 0.5306 - val_loss: 0.3705 - val_accuracy: 0.4545\n",
      "Epoch 111/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3396 - accuracy: 0.5204 - val_loss: 0.3624 - val_accuracy: 0.4848\n",
      "Epoch 112/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3454 - accuracy: 0.5816 - val_loss: 0.3538 - val_accuracy: 0.5152\n",
      "Epoch 113/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3343 - accuracy: 0.5408 - val_loss: 0.3639 - val_accuracy: 0.4545\n",
      "Epoch 114/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3413 - accuracy: 0.5102 - val_loss: 0.3596 - val_accuracy: 0.5152\n",
      "Epoch 115/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3395 - accuracy: 0.5102 - val_loss: 0.3650 - val_accuracy: 0.4848\n",
      "Epoch 116/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3449 - accuracy: 0.5102 - val_loss: 0.3603 - val_accuracy: 0.5152\n",
      "Epoch 117/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3303 - accuracy: 0.5510 - val_loss: 0.3622 - val_accuracy: 0.5152\n",
      "Epoch 118/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3478 - accuracy: 0.4490 - val_loss: 0.3597 - val_accuracy: 0.4545\n",
      "Epoch 119/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3424 - accuracy: 0.5102 - val_loss: 0.3501 - val_accuracy: 0.5152\n",
      "Epoch 120/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3331 - accuracy: 0.4694 - val_loss: 0.3532 - val_accuracy: 0.5455\n",
      "Epoch 121/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3357 - accuracy: 0.5714 - val_loss: 0.3623 - val_accuracy: 0.4848\n",
      "Epoch 122/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3319 - accuracy: 0.5306 - val_loss: 0.3639 - val_accuracy: 0.5152\n",
      "Epoch 123/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3326 - accuracy: 0.5408 - val_loss: 0.3514 - val_accuracy: 0.5152\n",
      "Epoch 124/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3238 - accuracy: 0.5612 - val_loss: 0.3498 - val_accuracy: 0.5152\n",
      "Epoch 125/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3374 - accuracy: 0.5000 - val_loss: 0.3495 - val_accuracy: 0.5152\n",
      "Epoch 126/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3301 - accuracy: 0.5102 - val_loss: 0.3576 - val_accuracy: 0.5455\n",
      "Epoch 127/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3397 - accuracy: 0.4592 - val_loss: 0.3473 - val_accuracy: 0.5152\n",
      "Epoch 128/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3299 - accuracy: 0.5204 - val_loss: 0.3519 - val_accuracy: 0.5152\n",
      "Epoch 129/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3315 - accuracy: 0.5918 - val_loss: 0.3500 - val_accuracy: 0.5152\n",
      "Epoch 130/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3338 - accuracy: 0.5306 - val_loss: 0.3542 - val_accuracy: 0.4545\n",
      "Epoch 131/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3427 - accuracy: 0.4898 - val_loss: 0.3462 - val_accuracy: 0.5152\n",
      "Epoch 132/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3436 - accuracy: 0.5204 - val_loss: 0.3571 - val_accuracy: 0.5152\n",
      "Epoch 133/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3407 - accuracy: 0.4898 - val_loss: 0.3544 - val_accuracy: 0.4848\n",
      "Epoch 134/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3314 - accuracy: 0.5204 - val_loss: 0.3475 - val_accuracy: 0.5152\n",
      "Epoch 135/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3381 - accuracy: 0.5306 - val_loss: 0.3428 - val_accuracy: 0.5152\n",
      "Epoch 136/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3360 - accuracy: 0.5510 - val_loss: 0.3471 - val_accuracy: 0.5152\n",
      "Epoch 137/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3294 - accuracy: 0.5510 - val_loss: 0.3555 - val_accuracy: 0.4848\n",
      "Epoch 138/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3304 - accuracy: 0.5306 - val_loss: 0.3450 - val_accuracy: 0.5152\n",
      "Epoch 139/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3318 - accuracy: 0.5408 - val_loss: 0.3372 - val_accuracy: 0.5455\n",
      "Epoch 140/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3348 - accuracy: 0.5102 - val_loss: 0.3480 - val_accuracy: 0.4848\n",
      "Epoch 141/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3273 - accuracy: 0.5204 - val_loss: 0.3487 - val_accuracy: 0.5152\n",
      "Epoch 142/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3261 - accuracy: 0.5306 - val_loss: 0.3440 - val_accuracy: 0.5152\n",
      "Epoch 143/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3318 - accuracy: 0.5306 - val_loss: 0.3461 - val_accuracy: 0.5152\n",
      "Epoch 144/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3296 - accuracy: 0.5306 - val_loss: 0.3474 - val_accuracy: 0.5152\n",
      "Epoch 145/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3350 - accuracy: 0.5102 - val_loss: 0.3529 - val_accuracy: 0.4848\n",
      "Epoch 146/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3357 - accuracy: 0.5408 - val_loss: 0.3524 - val_accuracy: 0.5152\n",
      "Epoch 147/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3301 - accuracy: 0.5408 - val_loss: 0.3409 - val_accuracy: 0.5152\n",
      "Epoch 148/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3318 - accuracy: 0.4592 - val_loss: 0.3513 - val_accuracy: 0.5152\n",
      "Epoch 149/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3501 - accuracy: 0.5306 - val_loss: 0.3383 - val_accuracy: 0.5152\n",
      "Epoch 150/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3275 - accuracy: 0.5102 - val_loss: 0.3555 - val_accuracy: 0.5152\n",
      "Epoch 151/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3428 - accuracy: 0.5306 - val_loss: 0.3574 - val_accuracy: 0.5152\n",
      "Epoch 152/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3284 - accuracy: 0.5510 - val_loss: 0.3612 - val_accuracy: 0.5152\n",
      "Epoch 153/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3271 - accuracy: 0.5102 - val_loss: 0.3452 - val_accuracy: 0.5152\n",
      "Epoch 154/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3173 - accuracy: 0.5408 - val_loss: 0.3357 - val_accuracy: 0.5152\n",
      "Epoch 155/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3279 - accuracy: 0.5204 - val_loss: 0.3450 - val_accuracy: 0.5152\n",
      "Epoch 156/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3198 - accuracy: 0.5408 - val_loss: 0.3402 - val_accuracy: 0.5152\n",
      "Epoch 157/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3151 - accuracy: 0.5612 - val_loss: 0.3369 - val_accuracy: 0.5152\n",
      "Epoch 158/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3223 - accuracy: 0.5408 - val_loss: 0.3573 - val_accuracy: 0.5152\n",
      "Epoch 159/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3247 - accuracy: 0.5510 - val_loss: 0.3437 - val_accuracy: 0.5152\n",
      "Epoch 160/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3316 - accuracy: 0.5510 - val_loss: 0.3459 - val_accuracy: 0.5152\n",
      "Epoch 161/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3197 - accuracy: 0.5408 - val_loss: 0.3407 - val_accuracy: 0.5152\n",
      "Epoch 162/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3199 - accuracy: 0.5408 - val_loss: 0.3366 - val_accuracy: 0.5152\n",
      "Epoch 163/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3219 - accuracy: 0.5510 - val_loss: 0.3462 - val_accuracy: 0.5152\n",
      "Epoch 164/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3175 - accuracy: 0.5408 - val_loss: 0.3401 - val_accuracy: 0.5758\n",
      "Epoch 165/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3245 - accuracy: 0.5408 - val_loss: 0.3521 - val_accuracy: 0.5152\n",
      "Epoch 166/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3170 - accuracy: 0.5204 - val_loss: 0.3563 - val_accuracy: 0.5758\n",
      "Epoch 167/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3224 - accuracy: 0.5612 - val_loss: 0.3461 - val_accuracy: 0.5152\n",
      "Epoch 168/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3349 - accuracy: 0.4898 - val_loss: 0.3651 - val_accuracy: 0.5152\n",
      "Epoch 169/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3288 - accuracy: 0.5102 - val_loss: 0.3403 - val_accuracy: 0.5152\n",
      "Epoch 170/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3303 - accuracy: 0.5306 - val_loss: 0.3462 - val_accuracy: 0.5152\n",
      "Epoch 171/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3376 - accuracy: 0.5306 - val_loss: 0.3457 - val_accuracy: 0.5758\n",
      "Epoch 172/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3320 - accuracy: 0.5102 - val_loss: 0.3524 - val_accuracy: 0.5152\n",
      "Epoch 173/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3341 - accuracy: 0.5408 - val_loss: 0.3479 - val_accuracy: 0.5758\n",
      "Epoch 174/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3261 - accuracy: 0.5612 - val_loss: 0.3565 - val_accuracy: 0.5758\n",
      "Epoch 175/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3259 - accuracy: 0.5510 - val_loss: 0.3455 - val_accuracy: 0.5152\n",
      "Epoch 176/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3289 - accuracy: 0.4898 - val_loss: 0.3429 - val_accuracy: 0.5152\n",
      "Epoch 177/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3187 - accuracy: 0.5510 - val_loss: 0.3514 - val_accuracy: 0.5758\n",
      "Epoch 178/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3082 - accuracy: 0.5408 - val_loss: 0.3452 - val_accuracy: 0.5152\n",
      "Epoch 179/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3166 - accuracy: 0.5510 - val_loss: 0.3321 - val_accuracy: 0.5152\n",
      "Epoch 180/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3132 - accuracy: 0.5204 - val_loss: 0.3321 - val_accuracy: 0.5758\n",
      "Epoch 181/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3262 - accuracy: 0.5000 - val_loss: 0.3389 - val_accuracy: 0.5152\n",
      "Epoch 182/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3217 - accuracy: 0.5102 - val_loss: 0.3429 - val_accuracy: 0.5152\n",
      "Epoch 183/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3268 - accuracy: 0.5510 - val_loss: 0.3323 - val_accuracy: 0.5758\n",
      "Epoch 184/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3281 - accuracy: 0.5306 - val_loss: 0.3341 - val_accuracy: 0.5152\n",
      "Epoch 185/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3175 - accuracy: 0.5204 - val_loss: 0.3257 - val_accuracy: 0.5152\n",
      "Epoch 186/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3147 - accuracy: 0.5612 - val_loss: 0.3298 - val_accuracy: 0.5152\n",
      "Epoch 187/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3239 - accuracy: 0.4694 - val_loss: 0.3217 - val_accuracy: 0.5758\n",
      "Epoch 188/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3185 - accuracy: 0.5408 - val_loss: 0.3294 - val_accuracy: 0.5152\n",
      "Epoch 189/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3047 - accuracy: 0.5612 - val_loss: 0.3324 - val_accuracy: 0.5152\n",
      "Epoch 190/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3318 - accuracy: 0.4898 - val_loss: 0.3407 - val_accuracy: 0.5455\n",
      "Epoch 191/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3169 - accuracy: 0.5612 - val_loss: 0.3324 - val_accuracy: 0.5152\n",
      "Epoch 192/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3159 - accuracy: 0.5612 - val_loss: 0.3280 - val_accuracy: 0.5152\n",
      "Epoch 193/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3107 - accuracy: 0.5408 - val_loss: 0.3219 - val_accuracy: 0.5152\n",
      "Epoch 194/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3208 - accuracy: 0.6122 - val_loss: 0.3331 - val_accuracy: 0.5758\n",
      "Epoch 195/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3269 - accuracy: 0.4898 - val_loss: 0.3357 - val_accuracy: 0.5758\n",
      "Epoch 196/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3229 - accuracy: 0.5612 - val_loss: 0.3451 - val_accuracy: 0.5152\n",
      "Epoch 197/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3154 - accuracy: 0.5204 - val_loss: 0.3444 - val_accuracy: 0.5152\n",
      "Epoch 198/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3219 - accuracy: 0.5408 - val_loss: 0.3256 - val_accuracy: 0.6667\n",
      "Epoch 199/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3026 - accuracy: 0.6429 - val_loss: 0.3334 - val_accuracy: 0.5152\n",
      "Epoch 200/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3114 - accuracy: 0.5408 - val_loss: 0.3285 - val_accuracy: 0.5152\n",
      "Epoch 201/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3113 - accuracy: 0.5306 - val_loss: 0.3178 - val_accuracy: 0.5152\n",
      "Epoch 202/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3142 - accuracy: 0.5510 - val_loss: 0.3198 - val_accuracy: 0.7576\n",
      "Epoch 203/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3113 - accuracy: 0.5306 - val_loss: 0.3223 - val_accuracy: 0.5152\n",
      "Epoch 204/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3053 - accuracy: 0.5408 - val_loss: 0.3226 - val_accuracy: 0.5152\n",
      "Epoch 205/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3047 - accuracy: 0.5612 - val_loss: 0.3178 - val_accuracy: 0.5152\n",
      "Epoch 206/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3049 - accuracy: 0.5714 - val_loss: 0.3140 - val_accuracy: 0.5152\n",
      "Epoch 207/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3044 - accuracy: 0.5102 - val_loss: 0.3200 - val_accuracy: 0.5152\n",
      "Epoch 208/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3163 - accuracy: 0.5306 - val_loss: 0.3139 - val_accuracy: 0.7273\n",
      "Epoch 209/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3143 - accuracy: 0.5510 - val_loss: 0.3246 - val_accuracy: 0.5152\n",
      "Epoch 210/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3096 - accuracy: 0.5714 - val_loss: 0.3233 - val_accuracy: 0.5455\n",
      "Epoch 211/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3048 - accuracy: 0.6020 - val_loss: 0.3226 - val_accuracy: 0.6364\n",
      "Epoch 212/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3020 - accuracy: 0.5714 - val_loss: 0.3234 - val_accuracy: 0.5152\n",
      "Epoch 213/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3094 - accuracy: 0.6122 - val_loss: 0.3287 - val_accuracy: 0.5152\n",
      "Epoch 214/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3178 - accuracy: 0.5714 - val_loss: 0.3273 - val_accuracy: 0.6970\n",
      "Epoch 215/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3063 - accuracy: 0.6020 - val_loss: 0.3186 - val_accuracy: 0.5758\n",
      "Epoch 216/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3055 - accuracy: 0.5918 - val_loss: 0.3246 - val_accuracy: 0.5455\n",
      "Epoch 217/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3097 - accuracy: 0.5612 - val_loss: 0.3189 - val_accuracy: 0.5152\n",
      "Epoch 218/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3089 - accuracy: 0.5510 - val_loss: 0.3363 - val_accuracy: 0.5152\n",
      "Epoch 219/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3007 - accuracy: 0.6327 - val_loss: 0.3466 - val_accuracy: 0.5152\n",
      "Epoch 220/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3087 - accuracy: 0.5204 - val_loss: 0.3311 - val_accuracy: 0.6667\n",
      "Epoch 221/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3068 - accuracy: 0.5510 - val_loss: 0.3241 - val_accuracy: 0.5455\n",
      "Epoch 222/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3095 - accuracy: 0.5408 - val_loss: 0.3380 - val_accuracy: 0.5152\n",
      "Epoch 223/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3165 - accuracy: 0.5918 - val_loss: 0.3137 - val_accuracy: 0.6970\n",
      "Epoch 224/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3133 - accuracy: 0.5306 - val_loss: 0.3365 - val_accuracy: 0.5152\n",
      "Epoch 225/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3000 - accuracy: 0.6020 - val_loss: 0.3222 - val_accuracy: 0.5152\n",
      "Epoch 226/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3066 - accuracy: 0.5714 - val_loss: 0.3309 - val_accuracy: 0.7576\n",
      "Epoch 227/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3080 - accuracy: 0.5816 - val_loss: 0.3267 - val_accuracy: 0.5152\n",
      "Epoch 228/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3129 - accuracy: 0.5204 - val_loss: 0.3352 - val_accuracy: 0.5152\n",
      "Epoch 229/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3061 - accuracy: 0.5816 - val_loss: 0.3222 - val_accuracy: 0.5152\n",
      "Epoch 230/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2961 - accuracy: 0.5816 - val_loss: 0.3254 - val_accuracy: 0.7273\n",
      "Epoch 231/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3133 - accuracy: 0.6122 - val_loss: 0.3183 - val_accuracy: 0.5152\n",
      "Epoch 232/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2993 - accuracy: 0.6224 - val_loss: 0.3230 - val_accuracy: 0.5152\n",
      "Epoch 233/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2983 - accuracy: 0.5408 - val_loss: 0.3227 - val_accuracy: 0.5152\n",
      "Epoch 234/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3120 - accuracy: 0.5102 - val_loss: 0.3208 - val_accuracy: 0.7576\n",
      "Epoch 235/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2995 - accuracy: 0.5816 - val_loss: 0.3336 - val_accuracy: 0.5152\n",
      "Epoch 236/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2979 - accuracy: 0.6122 - val_loss: 0.3186 - val_accuracy: 0.7576\n",
      "Epoch 237/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3050 - accuracy: 0.5816 - val_loss: 0.3282 - val_accuracy: 0.5152\n",
      "Epoch 238/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2939 - accuracy: 0.6020 - val_loss: 0.3125 - val_accuracy: 0.6667\n",
      "Epoch 239/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3001 - accuracy: 0.6429 - val_loss: 0.3246 - val_accuracy: 0.6061\n",
      "Epoch 240/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3171 - accuracy: 0.6020 - val_loss: 0.3088 - val_accuracy: 0.6667\n",
      "Epoch 241/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3203 - accuracy: 0.5612 - val_loss: 0.3448 - val_accuracy: 0.5152\n",
      "Epoch 242/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3054 - accuracy: 0.5510 - val_loss: 0.3302 - val_accuracy: 0.7576\n",
      "Epoch 243/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3062 - accuracy: 0.5408 - val_loss: 0.3188 - val_accuracy: 0.7879\n",
      "Epoch 244/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2988 - accuracy: 0.5714 - val_loss: 0.3261 - val_accuracy: 0.5152\n",
      "Epoch 245/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2998 - accuracy: 0.5816 - val_loss: 0.3141 - val_accuracy: 0.7576\n",
      "Epoch 246/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2952 - accuracy: 0.5510 - val_loss: 0.3217 - val_accuracy: 0.5152\n",
      "Epoch 247/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2993 - accuracy: 0.5816 - val_loss: 0.3184 - val_accuracy: 0.7576\n",
      "Epoch 248/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2880 - accuracy: 0.6429 - val_loss: 0.3211 - val_accuracy: 0.7576\n",
      "Epoch 249/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3060 - accuracy: 0.5510 - val_loss: 0.3263 - val_accuracy: 0.5152\n",
      "Epoch 250/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2910 - accuracy: 0.5714 - val_loss: 0.3384 - val_accuracy: 0.7273\n",
      "Epoch 251/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2922 - accuracy: 0.6122 - val_loss: 0.3322 - val_accuracy: 0.6667\n",
      "Epoch 252/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3221 - accuracy: 0.5918 - val_loss: 0.3247 - val_accuracy: 0.7273\n",
      "Epoch 253/400\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.3028 - accuracy: 0.6327 - val_loss: 0.3406 - val_accuracy: 0.6667\n",
      "Epoch 254/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3113 - accuracy: 0.5918 - val_loss: 0.3325 - val_accuracy: 0.7879\n",
      "Epoch 255/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3107 - accuracy: 0.5510 - val_loss: 0.3529 - val_accuracy: 0.4848\n",
      "Epoch 256/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3009 - accuracy: 0.6020 - val_loss: 0.3471 - val_accuracy: 0.6364\n",
      "Epoch 257/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3050 - accuracy: 0.5714 - val_loss: 0.3379 - val_accuracy: 0.7576\n",
      "Epoch 258/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3049 - accuracy: 0.5714 - val_loss: 0.3421 - val_accuracy: 0.7879\n",
      "Epoch 259/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2908 - accuracy: 0.5816 - val_loss: 0.3378 - val_accuracy: 0.4848\n",
      "Epoch 260/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2868 - accuracy: 0.5714 - val_loss: 0.3303 - val_accuracy: 0.6970\n",
      "Epoch 261/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3044 - accuracy: 0.5816 - val_loss: 0.3290 - val_accuracy: 0.6667\n",
      "Epoch 262/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2912 - accuracy: 0.6531 - val_loss: 0.3373 - val_accuracy: 0.7273\n",
      "Epoch 263/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2995 - accuracy: 0.6327 - val_loss: 0.3306 - val_accuracy: 0.7273\n",
      "Epoch 264/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2959 - accuracy: 0.6122 - val_loss: 0.3363 - val_accuracy: 0.6970\n",
      "Epoch 265/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2939 - accuracy: 0.5918 - val_loss: 0.3218 - val_accuracy: 0.6667\n",
      "Epoch 266/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3066 - accuracy: 0.5612 - val_loss: 0.3300 - val_accuracy: 0.7273\n",
      "Epoch 267/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2997 - accuracy: 0.6327 - val_loss: 0.3223 - val_accuracy: 0.6667\n",
      "Epoch 268/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2977 - accuracy: 0.6020 - val_loss: 0.3247 - val_accuracy: 0.6667\n",
      "Epoch 269/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2931 - accuracy: 0.6224 - val_loss: 0.3164 - val_accuracy: 0.7576\n",
      "Epoch 270/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3115 - accuracy: 0.5816 - val_loss: 0.3295 - val_accuracy: 0.6970\n",
      "Epoch 271/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3048 - accuracy: 0.5816 - val_loss: 0.3346 - val_accuracy: 0.7273\n",
      "Epoch 272/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3081 - accuracy: 0.5612 - val_loss: 0.3337 - val_accuracy: 0.6061\n",
      "Epoch 273/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2931 - accuracy: 0.6633 - val_loss: 0.3336 - val_accuracy: 0.6970\n",
      "Epoch 274/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2930 - accuracy: 0.6224 - val_loss: 0.3246 - val_accuracy: 0.7273\n",
      "Epoch 275/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2980 - accuracy: 0.5816 - val_loss: 0.3536 - val_accuracy: 0.7576\n",
      "Epoch 276/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2965 - accuracy: 0.6224 - val_loss: 0.3921 - val_accuracy: 0.6970\n",
      "Epoch 277/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2978 - accuracy: 0.6122 - val_loss: 0.3452 - val_accuracy: 0.6970\n",
      "Epoch 278/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3022 - accuracy: 0.5408 - val_loss: 0.3349 - val_accuracy: 0.7576\n",
      "Epoch 279/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2944 - accuracy: 0.5816 - val_loss: 0.3209 - val_accuracy: 0.5758\n",
      "Epoch 280/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3104 - accuracy: 0.5714 - val_loss: 0.3259 - val_accuracy: 0.7576\n",
      "Epoch 281/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3000 - accuracy: 0.6020 - val_loss: 0.3296 - val_accuracy: 0.7576\n",
      "Epoch 282/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3023 - accuracy: 0.5918 - val_loss: 0.3320 - val_accuracy: 0.6970\n",
      "Epoch 283/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2933 - accuracy: 0.6224 - val_loss: 0.3286 - val_accuracy: 0.7576\n",
      "Epoch 284/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2929 - accuracy: 0.6429 - val_loss: 0.3348 - val_accuracy: 0.7576\n",
      "Epoch 285/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2997 - accuracy: 0.5510 - val_loss: 0.3207 - val_accuracy: 0.7576\n",
      "Epoch 286/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3044 - accuracy: 0.5816 - val_loss: 0.3332 - val_accuracy: 0.6970\n",
      "Epoch 287/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2945 - accuracy: 0.6735 - val_loss: 0.3282 - val_accuracy: 0.7273\n",
      "Epoch 288/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2989 - accuracy: 0.5714 - val_loss: 0.3274 - val_accuracy: 0.7576\n",
      "Epoch 289/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2920 - accuracy: 0.6531 - val_loss: 0.3260 - val_accuracy: 0.5455\n",
      "Epoch 290/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2999 - accuracy: 0.5510 - val_loss: 0.3178 - val_accuracy: 0.6364\n",
      "Epoch 291/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2957 - accuracy: 0.5918 - val_loss: 0.3374 - val_accuracy: 0.5152\n",
      "Epoch 292/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3065 - accuracy: 0.5714 - val_loss: 0.3092 - val_accuracy: 0.6667\n",
      "Epoch 293/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2931 - accuracy: 0.5918 - val_loss: 0.3161 - val_accuracy: 0.6667\n",
      "Epoch 294/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2868 - accuracy: 0.6224 - val_loss: 0.3127 - val_accuracy: 0.5152\n",
      "Epoch 295/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3025 - accuracy: 0.5918 - val_loss: 0.3169 - val_accuracy: 0.7576\n",
      "Epoch 296/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2913 - accuracy: 0.6224 - val_loss: 0.3369 - val_accuracy: 0.6667\n",
      "Epoch 297/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2889 - accuracy: 0.6735 - val_loss: 0.3041 - val_accuracy: 0.6970\n",
      "Epoch 298/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2841 - accuracy: 0.6429 - val_loss: 0.3058 - val_accuracy: 0.6667\n",
      "Epoch 299/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3030 - accuracy: 0.5510 - val_loss: 0.3272 - val_accuracy: 0.7576\n",
      "Epoch 300/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2879 - accuracy: 0.6224 - val_loss: 0.3385 - val_accuracy: 0.6364\n",
      "Epoch 301/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2942 - accuracy: 0.6531 - val_loss: 0.3146 - val_accuracy: 0.7576\n",
      "Epoch 302/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2936 - accuracy: 0.6122 - val_loss: 0.3088 - val_accuracy: 0.7576\n",
      "Epoch 303/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2797 - accuracy: 0.7143 - val_loss: 0.3055 - val_accuracy: 0.6970\n",
      "Epoch 304/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2894 - accuracy: 0.5918 - val_loss: 0.3136 - val_accuracy: 0.7576\n",
      "Epoch 305/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2882 - accuracy: 0.6122 - val_loss: 0.3084 - val_accuracy: 0.7273\n",
      "Epoch 306/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2803 - accuracy: 0.6633 - val_loss: 0.3066 - val_accuracy: 0.7273\n",
      "Epoch 307/400\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.2940 - accuracy: 0.5612 - val_loss: 0.3042 - val_accuracy: 0.7576\n",
      "Epoch 308/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2854 - accuracy: 0.6633 - val_loss: 0.3134 - val_accuracy: 0.7576\n",
      "Epoch 309/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2902 - accuracy: 0.6122 - val_loss: 0.3125 - val_accuracy: 0.6667\n",
      "Epoch 310/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2928 - accuracy: 0.6224 - val_loss: 0.3742 - val_accuracy: 0.7273\n",
      "Epoch 311/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3051 - accuracy: 0.6020 - val_loss: 0.3863 - val_accuracy: 0.6970\n",
      "Epoch 312/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3147 - accuracy: 0.5714 - val_loss: 0.3363 - val_accuracy: 0.6970\n",
      "Epoch 313/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2803 - accuracy: 0.6531 - val_loss: 0.3255 - val_accuracy: 0.6061\n",
      "Epoch 314/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2925 - accuracy: 0.6020 - val_loss: 0.3199 - val_accuracy: 0.7576\n",
      "Epoch 315/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2871 - accuracy: 0.5714 - val_loss: 0.3119 - val_accuracy: 0.7576\n",
      "Epoch 316/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2900 - accuracy: 0.6224 - val_loss: 0.3113 - val_accuracy: 0.6970\n",
      "Epoch 317/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2991 - accuracy: 0.5204 - val_loss: 0.3243 - val_accuracy: 0.6970\n",
      "Epoch 318/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3133 - accuracy: 0.5816 - val_loss: 0.3107 - val_accuracy: 0.5758\n",
      "Epoch 319/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3181 - accuracy: 0.6327 - val_loss: 0.3358 - val_accuracy: 0.7273\n",
      "Epoch 320/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2963 - accuracy: 0.6327 - val_loss: 0.3291 - val_accuracy: 0.6667\n",
      "Epoch 321/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2845 - accuracy: 0.6327 - val_loss: 0.3248 - val_accuracy: 0.6667\n",
      "Epoch 322/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2920 - accuracy: 0.5816 - val_loss: 0.3235 - val_accuracy: 0.7576\n",
      "Epoch 323/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2906 - accuracy: 0.6122 - val_loss: 0.3219 - val_accuracy: 0.6970\n",
      "Epoch 324/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2816 - accuracy: 0.6224 - val_loss: 0.3166 - val_accuracy: 0.7576\n",
      "Epoch 325/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2850 - accuracy: 0.5918 - val_loss: 0.3170 - val_accuracy: 0.6970\n",
      "Epoch 326/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2767 - accuracy: 0.6837 - val_loss: 0.3392 - val_accuracy: 0.5758\n",
      "Epoch 327/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2946 - accuracy: 0.6429 - val_loss: 0.3252 - val_accuracy: 0.7273\n",
      "Epoch 328/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2984 - accuracy: 0.6224 - val_loss: 0.3305 - val_accuracy: 0.6970\n",
      "Epoch 329/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2925 - accuracy: 0.5816 - val_loss: 0.3119 - val_accuracy: 0.6364\n",
      "Epoch 330/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2769 - accuracy: 0.6429 - val_loss: 0.3155 - val_accuracy: 0.6970\n",
      "Epoch 331/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2800 - accuracy: 0.6122 - val_loss: 0.3135 - val_accuracy: 0.7273\n",
      "Epoch 332/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2829 - accuracy: 0.5816 - val_loss: 0.3194 - val_accuracy: 0.6970\n",
      "Epoch 333/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2873 - accuracy: 0.6429 - val_loss: 0.3183 - val_accuracy: 0.7576\n",
      "Epoch 334/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2836 - accuracy: 0.6531 - val_loss: 0.3209 - val_accuracy: 0.6667\n",
      "Epoch 335/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2824 - accuracy: 0.6122 - val_loss: 0.3149 - val_accuracy: 0.7273\n",
      "Epoch 336/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2831 - accuracy: 0.6224 - val_loss: 0.3142 - val_accuracy: 0.7273\n",
      "Epoch 337/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2876 - accuracy: 0.6020 - val_loss: 0.3109 - val_accuracy: 0.6667\n",
      "Epoch 338/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2841 - accuracy: 0.6837 - val_loss: 0.3074 - val_accuracy: 0.7576\n",
      "Epoch 339/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2631 - accuracy: 0.6633 - val_loss: 0.3023 - val_accuracy: 0.6970\n",
      "Epoch 340/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2885 - accuracy: 0.6531 - val_loss: 0.3063 - val_accuracy: 0.7273\n",
      "Epoch 341/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2824 - accuracy: 0.6531 - val_loss: 0.3017 - val_accuracy: 0.7576\n",
      "Epoch 342/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2794 - accuracy: 0.6020 - val_loss: 0.3013 - val_accuracy: 0.6970\n",
      "Epoch 343/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2992 - accuracy: 0.6122 - val_loss: 0.3114 - val_accuracy: 0.5758\n",
      "Epoch 344/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2851 - accuracy: 0.5918 - val_loss: 0.3231 - val_accuracy: 0.6667\n",
      "Epoch 345/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2794 - accuracy: 0.6122 - val_loss: 0.3256 - val_accuracy: 0.7576\n",
      "Epoch 346/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2759 - accuracy: 0.6735 - val_loss: 0.3202 - val_accuracy: 0.7576\n",
      "Epoch 347/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2852 - accuracy: 0.6224 - val_loss: 0.3361 - val_accuracy: 0.7273\n",
      "Epoch 348/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2788 - accuracy: 0.6837 - val_loss: 0.3266 - val_accuracy: 0.6970\n",
      "Epoch 349/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2770 - accuracy: 0.6531 - val_loss: 0.3207 - val_accuracy: 0.6970\n",
      "Epoch 350/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2684 - accuracy: 0.6837 - val_loss: 0.3169 - val_accuracy: 0.7273\n",
      "Epoch 351/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2700 - accuracy: 0.6633 - val_loss: 0.3169 - val_accuracy: 0.6970\n",
      "Epoch 352/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2811 - accuracy: 0.6020 - val_loss: 0.3231 - val_accuracy: 0.6970\n",
      "Epoch 353/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2837 - accuracy: 0.6224 - val_loss: 0.3049 - val_accuracy: 0.6970\n",
      "Epoch 354/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2782 - accuracy: 0.6429 - val_loss: 0.3034 - val_accuracy: 0.6061\n",
      "Epoch 355/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2795 - accuracy: 0.6429 - val_loss: 0.3104 - val_accuracy: 0.7576\n",
      "Epoch 356/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2786 - accuracy: 0.6633 - val_loss: 0.3168 - val_accuracy: 0.6970\n",
      "Epoch 357/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2853 - accuracy: 0.6429 - val_loss: 0.3088 - val_accuracy: 0.6667\n",
      "Epoch 358/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2813 - accuracy: 0.6633 - val_loss: 0.2997 - val_accuracy: 0.7273\n",
      "Epoch 359/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2984 - accuracy: 0.6122 - val_loss: 0.3088 - val_accuracy: 0.7273\n",
      "Epoch 360/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2891 - accuracy: 0.5816 - val_loss: 0.3042 - val_accuracy: 0.6667\n",
      "Epoch 361/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2830 - accuracy: 0.6327 - val_loss: 0.3258 - val_accuracy: 0.5758\n",
      "Epoch 362/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2711 - accuracy: 0.6429 - val_loss: 0.2889 - val_accuracy: 0.6364\n",
      "Epoch 363/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2774 - accuracy: 0.6429 - val_loss: 0.3063 - val_accuracy: 0.6667\n",
      "Epoch 364/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2787 - accuracy: 0.6224 - val_loss: 0.3222 - val_accuracy: 0.6667\n",
      "Epoch 365/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2690 - accuracy: 0.6327 - val_loss: 0.3168 - val_accuracy: 0.7576\n",
      "Epoch 366/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2751 - accuracy: 0.6531 - val_loss: 0.3082 - val_accuracy: 0.6970\n",
      "Epoch 367/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2684 - accuracy: 0.7245 - val_loss: 0.2973 - val_accuracy: 0.7273\n",
      "Epoch 368/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2649 - accuracy: 0.6735 - val_loss: 0.3080 - val_accuracy: 0.7576\n",
      "Epoch 369/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3039 - accuracy: 0.5918 - val_loss: 0.3345 - val_accuracy: 0.6364\n",
      "Epoch 370/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.3088 - accuracy: 0.6224 - val_loss: 0.3029 - val_accuracy: 0.6970\n",
      "Epoch 371/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2753 - accuracy: 0.6531 - val_loss: 0.3040 - val_accuracy: 0.6061\n",
      "Epoch 372/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2668 - accuracy: 0.6531 - val_loss: 0.3048 - val_accuracy: 0.6364\n",
      "Epoch 373/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2680 - accuracy: 0.6429 - val_loss: 0.3091 - val_accuracy: 0.6667\n",
      "Epoch 374/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2813 - accuracy: 0.6224 - val_loss: 0.3127 - val_accuracy: 0.7576\n",
      "Epoch 375/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2619 - accuracy: 0.6837 - val_loss: 0.3012 - val_accuracy: 0.6970\n",
      "Epoch 376/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2899 - accuracy: 0.5816 - val_loss: 0.2973 - val_accuracy: 0.6667\n",
      "Epoch 377/400\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.2835 - accuracy: 0.6531 - val_loss: 0.3126 - val_accuracy: 0.7576\n",
      "Epoch 378/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2659 - accuracy: 0.6429 - val_loss: 0.2945 - val_accuracy: 0.6364\n",
      "Epoch 379/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2693 - accuracy: 0.6837 - val_loss: 0.2979 - val_accuracy: 0.6970\n",
      "Epoch 380/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2737 - accuracy: 0.6429 - val_loss: 0.3016 - val_accuracy: 0.6061\n",
      "Epoch 381/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2654 - accuracy: 0.6633 - val_loss: 0.3006 - val_accuracy: 0.6364\n",
      "Epoch 382/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2645 - accuracy: 0.6837 - val_loss: 0.3013 - val_accuracy: 0.6970\n",
      "Epoch 383/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2685 - accuracy: 0.6122 - val_loss: 0.3005 - val_accuracy: 0.6364\n",
      "Epoch 384/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2624 - accuracy: 0.6531 - val_loss: 0.2978 - val_accuracy: 0.6970\n",
      "Epoch 385/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2595 - accuracy: 0.6735 - val_loss: 0.3003 - val_accuracy: 0.6667\n",
      "Epoch 386/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2619 - accuracy: 0.6837 - val_loss: 0.2977 - val_accuracy: 0.6970\n",
      "Epoch 387/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2594 - accuracy: 0.6837 - val_loss: 0.2968 - val_accuracy: 0.6667\n",
      "Epoch 388/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2614 - accuracy: 0.7143 - val_loss: 0.2992 - val_accuracy: 0.6667\n",
      "Epoch 389/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2575 - accuracy: 0.6837 - val_loss: 0.3013 - val_accuracy: 0.6061\n",
      "Epoch 390/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3317 - accuracy: 0.5816 - val_loss: 0.3240 - val_accuracy: 0.6667\n",
      "Epoch 391/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3166 - accuracy: 0.5816 - val_loss: 0.3096 - val_accuracy: 0.5455\n",
      "Epoch 392/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2704 - accuracy: 0.6224 - val_loss: 0.3052 - val_accuracy: 0.6364\n",
      "Epoch 393/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2575 - accuracy: 0.6939 - val_loss: 0.3063 - val_accuracy: 0.6667\n",
      "Epoch 394/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2758 - accuracy: 0.6837 - val_loss: 0.3278 - val_accuracy: 0.5758\n",
      "Epoch 395/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2839 - accuracy: 0.6122 - val_loss: 0.3043 - val_accuracy: 0.6364\n",
      "Epoch 396/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2636 - accuracy: 0.6633 - val_loss: 0.3083 - val_accuracy: 0.6061\n",
      "Epoch 397/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2674 - accuracy: 0.6327 - val_loss: 0.3012 - val_accuracy: 0.6364\n",
      "Epoch 398/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2635 - accuracy: 0.7041 - val_loss: 0.2925 - val_accuracy: 0.6667\n",
      "Epoch 399/400\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2593 - accuracy: 0.7041 - val_loss: 0.3047 - val_accuracy: 0.6364\n",
      "Epoch 400/400\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2609 - accuracy: 0.6837 - val_loss: 0.2938 - val_accuracy: 0.6364\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.657</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.559</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.061</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.503</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.273</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.490</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.273</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.491</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.273</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.264</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.606</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.267</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.636</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.263</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.667</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.259</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.636</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.261</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.636</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     loss  accuracy  val_loss  val_accuracy  epoch\n",
       "0   0.657     0.133     0.619         0.061      0\n",
       "1   0.559     0.143     0.535         0.061      1\n",
       "2   0.503     0.184     0.504         0.273      2\n",
       "3   0.490     0.265     0.496         0.273      3\n",
       "4   0.491     0.316     0.497         0.273      4\n",
       "..    ...       ...       ...           ...    ...\n",
       "395 0.264     0.663     0.308         0.606    395\n",
       "396 0.267     0.633     0.301         0.636    396\n",
       "397 0.263     0.704     0.293         0.667    397\n",
       "398 0.259     0.704     0.305         0.636    398\n",
       "399 0.261     0.684     0.294         0.636    399\n",
       "\n",
       "[400 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Curvas de Pérdida=Training Loss<br>Epoch=%{x}<br>Binary Cross Entropy=%{y}<extra></extra>",
         "legendgroup": "Training Loss",
         "line": {
          "color": "#46039f",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Training Loss",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399
         ],
         "xaxis": "x",
         "y": [
          0.6573367714881897,
          0.5593845844268799,
          0.5031857490539551,
          0.48994797468185425,
          0.4909020662307739,
          0.4906608760356903,
          0.49676454067230225,
          0.49562379717826843,
          0.4915952980518341,
          0.4890657067298889,
          0.4927055239677429,
          0.48986294865608215,
          0.48528504371643066,
          0.4870285093784332,
          0.48903295397758484,
          0.48822301626205444,
          0.49139755964279175,
          0.4916626214981079,
          0.4966932535171509,
          0.4916141629219055,
          0.4924549162387848,
          0.48926496505737305,
          0.48580530285835266,
          0.48881202936172485,
          0.4888119697570801,
          0.4863402843475342,
          0.4900261163711548,
          0.49001410603523254,
          0.4893018901348114,
          0.485851913690567,
          0.48675769567489624,
          0.4894620180130005,
          0.48142844438552856,
          0.4780091941356659,
          0.4775180518627167,
          0.47484004497528076,
          0.4739689826965332,
          0.4563048183917999,
          0.4561987817287445,
          0.44699227809906006,
          0.43557408452033997,
          0.4268565773963928,
          0.4161687195301056,
          0.4174293279647827,
          0.4073847234249115,
          0.41203153133392334,
          0.40140536427497864,
          0.4036787450313568,
          0.4025273621082306,
          0.39571455121040344,
          0.39807093143463135,
          0.40009570121765137,
          0.3960610628128052,
          0.3881462514400482,
          0.38987740874290466,
          0.3911445736885071,
          0.3922119438648224,
          0.3829081654548645,
          0.38815438747406006,
          0.38376086950302124,
          0.3898557424545288,
          0.38939350843429565,
          0.3877328634262085,
          0.3788979947566986,
          0.377234548330307,
          0.3789275884628296,
          0.3778952658176422,
          0.37472498416900635,
          0.3700751066207886,
          0.37116268277168274,
          0.37050390243530273,
          0.3675486147403717,
          0.36707040667533875,
          0.36766934394836426,
          0.3578333258628845,
          0.3562485873699188,
          0.3517909646034241,
          0.36219891905784607,
          0.36040645837783813,
          0.36222290992736816,
          0.3505832552909851,
          0.3457276225090027,
          0.3518938720226288,
          0.3482540547847748,
          0.3563050925731659,
          0.34904158115386963,
          0.34709328413009644,
          0.34887373447418213,
          0.35284778475761414,
          0.35113435983657837,
          0.3520243465900421,
          0.34681469202041626,
          0.3316856622695923,
          0.34498897194862366,
          0.3578929603099823,
          0.3434731960296631,
          0.34613415598869324,
          0.3490319848060608,
          0.3406825661659241,
          0.3321177363395691,
          0.34293311834335327,
          0.3374975621700287,
          0.3370305001735687,
          0.34183260798454285,
          0.33621931076049805,
          0.3423289358615875,
          0.3369503319263458,
          0.33571138978004456,
          0.33930203318595886,
          0.34024569392204285,
          0.33960649371147156,
          0.3454076945781708,
          0.3343040645122528,
          0.3412766456604004,
          0.3395497500896454,
          0.34486761689186096,
          0.3303334414958954,
          0.3478473722934723,
          0.34235429763793945,
          0.3331473469734192,
          0.33568650484085083,
          0.33192840218544006,
          0.3325822651386261,
          0.32378241419792175,
          0.3374095559120178,
          0.33013418316841125,
          0.3397059440612793,
          0.32992982864379883,
          0.33154475688934326,
          0.33380621671676636,
          0.34265071153640747,
          0.3436257541179657,
          0.3407425284385681,
          0.3313775658607483,
          0.33811110258102417,
          0.3359542787075043,
          0.3294309973716736,
          0.3303987681865692,
          0.3318084478378296,
          0.3348352611064911,
          0.3273269832134247,
          0.3261183798313141,
          0.3317507803440094,
          0.3296299874782562,
          0.3350451588630676,
          0.3357478082180023,
          0.33014410734176636,
          0.33182579278945923,
          0.3501397669315338,
          0.32746434211730957,
          0.34281930327415466,
          0.32843518257141113,
          0.3271276652812958,
          0.31726953387260437,
          0.3278641700744629,
          0.3198433220386505,
          0.31511056423187256,
          0.32231593132019043,
          0.32469984889030457,
          0.33159178495407104,
          0.3197278380393982,
          0.31994643807411194,
          0.3218976557254791,
          0.31746166944503784,
          0.3244616687297821,
          0.31703779101371765,
          0.32236313819885254,
          0.3348895013332367,
          0.32883337140083313,
          0.330311119556427,
          0.3376096487045288,
          0.3320029377937317,
          0.334142804145813,
          0.3260766267776489,
          0.3258739411830902,
          0.3288680613040924,
          0.3186512291431427,
          0.30817925930023193,
          0.3166390061378479,
          0.31317970156669617,
          0.3261950612068176,
          0.3217417895793915,
          0.32680121064186096,
          0.328061044216156,
          0.31754812598228455,
          0.3146772086620331,
          0.323932409286499,
          0.3185047507286072,
          0.30474093556404114,
          0.33176282048225403,
          0.31692272424697876,
          0.3158778250217438,
          0.310712069272995,
          0.32075050473213196,
          0.32691413164138794,
          0.3228823244571686,
          0.3153965175151825,
          0.3219149708747864,
          0.302614688873291,
          0.3114197254180908,
          0.311276912689209,
          0.3141668140888214,
          0.3112942576408386,
          0.3052595853805542,
          0.3046641945838928,
          0.3048599660396576,
          0.3044213354587555,
          0.31634989380836487,
          0.3143045902252197,
          0.3095783591270447,
          0.30482161045074463,
          0.30204373598098755,
          0.3093563914299011,
          0.31775033473968506,
          0.3063359260559082,
          0.30551931262016296,
          0.30969011783599854,
          0.30889126658439636,
          0.30071187019348145,
          0.308672159910202,
          0.30681249499320984,
          0.3094637095928192,
          0.3165086805820465,
          0.3132738769054413,
          0.3000226616859436,
          0.30663353204727173,
          0.30796071887016296,
          0.31287306547164917,
          0.30607062578201294,
          0.2961321473121643,
          0.3132641911506653,
          0.29934173822402954,
          0.29829832911491394,
          0.3120013475418091,
          0.29945021867752075,
          0.29791682958602905,
          0.3050324320793152,
          0.29393270611763,
          0.3000966012477875,
          0.31706440448760986,
          0.32025665044784546,
          0.3054361641407013,
          0.30615949630737305,
          0.29876694083213806,
          0.29975295066833496,
          0.2952285408973694,
          0.29928821325302124,
          0.2879910469055176,
          0.3060353100299835,
          0.2909943461418152,
          0.2921818792819977,
          0.3220857083797455,
          0.3027707040309906,
          0.3112885653972626,
          0.3107203543186188,
          0.300925612449646,
          0.3049502968788147,
          0.30487459897994995,
          0.29083746671676636,
          0.2868495285511017,
          0.3044275939464569,
          0.291154146194458,
          0.29945218563079834,
          0.2959343194961548,
          0.2938549220561981,
          0.3065997064113617,
          0.29974228143692017,
          0.29767897725105286,
          0.2931079566478729,
          0.3115025460720062,
          0.3048109710216522,
          0.3080972731113434,
          0.2930549383163452,
          0.2929808795452118,
          0.2979680001735687,
          0.29651230573654175,
          0.2978000044822693,
          0.30220577120780945,
          0.29444602131843567,
          0.31043219566345215,
          0.3000076711177826,
          0.30228695273399353,
          0.2933138608932495,
          0.2928663194179535,
          0.2997000217437744,
          0.30441582202911377,
          0.2944957911968231,
          0.2988542914390564,
          0.2920491099357605,
          0.29989373683929443,
          0.29570192098617554,
          0.30649498105049133,
          0.2930926978588104,
          0.2868000268936157,
          0.3024803698062897,
          0.29127785563468933,
          0.2888943552970886,
          0.2840611934661865,
          0.30304357409477234,
          0.28789955377578735,
          0.2941746413707733,
          0.2935619652271271,
          0.27968311309814453,
          0.28936800360679626,
          0.2882385551929474,
          0.280347615480423,
          0.2939946949481964,
          0.2854035496711731,
          0.2902100086212158,
          0.29279664158821106,
          0.3051081895828247,
          0.3146846294403076,
          0.2802952826023102,
          0.29248476028442383,
          0.2871294319629669,
          0.29000332951545715,
          0.29911813139915466,
          0.3132668137550354,
          0.3181290924549103,
          0.2962680459022522,
          0.28454649448394775,
          0.2919899821281433,
          0.29062390327453613,
          0.28162652254104614,
          0.2849874496459961,
          0.27674219012260437,
          0.294582724571228,
          0.298410564661026,
          0.2924794852733612,
          0.27692437171936035,
          0.2800437808036804,
          0.28286662697792053,
          0.28733789920806885,
          0.2835637927055359,
          0.2824259102344513,
          0.2831442952156067,
          0.2876465320587158,
          0.2840762734413147,
          0.2631405293941498,
          0.2885330319404602,
          0.2824001610279083,
          0.27939754724502563,
          0.2991868257522583,
          0.2851480543613434,
          0.2794204652309418,
          0.27593210339546204,
          0.2851889729499817,
          0.2788179814815521,
          0.2769586145877838,
          0.2684314250946045,
          0.27001968026161194,
          0.28110700845718384,
          0.28368639945983887,
          0.27815109491348267,
          0.2794642448425293,
          0.2785888612270355,
          0.2853107750415802,
          0.28132277727127075,
          0.2983839213848114,
          0.28914156556129456,
          0.28299927711486816,
          0.2711169421672821,
          0.2773972749710083,
          0.27871274948120117,
          0.26901775598526,
          0.27508047223091125,
          0.2683582603931427,
          0.2649168372154236,
          0.30387675762176514,
          0.3087989389896393,
          0.27526146173477173,
          0.26675522327423096,
          0.26800984144210815,
          0.28130561113357544,
          0.26186811923980713,
          0.28994426131248474,
          0.2835153639316559,
          0.2658771276473999,
          0.2692680358886719,
          0.2736889123916626,
          0.2654341757297516,
          0.2644730508327484,
          0.268476277589798,
          0.26239603757858276,
          0.25954708456993103,
          0.2618935704231262,
          0.259425550699234,
          0.2613835036754608,
          0.25750473141670227,
          0.3317057490348816,
          0.3165920078754425,
          0.27044960856437683,
          0.2574528753757477,
          0.2758105397224426,
          0.2839033901691437,
          0.2635669708251953,
          0.2674192190170288,
          0.26346901059150696,
          0.25926536321640015,
          0.2609487473964691
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Curvas de Pérdida=Validation Loss<br>Epoch=%{x}<br>Binary Cross Entropy=%{y}<extra></extra>",
         "legendgroup": "Validation Loss",
         "line": {
          "color": "#fb9f3a",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Validation Loss",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399
         ],
         "xaxis": "x",
         "y": [
          0.6185290813446045,
          0.5353409647941589,
          0.5037895441055298,
          0.4957890808582306,
          0.4973794221878052,
          0.5003848671913147,
          0.505486786365509,
          0.4928136169910431,
          0.503406286239624,
          0.49776598811149597,
          0.5007967948913574,
          0.49504441022872925,
          0.4972367584705353,
          0.49637505412101746,
          0.4966467022895813,
          0.4946863055229187,
          0.49678248167037964,
          0.49651798605918884,
          0.49972453713417053,
          0.4960479140281677,
          0.495403915643692,
          0.5023391246795654,
          0.4914262890815735,
          0.49893903732299805,
          0.49411237239837646,
          0.4955631494522095,
          0.4958685040473938,
          0.4987618923187256,
          0.4955750107765198,
          0.49606746435165405,
          0.4917859137058258,
          0.4984230399131775,
          0.4884311258792877,
          0.49644309282302856,
          0.4882207214832306,
          0.4873676896095276,
          0.4820288419723511,
          0.47020646929740906,
          0.4780470132827759,
          0.4653901755809784,
          0.45330068469047546,
          0.45164382457733154,
          0.4433256983757019,
          0.44932207465171814,
          0.4399665892124176,
          0.4423829913139343,
          0.4396425783634186,
          0.4440336525440216,
          0.43988656997680664,
          0.43582862615585327,
          0.43537020683288574,
          0.4283547103404999,
          0.43632519245147705,
          0.4293861985206604,
          0.4306327700614929,
          0.43088045716285706,
          0.4182884395122528,
          0.4217040538787842,
          0.42231398820877075,
          0.42264077067375183,
          0.42520156502723694,
          0.4306003749370575,
          0.41504374146461487,
          0.41949766874313354,
          0.40930649638175964,
          0.4129621684551239,
          0.412720650434494,
          0.4039413630962372,
          0.41399112343788147,
          0.40002527832984924,
          0.40196871757507324,
          0.38690829277038574,
          0.38732194900512695,
          0.3950805366039276,
          0.3922273814678192,
          0.3968847990036011,
          0.38223347067832947,
          0.3962947726249695,
          0.3861326575279236,
          0.38028132915496826,
          0.37082433700561523,
          0.3726312518119812,
          0.3742125332355499,
          0.3717826008796692,
          0.36902788281440735,
          0.3779512643814087,
          0.380822092294693,
          0.3679899275302887,
          0.36510658264160156,
          0.36732396483421326,
          0.36147865653038025,
          0.38399848341941833,
          0.36009469628334045,
          0.37288907170295715,
          0.37244006991386414,
          0.36757761240005493,
          0.37019646167755127,
          0.3584275543689728,
          0.361335426568985,
          0.3559970259666443,
          0.35784482955932617,
          0.3744244873523712,
          0.36405348777770996,
          0.35966819524765015,
          0.35083431005477905,
          0.37237387895584106,
          0.35558459162712097,
          0.3478144109249115,
          0.37543967366218567,
          0.370486855506897,
          0.36237064003944397,
          0.3537749648094177,
          0.36394262313842773,
          0.3596448302268982,
          0.3650425970554352,
          0.3602626919746399,
          0.3622278869152069,
          0.35973769426345825,
          0.35007205605506897,
          0.3531724214553833,
          0.36234116554260254,
          0.3639005124568939,
          0.3513677716255188,
          0.3497656285762787,
          0.3495030403137207,
          0.3576439619064331,
          0.3472706973552704,
          0.3519429564476013,
          0.34999263286590576,
          0.3542359471321106,
          0.3461858332157135,
          0.3570895493030548,
          0.35444512963294983,
          0.34745681285858154,
          0.3427916169166565,
          0.3470735251903534,
          0.355451762676239,
          0.3450026512145996,
          0.3371577262878418,
          0.3480258285999298,
          0.3487264811992645,
          0.34399691224098206,
          0.34607699513435364,
          0.347354531288147,
          0.3528987169265747,
          0.35235247015953064,
          0.3409230709075928,
          0.35132575035095215,
          0.33825919032096863,
          0.35554540157318115,
          0.3573908507823944,
          0.3612300157546997,
          0.3452465534210205,
          0.3356902301311493,
          0.34496140480041504,
          0.34024402499198914,
          0.336900532245636,
          0.3573421537876129,
          0.3437137007713318,
          0.3458561897277832,
          0.3406946063041687,
          0.3366073966026306,
          0.34619104862213135,
          0.3400939106941223,
          0.352127343416214,
          0.35625898838043213,
          0.3461366295814514,
          0.36513450741767883,
          0.34033268690109253,
          0.3461993932723999,
          0.3456932008266449,
          0.3524172902107239,
          0.3478766679763794,
          0.35648736357688904,
          0.3454999029636383,
          0.34290811419487,
          0.3513813018798828,
          0.345211923122406,
          0.3321303129196167,
          0.33211004734039307,
          0.33885711431503296,
          0.3429342806339264,
          0.3323485553264618,
          0.33411893248558044,
          0.32567012310028076,
          0.3298375904560089,
          0.3216857612133026,
          0.3293899893760681,
          0.3324348032474518,
          0.34067294001579285,
          0.33242249488830566,
          0.3279990255832672,
          0.32192543148994446,
          0.33307862281799316,
          0.3357361853122711,
          0.34511715173721313,
          0.3444380760192871,
          0.325629323720932,
          0.33336856961250305,
          0.3285059928894043,
          0.3177824020385742,
          0.3197544813156128,
          0.322273313999176,
          0.3225918114185333,
          0.31782159209251404,
          0.31401869654655457,
          0.31998151540756226,
          0.3139268159866333,
          0.324587345123291,
          0.32326367497444153,
          0.32258176803588867,
          0.32340767979621887,
          0.32872331142425537,
          0.32731616497039795,
          0.3186241388320923,
          0.3246331512928009,
          0.3188930153846741,
          0.33626192808151245,
          0.3465932309627533,
          0.3310854732990265,
          0.32410481572151184,
          0.3380154073238373,
          0.3137304186820984,
          0.3364996910095215,
          0.32218366861343384,
          0.33085137605667114,
          0.32667025923728943,
          0.3352351784706116,
          0.32218489050865173,
          0.32541272044181824,
          0.3182731866836548,
          0.3230317533016205,
          0.32266727089881897,
          0.3208277225494385,
          0.3336034119129181,
          0.3186366856098175,
          0.3281751275062561,
          0.3125345706939697,
          0.3246159255504608,
          0.3088212013244629,
          0.34483641386032104,
          0.3302140235900879,
          0.3187795579433441,
          0.32611656188964844,
          0.3141363561153412,
          0.3217200040817261,
          0.3183955252170563,
          0.32111290097236633,
          0.3262823224067688,
          0.3384152352809906,
          0.33216047286987305,
          0.32468289136886597,
          0.3406437635421753,
          0.3325384855270386,
          0.3529013395309448,
          0.3471022844314575,
          0.3379247486591339,
          0.3421119153499603,
          0.33782854676246643,
          0.33028262853622437,
          0.32900285720825195,
          0.33734846115112305,
          0.3306003510951996,
          0.3363220989704132,
          0.3218378722667694,
          0.32996582984924316,
          0.32225099205970764,
          0.3246808350086212,
          0.31639623641967773,
          0.32948437333106995,
          0.33456680178642273,
          0.3337092995643616,
          0.33362045884132385,
          0.32457491755485535,
          0.35360944271087646,
          0.3920513391494751,
          0.34516721963882446,
          0.3348572552204132,
          0.3209202289581299,
          0.3259400427341461,
          0.3296074867248535,
          0.3320036828517914,
          0.3285858631134033,
          0.33481505513191223,
          0.3207467496395111,
          0.33316144347190857,
          0.32818323373794556,
          0.327428936958313,
          0.3260325789451599,
          0.3178289532661438,
          0.3373984098434448,
          0.30919551849365234,
          0.3161286413669586,
          0.31270483136177063,
          0.3168531358242035,
          0.3368622362613678,
          0.30409684777259827,
          0.30576497316360474,
          0.3271820545196533,
          0.33847808837890625,
          0.3145945966243744,
          0.3087712228298187,
          0.30550718307495117,
          0.3136294484138489,
          0.30840277671813965,
          0.3065992295742035,
          0.30416256189346313,
          0.3133690059185028,
          0.31245648860931396,
          0.37421560287475586,
          0.386324942111969,
          0.33628687262535095,
          0.3255482316017151,
          0.3199279010295868,
          0.31191301345825195,
          0.3112642765045166,
          0.32434770464897156,
          0.3107312023639679,
          0.33577680587768555,
          0.3290656805038452,
          0.3247598111629486,
          0.3235282599925995,
          0.3219161927700043,
          0.3165771961212158,
          0.31697243452072144,
          0.3391944468021393,
          0.32520249485969543,
          0.33050674200057983,
          0.31188759207725525,
          0.3155178427696228,
          0.3134629428386688,
          0.31937798857688904,
          0.3183498978614807,
          0.32089856266975403,
          0.3148670196533203,
          0.31416791677474976,
          0.31090742349624634,
          0.30744603276252747,
          0.3022797107696533,
          0.30627480149269104,
          0.3016993999481201,
          0.30128276348114014,
          0.31137293577194214,
          0.3231411874294281,
          0.32557809352874756,
          0.32015952467918396,
          0.3360982835292816,
          0.3265922963619232,
          0.3206574022769928,
          0.3169471323490143,
          0.31688469648361206,
          0.3230785131454468,
          0.3048590421676636,
          0.3034185767173767,
          0.3103615641593933,
          0.3168104887008667,
          0.3088001012802124,
          0.2996608018875122,
          0.3088347613811493,
          0.3042412996292114,
          0.32577890157699585,
          0.2888723611831665,
          0.306295245885849,
          0.3222232162952423,
          0.31677377223968506,
          0.3082095980644226,
          0.29727116227149963,
          0.30804407596588135,
          0.33445438742637634,
          0.30286821722984314,
          0.3040253520011902,
          0.3047884702682495,
          0.30905261635780334,
          0.31266942620277405,
          0.3011932373046875,
          0.2972867786884308,
          0.3126312792301178,
          0.2945106625556946,
          0.29791688919067383,
          0.30162423849105835,
          0.3006369173526764,
          0.3012768030166626,
          0.30053964257240295,
          0.2978116273880005,
          0.30028632283210754,
          0.2976754903793335,
          0.29675862193107605,
          0.29921770095825195,
          0.3013051152229309,
          0.3239705264568329,
          0.30964940786361694,
          0.3052111566066742,
          0.30633649230003357,
          0.3277609944343567,
          0.3043394684791565,
          0.3083066940307617,
          0.3012234568595886,
          0.2925265431404114,
          0.3047328293323517,
          0.2937951385974884
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "Curvas de Pérdida"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Gráficas de Pérdida de Entrenamiento y Evaluación"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Binary Cross Entropy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.657</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.559</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.061</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.503</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.273</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.490</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.273</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.491</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.273</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.264</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.606</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.267</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.636</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.263</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.667</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.259</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.636</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.261</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.636</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Training Loss  accuracy  Validation Loss  val_accuracy  epoch\n",
       "0            0.657     0.133            0.619         0.061      0\n",
       "1            0.559     0.143            0.535         0.061      1\n",
       "2            0.503     0.184            0.504         0.273      2\n",
       "3            0.490     0.265            0.496         0.273      3\n",
       "4            0.491     0.316            0.497         0.273      4\n",
       "..             ...       ...              ...           ...    ...\n",
       "395          0.264     0.663            0.308         0.606    395\n",
       "396          0.267     0.633            0.301         0.636    396\n",
       "397          0.263     0.704            0.293         0.667    397\n",
       "398          0.259     0.704            0.305         0.636    398\n",
       "399          0.261     0.684            0.294         0.636    399\n",
       "\n",
       "[400 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = 0.03\n",
    "epochs = 400\n",
    "batch_size = 5\n",
    "model = my_model(learning_rate)\n",
    "history= train_model(model, train_features, train_labels, epochs, batch_size)\n",
    "display(history)\n",
    "loss_curves(history)\n",
    "#view_filters(model)\n",
    "display(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EA385A7EB0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 0s/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normal</th>\n",
       "      <th>collision_in_tool</th>\n",
       "      <th>collision_in_part</th>\n",
       "      <th>bottom_collision</th>\n",
       "      <th>bottom_obstruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.529</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.677</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.451</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.439</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.262</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.667</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    normal  collision_in_tool  collision_in_part  bottom_collision  \\\n",
       "0    0.529              0.178              0.282             0.008   \n",
       "1    0.000              0.000              0.000             0.000   \n",
       "2    0.000              0.000              0.000             1.000   \n",
       "3    0.677              0.136              0.181             0.004   \n",
       "4    0.451              0.155              0.324             0.061   \n",
       "..     ...                ...                ...               ...   \n",
       "28   0.000              0.001              0.006             0.989   \n",
       "29   0.439              0.163              0.371             0.021   \n",
       "30   0.001              0.002              0.007             0.986   \n",
       "31   0.262              0.150              0.422             0.152   \n",
       "32   0.667              0.140              0.187             0.004   \n",
       "\n",
       "    bottom_obstruction  \n",
       "0                0.003  \n",
       "1                1.000  \n",
       "2                0.000  \n",
       "3                0.002  \n",
       "4                0.009  \n",
       "..                 ...  \n",
       "28               0.004  \n",
       "29               0.006  \n",
       "30               0.003  \n",
       "31               0.014  \n",
       "32               0.002  \n",
       "\n",
       "[33 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1  2  3  4  5\n",
       "135  0  0  0  1  0\n",
       "115  0  0  0  0  1\n",
       "131  0  0  0  0  1\n",
       "55   1  0  0  0  0\n",
       "95   0  1  0  0  0\n",
       "..  .. .. .. .. ..\n",
       "134  0  0  0  1  0\n",
       "160  0  1  0  0  0\n",
       "139  0  0  0  1  0\n",
       "78   0  0  1  0  0\n",
       "60   1  0  0  0  0\n",
       "\n",
       "[33 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = model.predict(test_features)\n",
    "predictions_df = pd.DataFrame(predictions, columns=['normal', 'collision_in_tool', 'collision_in_part', 'bottom_collision', 'bottom_obstruction'])\n",
    "display(predictions_df)\n",
    "display(test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
