{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar Modulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import plotly.express as px\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                labels 1  2  3  4  5 \n",
       "0               normal               \n",
       "1               normal               \n",
       "2               normal               \n",
       "3               normal               \n",
       "4               normal               \n",
       "..                 ... .. .. .. .. ..\n",
       "159  collision_in_tool               \n",
       "160  collision_in_tool               \n",
       "161  collision_in_tool               \n",
       "162  collision_in_tool               \n",
       "163  collision_in_tool               \n",
       "\n",
       "[164 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ -2.,  -1.,  81.,   0.,  -5.,   0.],\n",
       "        [ -2.,  -1.,  79.,   0.,  -4.,   0.],\n",
       "        [ -2.,  -1.,  79.,   0.,  -4.,   0.],\n",
       "        ...,\n",
       "        [ -2.,  -1.,  78.,   0.,  -5.,   0.],\n",
       "        [ -3.,  -1.,  80.,   1.,  -4.,   1.],\n",
       "        [ -2.,  -1.,  79.,   0.,  -4.,   0.]],\n",
       "\n",
       "       [[  6.,  -1.,  79.,  -2.,   4.,  -3.],\n",
       "        [ 42.,  -3.,  80.,   5.,  53.,   3.],\n",
       "        [ -5.,   4.,  74., -15., -10.,  -1.],\n",
       "        ...,\n",
       "        [ -1.,  -5.,  80.,   6.,  -6.,   0.],\n",
       "        [ -4.,   5.,  78., -14.,  -9.,  -4.],\n",
       "        [ -4.,   1.,  80.,  -3., -12.,   5.]],\n",
       "\n",
       "       [[ -2.,  -6.,  85.,  14.,  -5.,   2.],\n",
       "        [  0.,   2.,  74.,  -7.,   1.,   0.],\n",
       "        [ -4.,  -5.,  76.,   7., -11.,   4.],\n",
       "        ...,\n",
       "        [  0.,  -9.,  87.,  13.,  -5.,   2.],\n",
       "        [ -5.,   5.,  67., -17., -16.,   7.],\n",
       "        [ -6., -10.,  86.,  16., -14.,  -1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-12.,  17.,   3., -19., -10.,  -4.],\n",
       "        [-12.,  12.,  11., -13., -16.,  -4.],\n",
       "        [ -8.,   3.,   6.,   2., -11.,  -4.],\n",
       "        ...,\n",
       "        [  0.,   1.,   3.,   1.,   1.,  -3.],\n",
       "        [  1.,   2.,   9.,   0.,   3.,  -4.],\n",
       "        [  1.,   2.,   9.,   0.,   3.,  -4.]],\n",
       "\n",
       "       [[-41.,  21.,  -5., -23., -59.,  -4.],\n",
       "        [-32.,  21.,  -6., -25., -45.,  -4.],\n",
       "        [-21.,  12.,  -6., -14., -31.,  -4.],\n",
       "        ...,\n",
       "        [ -4.,   4.,   3.,   0.,  -1.,  -3.],\n",
       "        [ -4.,   3.,   2.,   2.,  -3.,  -3.],\n",
       "        [ -2.,   3.,   5.,   0.,  -2.,  -3.]],\n",
       "\n",
       "       [[  9., -10., -11.,  17.,   7.,  -4.],\n",
       "        [  5.,   0.,   4.,   0.,   7.,  -4.],\n",
       "        [ -3.,   6.,  -2.,  -8.,  -8.,  -4.],\n",
       "        ...,\n",
       "        [ -1.,   1.,  -3.,  -3.,  -2.,  -3.],\n",
       "        [  0.,  -1.,  -5.,  -1.,   1.,  -3.],\n",
       "        [ -1.,   1.,   4.,   0.,  -1.,  -3.]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.3f}\".format\n",
    "\n",
    "data = np.loadtxt(\"lp5.csv\", delimiter=\",\", dtype=str)\n",
    "data_copy = data #va de 0 a 2623 o sea 2624 datos\n",
    "cont = 16\n",
    "labels = []\n",
    "numbers = []\n",
    "features= np.zeros((164,15,6))\n",
    "\n",
    "for i in range(len(data_copy)):\n",
    "    if(cont == 16):\n",
    "        labels.append(data_copy[i])\n",
    "    if(cont<16):\n",
    "        numbers.append(data_copy[i])\n",
    "    cont -= 1\n",
    "    if(cont == 0):\n",
    "        cont = 16\n",
    "        \n",
    "cont = 0\n",
    "for i in range(164):\n",
    "    for j in range(15):\n",
    "        for z in range(6):\n",
    "            features[i][j][z] = numbers[cont][z]\n",
    "        cont += 1\n",
    "        \n",
    "labels_df = pd.DataFrame(labels, columns=['labels','1','2','3','4','5'])\n",
    "#labels_df = labels_df.loc[:,['labels']]\n",
    "display(labels_df)\n",
    "display(features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manejo de Datos y Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dave4\\AppData\\Local\\Temp\\ipykernel_16648\\1747463221.py:8: FutureWarning:\n",
      "\n",
      "In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.77660263, 0.78156702, 0.79084826, 0.77055903, 0.7684006 ,\n",
       "         0.77422836],\n",
       "        [0.77466005, 0.78739478, 0.78717893, 0.75156486, 0.7638679 ,\n",
       "         0.77206993],\n",
       "        [0.77401252, 0.78804231, 0.79041658, 0.74465789, 0.76106195,\n",
       "         0.77012735],\n",
       "        ...,\n",
       "        [0.78005612, 0.7804878 , 0.78437298, 0.78135118, 0.78113533,\n",
       "         0.78027196],\n",
       "        [0.78027196, 0.78091949, 0.77962443, 0.78091949, 0.78027196,\n",
       "         0.78027196],\n",
       "        [0.78027196, 0.78070365, 0.78286208, 0.78156702, 0.78091949,\n",
       "         0.78005612]],\n",
       "\n",
       "       [[0.78027196, 0.77940859, 0.79905029, 0.78372545, 0.77962443,\n",
       "         0.78113533],\n",
       "        [0.78070365, 0.78113533, 0.79667602, 0.77919275, 0.78091949,\n",
       "         0.78070365],\n",
       "        [0.77984028, 0.77962443, 0.79710771, 0.78221455, 0.77832938,\n",
       "         0.78156702],\n",
       "        ...,\n",
       "        [0.78070365, 0.77876106, 0.79948198, 0.78350961, 0.77962443,\n",
       "         0.78113533],\n",
       "        [0.77962443, 0.78178286, 0.79516512, 0.77703432, 0.77725016,\n",
       "         0.78221455],\n",
       "        [0.77940859, 0.77854522, 0.79926613, 0.78415713, 0.77768185,\n",
       "         0.7804878 ]],\n",
       "\n",
       "       [[0.78502051, 0.77725016, 0.78264623, 0.78631556, 0.78696309,\n",
       "         0.7804878 ],\n",
       "        [0.78588388, 0.78005612, 0.78717893, 0.78243039, 0.78890568,\n",
       "         0.77984028],\n",
       "        [0.78653141, 0.77876106, 0.78027196, 0.78480466, 0.78933736,\n",
       "         0.78027196],\n",
       "        ...,\n",
       "        [0.78005612, 0.78135118, 0.77962443, 0.7804878 , 0.77962443,\n",
       "         0.77962443],\n",
       "        [0.78005612, 0.78091949, 0.7789769 , 0.78091949, 0.78027196,\n",
       "         0.77962443],\n",
       "        [0.7804878 , 0.78091949, 0.78005612, 0.78156702, 0.78070365,\n",
       "         0.77962443]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.78005612, 0.78070365, 0.79689186, 0.77919275, 0.77660263,\n",
       "         0.77984028],\n",
       "        [0.77962443, 0.77832938, 0.79883445, 0.78307792, 0.77617095,\n",
       "         0.77811353],\n",
       "        [0.78005612, 0.78264623, 0.79516512, 0.77487589, 0.77681848,\n",
       "         0.7789769 ],\n",
       "        ...,\n",
       "        [0.7804878 , 0.77832938, 0.79969782, 0.78307792, 0.777466  ,\n",
       "         0.77854522],\n",
       "        [0.77940859, 0.77984028, 0.79710771, 0.77962443, 0.77573926,\n",
       "         0.77984028],\n",
       "        [0.77940859, 0.78243039, 0.79710771, 0.77530758, 0.77617095,\n",
       "         0.77940859]],\n",
       "\n",
       "       [[0.77077488, 0.79494928, 0.77617095, 0.75588172, 0.77832938,\n",
       "         0.7804878 ],\n",
       "        [0.7653788 , 0.78135118, 0.78243039, 0.777466  , 0.7638679 ,\n",
       "         0.78027196],\n",
       "        [0.76365206, 0.78264623, 0.78350961, 0.78005612, 0.75674509,\n",
       "         0.78027196],\n",
       "        ...,\n",
       "        [0.77984028, 0.78113533, 0.78243039, 0.78113533, 0.78005612,\n",
       "         0.78005612],\n",
       "        [0.78027196, 0.78113533, 0.78350961, 0.78091949, 0.78070365,\n",
       "         0.78027196],\n",
       "        [0.7804878 , 0.78091949, 0.78480466, 0.78221455, 0.78156702,\n",
       "         0.78005612]],\n",
       "\n",
       "       [[0.77789769, 0.77789769, 0.7744442 , 0.78696309, 0.78653141,\n",
       "         0.77789769],\n",
       "        [0.77832938, 0.77811353, 0.77422836, 0.78631556, 0.78868983,\n",
       "         0.7789769 ],\n",
       "        [0.77832938, 0.77789769, 0.77466005, 0.78696309, 0.78804231,\n",
       "         0.77768185],\n",
       "        ...,\n",
       "        [0.67882581, 0.70472696, 0.09842435, 0.7759551 , 0.85732787,\n",
       "         0.74940643],\n",
       "        [0.69933089, 0.73796676, 0.34448521, 0.77099072, 0.81674941,\n",
       "         0.77530758],\n",
       "        [0.72199439, 0.73170732, 0.7155191 , 0.81804446, 0.69156054,\n",
       "         0.77012735]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1  2  3  4  5\n",
       "84   0  0  1  0  0\n",
       "2    1  0  0  0  0\n",
       "94   0  1  0  0  0\n",
       "45   0  0  1  0  0\n",
       "42   0  0  1  0  0\n",
       "..  .. .. .. .. ..\n",
       "71   0  0  1  0  0\n",
       "106  0  0  0  1  0\n",
       "14   1  0  0  0  0\n",
       "92   0  1  0  0  0\n",
       "102  0  0  0  0  1\n",
       "\n",
       "[131 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes_dict = {'normal':'1 0 0 0 0', #1\n",
    "        'collision_in_tool':'0 1 0 0 0', #2\n",
    "        'collision_in_part':'0 0 1 0 0', #3\n",
    "        'bottom_collision':'0 0 0 1 0', #4\n",
    "        'bottom_obstruction':'0 0 0 0 1'} #5\n",
    "\n",
    "labels_df = labels_df.replace({'labels':classes_dict})\n",
    "labels_df[['1', '2', '3', '4', '5']] = labels_df['labels'].str.split(' ', 4, expand= True)\n",
    "labels_df = labels_df.loc[:,['1', '2', '3', '4', '5']]\n",
    "labels_df[['1', '2', '3', '4', '5']]=labels_df[['1', '2', '3', '4', '5']].astype(str).astype(int)\n",
    "\n",
    "def norm(x):\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "    range = x_max - x_min  #min max entre 0 y 1\n",
    "    return((x-x_min)/(range))\n",
    "\n",
    "norm_features = norm(features)\n",
    "#split para entrenamiento y validacion\n",
    "train_features, test_features, train_labels, test_labels  = train_test_split(norm_features, labels_df, test_size=0.2, random_state= 42)\n",
    "display(train_features)\n",
    "display(train_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(learning_rate):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(8, (3,3), activation='relu', input_shape = (15,6,1), padding='same'), # #de feature maps, dimensiones del kernel, kernel entre más pequeño mejor y se prefiere un número impar\n",
    "        tf.keras.layers.Conv2D(8, (3,3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D((2,2), strides= 2, padding= 'valid'), # dimensiones del pooling\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Conv2D(16, (3,3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.Conv2D(16, (3,3), activation='relu', padding='same'),# #de feature maps, dimensiones del kernel, kernel entre más pequeño mejor y se prefiere un número impar\n",
    "        tf.keras.layers.MaxPooling2D((2,2), strides= 2, padding='valid'), # dimensiones del pooling\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(24, activation='relu'),\n",
    "        tf.keras.layers.Dense(5, activation='softmax')])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "# input shape es 15 filas, 6 columnas y 1 canal de color\n",
    "#model = model = my_model(0.01)\n",
    "#model.summary()\n",
    "\n",
    "# initialize tuner to run the model.\n",
    "# using the Hyperband search algorithm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, features, labels, epochs, batch_size):\n",
    "    history = model.fit(\n",
    "        x = features,\n",
    "        y = labels,\n",
    "        epochs= epochs,\n",
    "        batch_size= batch_size,\n",
    "        validation_split= 0.25\n",
    "    )\n",
    "    hist= pd.DataFrame(history.history) #se guardan los valores de errores y metricas en un diccionario\n",
    "    hist['epoch'] = history.epoch #los epochs se deben añadir aparte\n",
    "    return hist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizaciones\n",
    "### Pasos para visualizar los filtros:\n",
    "##### 1. Iterar por todas las capas del modelo usando model.layers\n",
    "##### 2. Si la capa actual es de convolucion se extraen los pesos y sesgos usando get_weights()\n",
    "##### 3. Se normalizan los pesos de los filtros entre 0 y 1\n",
    "##### 4. Se plotean los filtros para cada capa convolutional y todos los canales de color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_curves(history):\n",
    "    hist = history\n",
    "    labels = {\"loss\":\"Training Loss\", \"val_loss\":\"Validation Loss\"}\n",
    "    hist.rename(columns = labels, inplace = True)\n",
    "    \n",
    "    fig = px.line(hist, x='epoch', y=['Training Loss', 'Validation Loss'],\n",
    "                title='Gráficas de Pérdida de Entrenamiento y Evaluación',\n",
    "                labels={\"epoch\": \"Epoch\", \"value\":\"Binary Cross Entropy\", \"variable\":\"Curvas de Pérdida\"},\n",
    "                color_discrete_map={ # replaces default color mapping by value\n",
    "                \"Training Loss\": \"#46039f\", \"Validation Loss\": \"#fb9f3a\"})\n",
    "    fig.update_layout(template='plotly_white')\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se corren las funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dave4\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\backend.py:5703: UserWarning:\n",
      "\n",
      "\"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 1s 3ms/step - loss: 0.5826 - accuracy: 0.3163 - val_loss: 0.5127 - val_accuracy: 0.2727\n",
      "Epoch 2/250\n",
      "30/98 [========>.....................] - ETA: 0s - loss: 0.5409 - accuracy: 0.2000    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dave4\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\backend.py:5703: UserWarning:\n",
      "\n",
      "\"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 0s 2ms/step - loss: 0.5120 - accuracy: 0.3061 - val_loss: 0.5258 - val_accuracy: 0.2727\n",
      "Epoch 3/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.2959 - val_loss: 0.4921 - val_accuracy: 0.2727\n",
      "Epoch 4/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.5006 - accuracy: 0.3061 - val_loss: 0.4989 - val_accuracy: 0.2727\n",
      "Epoch 5/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4960 - accuracy: 0.3061 - val_loss: 0.4941 - val_accuracy: 0.2727\n",
      "Epoch 6/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4965 - accuracy: 0.3061 - val_loss: 0.4960 - val_accuracy: 0.2727\n",
      "Epoch 7/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4936 - accuracy: 0.3163 - val_loss: 0.4946 - val_accuracy: 0.2727\n",
      "Epoch 8/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.5026 - accuracy: 0.3367 - val_loss: 0.4913 - val_accuracy: 0.2727\n",
      "Epoch 9/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.3163 - val_loss: 0.4893 - val_accuracy: 0.2727\n",
      "Epoch 10/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4929 - accuracy: 0.3061 - val_loss: 0.4939 - val_accuracy: 0.2727\n",
      "Epoch 11/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4890 - accuracy: 0.3469 - val_loss: 0.4885 - val_accuracy: 0.2727\n",
      "Epoch 12/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.2857 - val_loss: 0.4977 - val_accuracy: 0.2727\n",
      "Epoch 13/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3061 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 14/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4838 - accuracy: 0.3367 - val_loss: 0.4809 - val_accuracy: 0.2727\n",
      "Epoch 15/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4701 - accuracy: 0.3571 - val_loss: 0.4709 - val_accuracy: 0.3030\n",
      "Epoch 16/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4511 - accuracy: 0.3878 - val_loss: 0.4877 - val_accuracy: 0.3333\n",
      "Epoch 17/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4258 - accuracy: 0.4082 - val_loss: 0.4586 - val_accuracy: 0.3030\n",
      "Epoch 18/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4231 - accuracy: 0.3980 - val_loss: 0.4382 - val_accuracy: 0.3333\n",
      "Epoch 19/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4066 - accuracy: 0.4082 - val_loss: 0.4362 - val_accuracy: 0.3333\n",
      "Epoch 20/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4079 - accuracy: 0.4592 - val_loss: 0.4441 - val_accuracy: 0.3333\n",
      "Epoch 21/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4127 - accuracy: 0.4286 - val_loss: 0.4365 - val_accuracy: 0.3333\n",
      "Epoch 22/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4045 - accuracy: 0.4184 - val_loss: 0.4516 - val_accuracy: 0.3333\n",
      "Epoch 23/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4063 - accuracy: 0.4286 - val_loss: 0.4362 - val_accuracy: 0.3333\n",
      "Epoch 24/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4057 - accuracy: 0.4082 - val_loss: 0.4357 - val_accuracy: 0.3333\n",
      "Epoch 25/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4000 - accuracy: 0.4592 - val_loss: 0.4330 - val_accuracy: 0.3333\n",
      "Epoch 26/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3997 - accuracy: 0.4592 - val_loss: 0.4503 - val_accuracy: 0.3333\n",
      "Epoch 27/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4001 - accuracy: 0.4490 - val_loss: 0.4341 - val_accuracy: 0.3333\n",
      "Epoch 28/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4009 - accuracy: 0.4490 - val_loss: 0.4805 - val_accuracy: 0.3333\n",
      "Epoch 29/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4000 - accuracy: 0.4388 - val_loss: 0.4574 - val_accuracy: 0.3333\n",
      "Epoch 30/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3961 - accuracy: 0.4388 - val_loss: 0.4308 - val_accuracy: 0.3333\n",
      "Epoch 31/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4022 - accuracy: 0.4592 - val_loss: 0.4314 - val_accuracy: 0.3333\n",
      "Epoch 32/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3986 - accuracy: 0.4490 - val_loss: 0.4265 - val_accuracy: 0.3333\n",
      "Epoch 33/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.4388 - val_loss: 0.4670 - val_accuracy: 0.3333\n",
      "Epoch 34/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3999 - accuracy: 0.4694 - val_loss: 0.4301 - val_accuracy: 0.3333\n",
      "Epoch 35/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4006 - accuracy: 0.4286 - val_loss: 0.4385 - val_accuracy: 0.3333\n",
      "Epoch 36/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4012 - accuracy: 0.4490 - val_loss: 0.4435 - val_accuracy: 0.3333\n",
      "Epoch 37/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4032 - accuracy: 0.4184 - val_loss: 0.4815 - val_accuracy: 0.3333\n",
      "Epoch 38/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4005 - accuracy: 0.4490 - val_loss: 0.4237 - val_accuracy: 0.3333\n",
      "Epoch 39/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3993 - accuracy: 0.4592 - val_loss: 0.4258 - val_accuracy: 0.3333\n",
      "Epoch 40/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4046 - accuracy: 0.4388 - val_loss: 0.4281 - val_accuracy: 0.3333\n",
      "Epoch 41/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4028 - accuracy: 0.4592 - val_loss: 0.4264 - val_accuracy: 0.3333\n",
      "Epoch 42/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4080 - accuracy: 0.4388 - val_loss: 0.4258 - val_accuracy: 0.3333\n",
      "Epoch 43/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3959 - accuracy: 0.4490 - val_loss: 0.4235 - val_accuracy: 0.3333\n",
      "Epoch 44/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3948 - accuracy: 0.4490 - val_loss: 0.4254 - val_accuracy: 0.3333\n",
      "Epoch 45/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3974 - accuracy: 0.4490 - val_loss: 0.4252 - val_accuracy: 0.3333\n",
      "Epoch 46/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3913 - accuracy: 0.4490 - val_loss: 0.4232 - val_accuracy: 0.3333\n",
      "Epoch 47/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3941 - accuracy: 0.4286 - val_loss: 0.4228 - val_accuracy: 0.3333\n",
      "Epoch 48/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3903 - accuracy: 0.4490 - val_loss: 0.4466 - val_accuracy: 0.3333\n",
      "Epoch 49/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3869 - accuracy: 0.4490 - val_loss: 0.4204 - val_accuracy: 0.3333\n",
      "Epoch 50/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3922 - accuracy: 0.4490 - val_loss: 0.4357 - val_accuracy: 0.3333\n",
      "Epoch 51/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3872 - accuracy: 0.4592 - val_loss: 0.4128 - val_accuracy: 0.3333\n",
      "Epoch 52/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3850 - accuracy: 0.4490 - val_loss: 0.4259 - val_accuracy: 0.3333\n",
      "Epoch 53/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4025 - accuracy: 0.4286 - val_loss: 0.4111 - val_accuracy: 0.3333\n",
      "Epoch 54/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3878 - accuracy: 0.4388 - val_loss: 0.4147 - val_accuracy: 0.3333\n",
      "Epoch 55/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3783 - accuracy: 0.4592 - val_loss: 0.4400 - val_accuracy: 0.4848\n",
      "Epoch 56/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3847 - accuracy: 0.5000 - val_loss: 0.4176 - val_accuracy: 0.3333\n",
      "Epoch 57/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3820 - accuracy: 0.4388 - val_loss: 0.4066 - val_accuracy: 0.3333\n",
      "Epoch 58/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3765 - accuracy: 0.4898 - val_loss: 0.4041 - val_accuracy: 0.3939\n",
      "Epoch 59/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3849 - accuracy: 0.4388 - val_loss: 0.4140 - val_accuracy: 0.3939\n",
      "Epoch 60/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3782 - accuracy: 0.4592 - val_loss: 0.4082 - val_accuracy: 0.3939\n",
      "Epoch 61/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3680 - accuracy: 0.4796 - val_loss: 0.3975 - val_accuracy: 0.3939\n",
      "Epoch 62/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3828 - accuracy: 0.5000 - val_loss: 0.4021 - val_accuracy: 0.3636\n",
      "Epoch 63/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3814 - accuracy: 0.4592 - val_loss: 0.4024 - val_accuracy: 0.3333\n",
      "Epoch 64/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3740 - accuracy: 0.4796 - val_loss: 0.3933 - val_accuracy: 0.3939\n",
      "Epoch 65/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3813 - accuracy: 0.4694 - val_loss: 0.4275 - val_accuracy: 0.3333\n",
      "Epoch 66/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3625 - accuracy: 0.5000 - val_loss: 0.4920 - val_accuracy: 0.2121\n",
      "Epoch 67/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3675 - accuracy: 0.5000 - val_loss: 0.3769 - val_accuracy: 0.3939\n",
      "Epoch 68/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3642 - accuracy: 0.4796 - val_loss: 0.3722 - val_accuracy: 0.3939\n",
      "Epoch 69/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3725 - accuracy: 0.4898 - val_loss: 0.3831 - val_accuracy: 0.3939\n",
      "Epoch 70/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3630 - accuracy: 0.4490 - val_loss: 0.3860 - val_accuracy: 0.4242\n",
      "Epoch 71/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3788 - accuracy: 0.4592 - val_loss: 0.3976 - val_accuracy: 0.3939\n",
      "Epoch 72/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3728 - accuracy: 0.4592 - val_loss: 0.3883 - val_accuracy: 0.3939\n",
      "Epoch 73/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3638 - accuracy: 0.5306 - val_loss: 0.3948 - val_accuracy: 0.4242\n",
      "Epoch 74/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3776 - accuracy: 0.4490 - val_loss: 0.3822 - val_accuracy: 0.5758\n",
      "Epoch 75/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3525 - accuracy: 0.4796 - val_loss: 0.3494 - val_accuracy: 0.4545\n",
      "Epoch 76/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3633 - accuracy: 0.4694 - val_loss: 0.3893 - val_accuracy: 0.4545\n",
      "Epoch 77/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3738 - accuracy: 0.5102 - val_loss: 0.3909 - val_accuracy: 0.3939\n",
      "Epoch 78/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3457 - accuracy: 0.5408 - val_loss: 0.3689 - val_accuracy: 0.6364\n",
      "Epoch 79/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3419 - accuracy: 0.5204 - val_loss: 0.3374 - val_accuracy: 0.5758\n",
      "Epoch 80/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3514 - accuracy: 0.4898 - val_loss: 0.3685 - val_accuracy: 0.6667\n",
      "Epoch 81/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3459 - accuracy: 0.4898 - val_loss: 0.3386 - val_accuracy: 0.6061\n",
      "Epoch 82/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3398 - accuracy: 0.5102 - val_loss: 0.3527 - val_accuracy: 0.5758\n",
      "Epoch 83/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3384 - accuracy: 0.5204 - val_loss: 0.3236 - val_accuracy: 0.7273\n",
      "Epoch 84/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3554 - accuracy: 0.5000 - val_loss: 0.4220 - val_accuracy: 0.4545\n",
      "Epoch 85/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3580 - accuracy: 0.4796 - val_loss: 0.3305 - val_accuracy: 0.6364\n",
      "Epoch 86/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3561 - accuracy: 0.5102 - val_loss: 0.3328 - val_accuracy: 0.6970\n",
      "Epoch 87/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3280 - accuracy: 0.5102 - val_loss: 0.3460 - val_accuracy: 0.5758\n",
      "Epoch 88/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3410 - accuracy: 0.5816 - val_loss: 0.3228 - val_accuracy: 0.6970\n",
      "Epoch 89/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3358 - accuracy: 0.5510 - val_loss: 0.3349 - val_accuracy: 0.6667\n",
      "Epoch 90/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3360 - accuracy: 0.5408 - val_loss: 0.3228 - val_accuracy: 0.5758\n",
      "Epoch 91/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3208 - accuracy: 0.5612 - val_loss: 0.3169 - val_accuracy: 0.5455\n",
      "Epoch 92/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3494 - accuracy: 0.5408 - val_loss: 0.3606 - val_accuracy: 0.4848\n",
      "Epoch 93/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3628 - accuracy: 0.5306 - val_loss: 0.3395 - val_accuracy: 0.5758\n",
      "Epoch 94/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3454 - accuracy: 0.4898 - val_loss: 0.3234 - val_accuracy: 0.7576\n",
      "Epoch 95/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3249 - accuracy: 0.5816 - val_loss: 0.3230 - val_accuracy: 0.6970\n",
      "Epoch 96/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3325 - accuracy: 0.5714 - val_loss: 0.3552 - val_accuracy: 0.4545\n",
      "Epoch 97/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3478 - accuracy: 0.5918 - val_loss: 0.3404 - val_accuracy: 0.5758\n",
      "Epoch 98/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3511 - accuracy: 0.5000 - val_loss: 0.3897 - val_accuracy: 0.5455\n",
      "Epoch 99/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3605 - accuracy: 0.5612 - val_loss: 0.3306 - val_accuracy: 0.5758\n",
      "Epoch 100/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3296 - accuracy: 0.5612 - val_loss: 0.3270 - val_accuracy: 0.6667\n",
      "Epoch 101/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3170 - accuracy: 0.5918 - val_loss: 0.3032 - val_accuracy: 0.7273\n",
      "Epoch 102/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3462 - accuracy: 0.5510 - val_loss: 0.3254 - val_accuracy: 0.5758\n",
      "Epoch 103/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3278 - accuracy: 0.5408 - val_loss: 0.3103 - val_accuracy: 0.6061\n",
      "Epoch 104/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3230 - accuracy: 0.5612 - val_loss: 0.3100 - val_accuracy: 0.7273\n",
      "Epoch 105/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3251 - accuracy: 0.5204 - val_loss: 0.3246 - val_accuracy: 0.6061\n",
      "Epoch 106/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3230 - accuracy: 0.6020 - val_loss: 0.3116 - val_accuracy: 0.6667\n",
      "Epoch 107/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3170 - accuracy: 0.5510 - val_loss: 0.3135 - val_accuracy: 0.7576\n",
      "Epoch 108/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3298 - accuracy: 0.5306 - val_loss: 0.3266 - val_accuracy: 0.6364\n",
      "Epoch 109/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3267 - accuracy: 0.6020 - val_loss: 0.2981 - val_accuracy: 0.7273\n",
      "Epoch 110/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3147 - accuracy: 0.6020 - val_loss: 0.3005 - val_accuracy: 0.6364\n",
      "Epoch 111/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3166 - accuracy: 0.6122 - val_loss: 0.3330 - val_accuracy: 0.4848\n",
      "Epoch 112/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3202 - accuracy: 0.5714 - val_loss: 0.3456 - val_accuracy: 0.6667\n",
      "Epoch 113/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3125 - accuracy: 0.6020 - val_loss: 0.3370 - val_accuracy: 0.4848\n",
      "Epoch 114/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3525 - accuracy: 0.5102 - val_loss: 0.3278 - val_accuracy: 0.4848\n",
      "Epoch 115/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3663 - accuracy: 0.4592 - val_loss: 0.3515 - val_accuracy: 0.4848\n",
      "Epoch 116/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3468 - accuracy: 0.4898 - val_loss: 0.3586 - val_accuracy: 0.5152\n",
      "Epoch 117/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3380 - accuracy: 0.5816 - val_loss: 0.3206 - val_accuracy: 0.6970\n",
      "Epoch 118/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3272 - accuracy: 0.5510 - val_loss: 0.3122 - val_accuracy: 0.6970\n",
      "Epoch 119/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3310 - accuracy: 0.5102 - val_loss: 0.3081 - val_accuracy: 0.6970\n",
      "Epoch 120/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3415 - accuracy: 0.5306 - val_loss: 0.3219 - val_accuracy: 0.6061\n",
      "Epoch 121/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3292 - accuracy: 0.5510 - val_loss: 0.3111 - val_accuracy: 0.6061\n",
      "Epoch 122/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3221 - accuracy: 0.5408 - val_loss: 0.3081 - val_accuracy: 0.6667\n",
      "Epoch 123/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3162 - accuracy: 0.5816 - val_loss: 0.3019 - val_accuracy: 0.7273\n",
      "Epoch 124/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3509 - accuracy: 0.5408 - val_loss: 0.3246 - val_accuracy: 0.5455\n",
      "Epoch 125/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3529 - accuracy: 0.4592 - val_loss: 0.3316 - val_accuracy: 0.5152\n",
      "Epoch 126/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3358 - accuracy: 0.5612 - val_loss: 0.3237 - val_accuracy: 0.7273\n",
      "Epoch 127/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3484 - accuracy: 0.5102 - val_loss: 0.3348 - val_accuracy: 0.5152\n",
      "Epoch 128/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3307 - accuracy: 0.5714 - val_loss: 0.3232 - val_accuracy: 0.6364\n",
      "Epoch 129/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3368 - accuracy: 0.5510 - val_loss: 0.2938 - val_accuracy: 0.7273\n",
      "Epoch 130/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3387 - accuracy: 0.5102 - val_loss: 0.3212 - val_accuracy: 0.5758\n",
      "Epoch 131/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3287 - accuracy: 0.5714 - val_loss: 0.3195 - val_accuracy: 0.6364\n",
      "Epoch 132/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3185 - accuracy: 0.6327 - val_loss: 0.3146 - val_accuracy: 0.6970\n",
      "Epoch 133/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3282 - accuracy: 0.5204 - val_loss: 0.3000 - val_accuracy: 0.7576\n",
      "Epoch 134/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3373 - accuracy: 0.5510 - val_loss: 0.3668 - val_accuracy: 0.4848\n",
      "Epoch 135/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3354 - accuracy: 0.5408 - val_loss: 0.3063 - val_accuracy: 0.5455\n",
      "Epoch 136/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3358 - accuracy: 0.5306 - val_loss: 0.3241 - val_accuracy: 0.6061\n",
      "Epoch 137/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3191 - accuracy: 0.5408 - val_loss: 0.2991 - val_accuracy: 0.7273\n",
      "Epoch 138/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3263 - accuracy: 0.5204 - val_loss: 0.3090 - val_accuracy: 0.7576\n",
      "Epoch 139/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3465 - accuracy: 0.4898 - val_loss: 0.3023 - val_accuracy: 0.6970\n",
      "Epoch 140/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3464 - accuracy: 0.5102 - val_loss: 0.3272 - val_accuracy: 0.5152\n",
      "Epoch 141/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3319 - accuracy: 0.5816 - val_loss: 0.3219 - val_accuracy: 0.7273\n",
      "Epoch 142/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3240 - accuracy: 0.5816 - val_loss: 0.3652 - val_accuracy: 0.6667\n",
      "Epoch 143/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3242 - accuracy: 0.5408 - val_loss: 0.3266 - val_accuracy: 0.5758\n",
      "Epoch 144/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3272 - accuracy: 0.5204 - val_loss: 0.3516 - val_accuracy: 0.4848\n",
      "Epoch 145/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3354 - accuracy: 0.5714 - val_loss: 0.3101 - val_accuracy: 0.5758\n",
      "Epoch 146/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3221 - accuracy: 0.5510 - val_loss: 0.3073 - val_accuracy: 0.7273\n",
      "Epoch 147/250\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 0.3222 - accuracy: 0.5714 - val_loss: 0.3068 - val_accuracy: 0.6970\n",
      "Epoch 148/250\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 0.3291 - accuracy: 0.5306 - val_loss: 0.3611 - val_accuracy: 0.4848\n",
      "Epoch 149/250\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 0.3284 - accuracy: 0.5612 - val_loss: 0.3629 - val_accuracy: 0.4545\n",
      "Epoch 150/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3206 - accuracy: 0.5510 - val_loss: 0.3007 - val_accuracy: 0.7576\n",
      "Epoch 151/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3307 - accuracy: 0.5204 - val_loss: 0.3300 - val_accuracy: 0.5152\n",
      "Epoch 152/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3350 - accuracy: 0.5102 - val_loss: 0.3321 - val_accuracy: 0.5455\n",
      "Epoch 153/250\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 0.3230 - accuracy: 0.5000 - val_loss: 0.3029 - val_accuracy: 0.7273\n",
      "Epoch 154/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3143 - accuracy: 0.5306 - val_loss: 0.3378 - val_accuracy: 0.6667\n",
      "Epoch 155/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3294 - accuracy: 0.5306 - val_loss: 0.3352 - val_accuracy: 0.5455\n",
      "Epoch 156/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3138 - accuracy: 0.5816 - val_loss: 0.3204 - val_accuracy: 0.7273\n",
      "Epoch 157/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3108 - accuracy: 0.5918 - val_loss: 0.3032 - val_accuracy: 0.6970\n",
      "Epoch 158/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3273 - accuracy: 0.5306 - val_loss: 0.3057 - val_accuracy: 0.6667\n",
      "Epoch 159/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3100 - accuracy: 0.6020 - val_loss: 0.2968 - val_accuracy: 0.6667\n",
      "Epoch 160/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3156 - accuracy: 0.6020 - val_loss: 0.3171 - val_accuracy: 0.6061\n",
      "Epoch 161/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3156 - accuracy: 0.5816 - val_loss: 0.3114 - val_accuracy: 0.6061\n",
      "Epoch 162/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3278 - accuracy: 0.5510 - val_loss: 0.3536 - val_accuracy: 0.5152\n",
      "Epoch 163/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3356 - accuracy: 0.5510 - val_loss: 0.3925 - val_accuracy: 0.3939\n",
      "Epoch 164/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3280 - accuracy: 0.5102 - val_loss: 0.2953 - val_accuracy: 0.6061\n",
      "Epoch 165/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3202 - accuracy: 0.6224 - val_loss: 0.3064 - val_accuracy: 0.6667\n",
      "Epoch 166/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3121 - accuracy: 0.5510 - val_loss: 0.3295 - val_accuracy: 0.5758\n",
      "Epoch 167/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3190 - accuracy: 0.5816 - val_loss: 0.3054 - val_accuracy: 0.6061\n",
      "Epoch 168/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3153 - accuracy: 0.5714 - val_loss: 0.3568 - val_accuracy: 0.5455\n",
      "Epoch 169/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3354 - accuracy: 0.5612 - val_loss: 0.3775 - val_accuracy: 0.5455\n",
      "Epoch 170/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3080 - accuracy: 0.6122 - val_loss: 0.3254 - val_accuracy: 0.5152\n",
      "Epoch 171/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2858 - accuracy: 0.6837 - val_loss: 0.2920 - val_accuracy: 0.7879\n",
      "Epoch 172/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3197 - accuracy: 0.5714 - val_loss: 0.3279 - val_accuracy: 0.6364\n",
      "Epoch 173/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3230 - accuracy: 0.5612 - val_loss: 0.2922 - val_accuracy: 0.7879\n",
      "Epoch 174/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3026 - accuracy: 0.6122 - val_loss: 0.2933 - val_accuracy: 0.6061\n",
      "Epoch 175/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2897 - accuracy: 0.6633 - val_loss: 0.2971 - val_accuracy: 0.5758\n",
      "Epoch 176/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3188 - accuracy: 0.5714 - val_loss: 0.3417 - val_accuracy: 0.5455\n",
      "Epoch 177/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3315 - accuracy: 0.5102 - val_loss: 0.3879 - val_accuracy: 0.4848\n",
      "Epoch 178/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3698 - accuracy: 0.5204 - val_loss: 0.4663 - val_accuracy: 0.4545\n",
      "Epoch 179/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3571 - accuracy: 0.4796 - val_loss: 0.4321 - val_accuracy: 0.4848\n",
      "Epoch 180/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3222 - accuracy: 0.5918 - val_loss: 0.2833 - val_accuracy: 0.7273\n",
      "Epoch 181/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3398 - accuracy: 0.5408 - val_loss: 0.2950 - val_accuracy: 0.7273\n",
      "Epoch 182/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2964 - accuracy: 0.5816 - val_loss: 0.2962 - val_accuracy: 0.6364\n",
      "Epoch 183/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3021 - accuracy: 0.5918 - val_loss: 0.2857 - val_accuracy: 0.7576\n",
      "Epoch 184/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3047 - accuracy: 0.6122 - val_loss: 0.2894 - val_accuracy: 0.6061\n",
      "Epoch 185/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3029 - accuracy: 0.5918 - val_loss: 0.3054 - val_accuracy: 0.5758\n",
      "Epoch 186/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3006 - accuracy: 0.6122 - val_loss: 0.2879 - val_accuracy: 0.6364\n",
      "Epoch 187/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3254 - accuracy: 0.5306 - val_loss: 0.2911 - val_accuracy: 0.6970\n",
      "Epoch 188/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3138 - accuracy: 0.5612 - val_loss: 0.3015 - val_accuracy: 0.7273\n",
      "Epoch 189/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3149 - accuracy: 0.5510 - val_loss: 0.2929 - val_accuracy: 0.6364\n",
      "Epoch 190/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3167 - accuracy: 0.5714 - val_loss: 0.3115 - val_accuracy: 0.5152\n",
      "Epoch 191/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3244 - accuracy: 0.5408 - val_loss: 0.2948 - val_accuracy: 0.7273\n",
      "Epoch 192/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2987 - accuracy: 0.6633 - val_loss: 0.2904 - val_accuracy: 0.7576\n",
      "Epoch 193/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3037 - accuracy: 0.6122 - val_loss: 0.2881 - val_accuracy: 0.6667\n",
      "Epoch 194/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3304 - accuracy: 0.5306 - val_loss: 0.3171 - val_accuracy: 0.7273\n",
      "Epoch 195/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3075 - accuracy: 0.6122 - val_loss: 0.3584 - val_accuracy: 0.5758\n",
      "Epoch 196/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2950 - accuracy: 0.5816 - val_loss: 0.2774 - val_accuracy: 0.7576\n",
      "Epoch 197/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3166 - accuracy: 0.5204 - val_loss: 0.2940 - val_accuracy: 0.5758\n",
      "Epoch 198/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3110 - accuracy: 0.6020 - val_loss: 0.3291 - val_accuracy: 0.5455\n",
      "Epoch 199/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2973 - accuracy: 0.6224 - val_loss: 0.2863 - val_accuracy: 0.6061\n",
      "Epoch 200/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3212 - accuracy: 0.5612 - val_loss: 0.3219 - val_accuracy: 0.4848\n",
      "Epoch 201/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3108 - accuracy: 0.6224 - val_loss: 0.5411 - val_accuracy: 0.5758\n",
      "Epoch 202/250\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 0.3104 - accuracy: 0.5816 - val_loss: 0.2837 - val_accuracy: 0.6667\n",
      "Epoch 203/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3017 - accuracy: 0.6327 - val_loss: 0.2796 - val_accuracy: 0.7879\n",
      "Epoch 204/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3072 - accuracy: 0.5612 - val_loss: 0.2840 - val_accuracy: 0.7576\n",
      "Epoch 205/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3122 - accuracy: 0.5816 - val_loss: 0.2826 - val_accuracy: 0.6667\n",
      "Epoch 206/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3010 - accuracy: 0.5816 - val_loss: 0.3594 - val_accuracy: 0.6061\n",
      "Epoch 207/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3076 - accuracy: 0.6020 - val_loss: 0.3325 - val_accuracy: 0.5455\n",
      "Epoch 208/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3287 - accuracy: 0.6020 - val_loss: 0.3170 - val_accuracy: 0.6061\n",
      "Epoch 209/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2990 - accuracy: 0.6020 - val_loss: 0.2925 - val_accuracy: 0.6364\n",
      "Epoch 210/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3110 - accuracy: 0.6020 - val_loss: 0.2929 - val_accuracy: 0.6970\n",
      "Epoch 211/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2982 - accuracy: 0.6224 - val_loss: 0.2974 - val_accuracy: 0.6364\n",
      "Epoch 212/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2959 - accuracy: 0.6020 - val_loss: 0.2853 - val_accuracy: 0.7273\n",
      "Epoch 213/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3061 - accuracy: 0.5612 - val_loss: 0.3031 - val_accuracy: 0.7879\n",
      "Epoch 214/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3096 - accuracy: 0.5816 - val_loss: 0.2878 - val_accuracy: 0.6970\n",
      "Epoch 215/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2979 - accuracy: 0.6122 - val_loss: 0.2821 - val_accuracy: 0.7273\n",
      "Epoch 216/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2946 - accuracy: 0.6429 - val_loss: 0.2860 - val_accuracy: 0.7576\n",
      "Epoch 217/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3058 - accuracy: 0.5816 - val_loss: 0.2819 - val_accuracy: 0.7273\n",
      "Epoch 218/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3101 - accuracy: 0.5612 - val_loss: 0.2851 - val_accuracy: 0.7576\n",
      "Epoch 219/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2915 - accuracy: 0.6224 - val_loss: 0.2934 - val_accuracy: 0.6364\n",
      "Epoch 220/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2891 - accuracy: 0.6429 - val_loss: 0.2796 - val_accuracy: 0.6970\n",
      "Epoch 221/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2973 - accuracy: 0.6020 - val_loss: 0.3045 - val_accuracy: 0.6061\n",
      "Epoch 222/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2970 - accuracy: 0.5816 - val_loss: 0.2860 - val_accuracy: 0.7576\n",
      "Epoch 223/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3036 - accuracy: 0.5306 - val_loss: 0.2942 - val_accuracy: 0.6667\n",
      "Epoch 224/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2951 - accuracy: 0.5612 - val_loss: 0.3024 - val_accuracy: 0.7273\n",
      "Epoch 225/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2784 - accuracy: 0.6531 - val_loss: 0.3603 - val_accuracy: 0.6061\n",
      "Epoch 226/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2975 - accuracy: 0.6429 - val_loss: 0.2854 - val_accuracy: 0.7576\n",
      "Epoch 227/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3077 - accuracy: 0.5816 - val_loss: 0.4242 - val_accuracy: 0.6667\n",
      "Epoch 228/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2771 - accuracy: 0.6633 - val_loss: 0.2872 - val_accuracy: 0.6970\n",
      "Epoch 229/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2949 - accuracy: 0.6429 - val_loss: 0.3031 - val_accuracy: 0.7576\n",
      "Epoch 230/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2887 - accuracy: 0.5918 - val_loss: 0.2998 - val_accuracy: 0.7576\n",
      "Epoch 231/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3013 - accuracy: 0.6429 - val_loss: 0.2950 - val_accuracy: 0.5758\n",
      "Epoch 232/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2909 - accuracy: 0.6735 - val_loss: 0.2818 - val_accuracy: 0.6970\n",
      "Epoch 233/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2758 - accuracy: 0.6633 - val_loss: 0.2766 - val_accuracy: 0.7576\n",
      "Epoch 234/250\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 0.2786 - accuracy: 0.7347 - val_loss: 0.2895 - val_accuracy: 0.6061\n",
      "Epoch 235/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2772 - accuracy: 0.6531 - val_loss: 0.3113 - val_accuracy: 0.5758\n",
      "Epoch 236/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3036 - accuracy: 0.5816 - val_loss: 0.3097 - val_accuracy: 0.5758\n",
      "Epoch 237/250\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 0.2895 - accuracy: 0.6224 - val_loss: 0.2804 - val_accuracy: 0.7273\n",
      "Epoch 238/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2857 - accuracy: 0.5918 - val_loss: 0.2794 - val_accuracy: 0.7576\n",
      "Epoch 239/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2807 - accuracy: 0.5816 - val_loss: 0.2861 - val_accuracy: 0.6364\n",
      "Epoch 240/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.3052 - accuracy: 0.6122 - val_loss: 0.2875 - val_accuracy: 0.6364\n",
      "Epoch 241/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2895 - accuracy: 0.6020 - val_loss: 0.2841 - val_accuracy: 0.6364\n",
      "Epoch 242/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2700 - accuracy: 0.6633 - val_loss: 0.2812 - val_accuracy: 0.6364\n",
      "Epoch 243/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2952 - accuracy: 0.6429 - val_loss: 0.2925 - val_accuracy: 0.6061\n",
      "Epoch 244/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2815 - accuracy: 0.6735 - val_loss: 0.2883 - val_accuracy: 0.6061\n",
      "Epoch 245/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2789 - accuracy: 0.6633 - val_loss: 0.2861 - val_accuracy: 0.6970\n",
      "Epoch 246/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2768 - accuracy: 0.6429 - val_loss: 0.3404 - val_accuracy: 0.6667\n",
      "Epoch 247/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2778 - accuracy: 0.6429 - val_loss: 0.2870 - val_accuracy: 0.6364\n",
      "Epoch 248/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2989 - accuracy: 0.6122 - val_loss: 0.2818 - val_accuracy: 0.6364\n",
      "Epoch 249/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2823 - accuracy: 0.6837 - val_loss: 0.2803 - val_accuracy: 0.6970\n",
      "Epoch 250/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.2772 - accuracy: 0.6429 - val_loss: 0.2788 - val_accuracy: 0.6364\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.583</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.512</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.509</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.273</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.501</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.273</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.496</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.273</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.277</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.667</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.278</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.636</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.299</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.636</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.282</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.697</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.277</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.636</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     loss  accuracy  val_loss  val_accuracy  epoch\n",
       "0   0.583     0.316     0.513         0.273      0\n",
       "1   0.512     0.306     0.526         0.273      1\n",
       "2   0.509     0.296     0.492         0.273      2\n",
       "3   0.501     0.306     0.499         0.273      3\n",
       "4   0.496     0.306     0.494         0.273      4\n",
       "..    ...       ...       ...           ...    ...\n",
       "245 0.277     0.643     0.340         0.667    245\n",
       "246 0.278     0.643     0.287         0.636    246\n",
       "247 0.299     0.612     0.282         0.636    247\n",
       "248 0.282     0.684     0.280         0.697    248\n",
       "249 0.277     0.643     0.279         0.636    249\n",
       "\n",
       "[250 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Curvas de Pérdida=Training Loss<br>Epoch=%{x}<br>Binary Cross Entropy=%{y}<extra></extra>",
         "legendgroup": "Training Loss",
         "line": {
          "color": "#46039f",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Training Loss",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249
         ],
         "xaxis": "x",
         "y": [
          0.5826265215873718,
          0.5119792222976685,
          0.5091506242752075,
          0.5005943179130554,
          0.49600300192832947,
          0.49652811884880066,
          0.4936380088329315,
          0.5025888681411743,
          0.4883160889148712,
          0.49286919832229614,
          0.48895418643951416,
          0.4872709810733795,
          0.48777392506599426,
          0.48384350538253784,
          0.47010892629623413,
          0.45112112164497375,
          0.4257816672325134,
          0.4230821430683136,
          0.40664467215538025,
          0.40787896513938904,
          0.4127189517021179,
          0.4044751524925232,
          0.40628284215927124,
          0.4057205617427826,
          0.3999578654766083,
          0.3996691107749939,
          0.4000765383243561,
          0.4008866846561432,
          0.4000084400177002,
          0.39609548449516296,
          0.40218132734298706,
          0.39859631657600403,
          0.3967461585998535,
          0.3998594880104065,
          0.40060752630233765,
          0.40122145414352417,
          0.4031809866428375,
          0.400475412607193,
          0.3993285894393921,
          0.40459150075912476,
          0.40283700823783875,
          0.40802761912345886,
          0.3959142863750458,
          0.3947541415691376,
          0.39736756682395935,
          0.39133796095848083,
          0.3941035270690918,
          0.3902740180492401,
          0.3869186341762543,
          0.39221009612083435,
          0.3871747553348541,
          0.3849771022796631,
          0.40254250168800354,
          0.38775911927223206,
          0.3783195912837982,
          0.38467445969581604,
          0.3819575905799866,
          0.3764531910419464,
          0.384949654340744,
          0.3782121241092682,
          0.368032842874527,
          0.3827994465827942,
          0.3813877999782562,
          0.37404075264930725,
          0.3812507390975952,
          0.3624557852745056,
          0.3675011992454529,
          0.3641919493675232,
          0.37245994806289673,
          0.3630213439464569,
          0.3787660300731659,
          0.37279292941093445,
          0.36381351947784424,
          0.3775959014892578,
          0.3525155484676361,
          0.3633074462413788,
          0.3737519383430481,
          0.34570348262786865,
          0.3419221043586731,
          0.35142046213150024,
          0.3459370732307434,
          0.3398168087005615,
          0.3384277820587158,
          0.35543492436408997,
          0.357977032661438,
          0.3560543358325958,
          0.32796338200569153,
          0.3409932255744934,
          0.3357667326927185,
          0.3359684348106384,
          0.32082730531692505,
          0.3493863046169281,
          0.3628028929233551,
          0.34544411301612854,
          0.32486921548843384,
          0.3325238525867462,
          0.347797155380249,
          0.3510940372943878,
          0.3605077266693115,
          0.3295692503452301,
          0.3169611990451813,
          0.34622693061828613,
          0.3277638554573059,
          0.3230195641517639,
          0.3251441419124603,
          0.32304447889328003,
          0.31696245074272156,
          0.3297845423221588,
          0.3266946077346802,
          0.31473982334136963,
          0.3166292607784271,
          0.32023465633392334,
          0.3124943971633911,
          0.35246217250823975,
          0.36634504795074463,
          0.34679314494132996,
          0.33795663714408875,
          0.32718968391418457,
          0.3309704065322876,
          0.3415009379386902,
          0.3292424976825714,
          0.32211482524871826,
          0.3161821961402893,
          0.3508598506450653,
          0.35290005803108215,
          0.33578529953956604,
          0.3483809530735016,
          0.33067864179611206,
          0.3367900848388672,
          0.3386636972427368,
          0.3286604881286621,
          0.31854116916656494,
          0.3282371163368225,
          0.3373236358165741,
          0.33537885546684265,
          0.3357515037059784,
          0.319051057100296,
          0.3263213634490967,
          0.34650737047195435,
          0.3463827967643738,
          0.3318755030632019,
          0.3240354657173157,
          0.3242338001728058,
          0.32722926139831543,
          0.33542585372924805,
          0.32206907868385315,
          0.32222679257392883,
          0.32910221815109253,
          0.32840827107429504,
          0.3205558955669403,
          0.33072179555892944,
          0.33498615026474,
          0.32296255230903625,
          0.31430134177207947,
          0.32940176129341125,
          0.3137607276439667,
          0.3108113408088684,
          0.3272739350795746,
          0.3100353479385376,
          0.3155902624130249,
          0.31564784049987793,
          0.3278091549873352,
          0.33557653427124023,
          0.3279724419116974,
          0.3202361762523651,
          0.3120744228363037,
          0.31901228427886963,
          0.3153119683265686,
          0.3354071378707886,
          0.307994544506073,
          0.28575336933135986,
          0.3197268843650818,
          0.3229829668998718,
          0.3026215434074402,
          0.2897200882434845,
          0.31881898641586304,
          0.331527978181839,
          0.36979368329048157,
          0.35714954137802124,
          0.32218173146247864,
          0.3397614657878876,
          0.29644301533699036,
          0.3020651638507843,
          0.30469265580177307,
          0.30293187499046326,
          0.3005601763725281,
          0.325394868850708,
          0.313814252614975,
          0.3148908019065857,
          0.31665152311325073,
          0.3244398534297943,
          0.2986889183521271,
          0.30369874835014343,
          0.33035796880722046,
          0.3075281083583832,
          0.2950291931629181,
          0.31663256883621216,
          0.31095102429389954,
          0.29728296399116516,
          0.32122430205345154,
          0.31075137853622437,
          0.3103759288787842,
          0.3016888201236725,
          0.30724817514419556,
          0.3122439682483673,
          0.3010013997554779,
          0.30759674310684204,
          0.3286603093147278,
          0.29896998405456543,
          0.3110053837299347,
          0.298247367143631,
          0.29587745666503906,
          0.3060632646083832,
          0.30955204367637634,
          0.29785043001174927,
          0.2945517897605896,
          0.3058306872844696,
          0.3101462721824646,
          0.2914670407772064,
          0.2890845537185669,
          0.29728370904922485,
          0.29701218008995056,
          0.3036358058452606,
          0.2950776219367981,
          0.278363436460495,
          0.29750731587409973,
          0.3076511323451996,
          0.27710843086242676,
          0.2949348986148834,
          0.28871893882751465,
          0.30134427547454834,
          0.2909322679042816,
          0.2758316099643707,
          0.27858152985572815,
          0.27722129225730896,
          0.3035798668861389,
          0.2895432114601135,
          0.28567150235176086,
          0.2807143032550812,
          0.30523940920829773,
          0.2894805073738098,
          0.26996180415153503,
          0.29519471526145935,
          0.28154101967811584,
          0.2789028286933899,
          0.27675312757492065,
          0.27775654196739197,
          0.29893413186073303,
          0.2822669744491577,
          0.2772218883037567
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Curvas de Pérdida=Validation Loss<br>Epoch=%{x}<br>Binary Cross Entropy=%{y}<extra></extra>",
         "legendgroup": "Validation Loss",
         "line": {
          "color": "#fb9f3a",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Validation Loss",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249
         ],
         "xaxis": "x",
         "y": [
          0.5127098560333252,
          0.5257666110992432,
          0.4921376705169678,
          0.4988880753517151,
          0.4941464066505432,
          0.4960332214832306,
          0.49463751912117004,
          0.4912889003753662,
          0.489253431558609,
          0.4939194321632385,
          0.48848360776901245,
          0.49772632122039795,
          0.4928017854690552,
          0.4809444844722748,
          0.47087371349334717,
          0.4877091646194458,
          0.45857781171798706,
          0.43822675943374634,
          0.43615972995758057,
          0.44411706924438477,
          0.4365091025829315,
          0.451577365398407,
          0.436185359954834,
          0.43567243218421936,
          0.4330310523509979,
          0.4503178596496582,
          0.43412986397743225,
          0.48053455352783203,
          0.4573554992675781,
          0.43081560730934143,
          0.43139904737472534,
          0.42649948596954346,
          0.4669761061668396,
          0.4301100969314575,
          0.4384908974170685,
          0.4435277283191681,
          0.48145657777786255,
          0.4236901104450226,
          0.425846129655838,
          0.428107351064682,
          0.426405131816864,
          0.42577236890792847,
          0.4235478937625885,
          0.42539143562316895,
          0.4251648485660553,
          0.42317742109298706,
          0.42282307147979736,
          0.44657084345817566,
          0.42036154866218567,
          0.43570268154144287,
          0.4128111004829407,
          0.42585182189941406,
          0.41114646196365356,
          0.4147467017173767,
          0.4399704039096832,
          0.417638897895813,
          0.4065706729888916,
          0.40414243936538696,
          0.4139561653137207,
          0.4081695079803467,
          0.397514283657074,
          0.4021412134170532,
          0.40237101912498474,
          0.3932710886001587,
          0.4275456666946411,
          0.4920254945755005,
          0.3768792152404785,
          0.3722376227378845,
          0.38309526443481445,
          0.38604599237442017,
          0.3975600302219391,
          0.3882829248905182,
          0.3947969675064087,
          0.3822421729564667,
          0.349396288394928,
          0.3893188536167145,
          0.3909185230731964,
          0.36892855167388916,
          0.3373946249485016,
          0.3684903681278229,
          0.3386411666870117,
          0.35267534852027893,
          0.32358330488204956,
          0.4220377802848816,
          0.33050572872161865,
          0.33275970816612244,
          0.34601834416389465,
          0.322818398475647,
          0.33485013246536255,
          0.32281494140625,
          0.31688401103019714,
          0.360597163438797,
          0.3394771218299866,
          0.3234453499317169,
          0.3230469822883606,
          0.3552420139312744,
          0.340440571308136,
          0.38967645168304443,
          0.33057400584220886,
          0.3269931375980377,
          0.3032080829143524,
          0.3253765106201172,
          0.31028255820274353,
          0.31000885367393494,
          0.32464709877967834,
          0.31156110763549805,
          0.31348544359207153,
          0.3265947699546814,
          0.2981290817260742,
          0.3005020022392273,
          0.3330177962779999,
          0.34557849168777466,
          0.3370095491409302,
          0.32780155539512634,
          0.35154974460601807,
          0.3586297631263733,
          0.3205951452255249,
          0.31215062737464905,
          0.3081422746181488,
          0.3219333291053772,
          0.311062753200531,
          0.3080623745918274,
          0.30189603567123413,
          0.3246074616909027,
          0.33155226707458496,
          0.32367703318595886,
          0.3347659111022949,
          0.3231562077999115,
          0.2938271760940552,
          0.3212239444255829,
          0.31945088505744934,
          0.31457874178886414,
          0.30001482367515564,
          0.3668002486228943,
          0.3063380718231201,
          0.32412809133529663,
          0.29907962679862976,
          0.30900999903678894,
          0.30228397250175476,
          0.3271942734718323,
          0.3219243288040161,
          0.3651934266090393,
          0.32663694024086,
          0.3515647351741791,
          0.31011244654655457,
          0.3072713315486908,
          0.3067566156387329,
          0.3610663115978241,
          0.36291763186454773,
          0.30068737268447876,
          0.32996678352355957,
          0.33211708068847656,
          0.30292725563049316,
          0.33778515458106995,
          0.3352304697036743,
          0.32039088010787964,
          0.30323538184165955,
          0.30568552017211914,
          0.2968296408653259,
          0.31711816787719727,
          0.3114239275455475,
          0.3535814583301544,
          0.39252766966819763,
          0.29530686140060425,
          0.3063504993915558,
          0.329470157623291,
          0.30543994903564453,
          0.3568477928638458,
          0.3775401711463928,
          0.325378954410553,
          0.2919832468032837,
          0.32788336277008057,
          0.29218047857284546,
          0.2932523787021637,
          0.29707637429237366,
          0.3416532874107361,
          0.38785985112190247,
          0.4663272202014923,
          0.43214890360832214,
          0.283264696598053,
          0.2950116693973541,
          0.2961729168891907,
          0.285745233297348,
          0.2893662452697754,
          0.3054455816745758,
          0.28789883852005005,
          0.29111823439598083,
          0.30146345496177673,
          0.29289498925209045,
          0.3115035593509674,
          0.2947700619697571,
          0.2904301881790161,
          0.28813430666923523,
          0.31709614396095276,
          0.35844123363494873,
          0.27737191319465637,
          0.2939797043800354,
          0.32913798093795776,
          0.2862837314605713,
          0.3218628168106079,
          0.5410553216934204,
          0.2837314307689667,
          0.2796320915222168,
          0.2840441167354584,
          0.282572478055954,
          0.3593864440917969,
          0.33253708481788635,
          0.31700563430786133,
          0.2924853265285492,
          0.2928825914859772,
          0.29743221402168274,
          0.28532588481903076,
          0.3031065762042999,
          0.2878013551235199,
          0.2820996642112732,
          0.2860187888145447,
          0.2819259464740753,
          0.2850595712661743,
          0.2934345006942749,
          0.2795625627040863,
          0.3044862449169159,
          0.2860426902770996,
          0.29419267177581787,
          0.3024155795574188,
          0.3602711856365204,
          0.2853982448577881,
          0.4242042005062103,
          0.2872181534767151,
          0.3031125068664551,
          0.29976627230644226,
          0.29502907395362854,
          0.2818193733692169,
          0.27660658955574036,
          0.2895449995994568,
          0.31128424406051636,
          0.3096872866153717,
          0.2803604006767273,
          0.27936503291130066,
          0.286066472530365,
          0.2875126600265503,
          0.2841399312019348,
          0.28119009733200073,
          0.29250288009643555,
          0.288301020860672,
          0.2861368656158447,
          0.3403908312320709,
          0.2870400547981262,
          0.2817991077899933,
          0.2802770137786865,
          0.27876636385917664
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "Curvas de Pérdida"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Gráficas de Pérdida de Entrenamiento y Evaluación"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Binary Cross Entropy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.583</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.512</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.509</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.273</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.501</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.273</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.496</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.273</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.277</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.667</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.278</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.636</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.299</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.636</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.282</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.697</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.277</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.636</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Training Loss  accuracy  Validation Loss  val_accuracy  epoch\n",
       "0            0.583     0.316            0.513         0.273      0\n",
       "1            0.512     0.306            0.526         0.273      1\n",
       "2            0.509     0.296            0.492         0.273      2\n",
       "3            0.501     0.306            0.499         0.273      3\n",
       "4            0.496     0.306            0.494         0.273      4\n",
       "..             ...       ...              ...           ...    ...\n",
       "245          0.277     0.643            0.340         0.667    245\n",
       "246          0.278     0.643            0.287         0.636    246\n",
       "247          0.299     0.612            0.282         0.636    247\n",
       "248          0.282     0.684            0.280         0.697    248\n",
       "249          0.277     0.643            0.279         0.636    249\n",
       "\n",
       "[250 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 250\n",
    "batch_size = 1\n",
    "model = my_model(learning_rate)\n",
    "history= train_model(model, train_features, train_labels, epochs, batch_size)\n",
    "display(history)\n",
    "loss_curves(history)\n",
    "#view_filters(model)\n",
    "display(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000016FE40DEEF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normal</th>\n",
       "      <th>collision_in_tool</th>\n",
       "      <th>collision_in_part</th>\n",
       "      <th>bottom_collision</th>\n",
       "      <th>bottom_obstruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.043</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.713</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.385</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.368</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.069</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.700</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    normal  collision_in_tool  collision_in_part  bottom_collision  \\\n",
       "0    0.043              0.088              0.505             0.302   \n",
       "1    0.000              0.000              0.000             0.000   \n",
       "2    0.000              0.000              0.000             0.991   \n",
       "3    0.713              0.149              0.135             0.001   \n",
       "4    0.385              0.216              0.389             0.009   \n",
       "..     ...                ...                ...               ...   \n",
       "28   0.000              0.000              0.009             0.972   \n",
       "29   0.368              0.219              0.395             0.014   \n",
       "30   0.000              0.001              0.033             0.930   \n",
       "31   0.069              0.135              0.552             0.165   \n",
       "32   0.700              0.152              0.146             0.002   \n",
       "\n",
       "    bottom_obstruction  \n",
       "0                0.062  \n",
       "1                1.000  \n",
       "2                0.009  \n",
       "3                0.000  \n",
       "4                0.001  \n",
       "..                 ...  \n",
       "28               0.019  \n",
       "29               0.003  \n",
       "30               0.037  \n",
       "31               0.079  \n",
       "32               0.000  \n",
       "\n",
       "[33 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1  2  3  4  5\n",
       "135  0  0  0  1  0\n",
       "115  0  0  0  0  1\n",
       "131  0  0  0  0  1\n",
       "55   1  0  0  0  0\n",
       "95   0  1  0  0  0\n",
       "..  .. .. .. .. ..\n",
       "134  0  0  0  1  0\n",
       "160  0  1  0  0  0\n",
       "139  0  0  0  1  0\n",
       "78   0  0  1  0  0\n",
       "60   1  0  0  0  0\n",
       "\n",
       "[33 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = model.predict(test_features)\n",
    "predictions_df = pd.DataFrame(predictions, columns=['normal', 'collision_in_tool', 'collision_in_part', 'bottom_collision', 'bottom_obstruction'])\n",
    "display(predictions_df)\n",
    "display(test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
