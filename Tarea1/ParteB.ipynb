{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar Modulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import plotly.express as px\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>normal</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>collision_in_tool</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                labels 1  2  3  4  5 \n",
       "0               normal               \n",
       "1               normal               \n",
       "2               normal               \n",
       "3               normal               \n",
       "4               normal               \n",
       "..                 ... .. .. .. .. ..\n",
       "159  collision_in_tool               \n",
       "160  collision_in_tool               \n",
       "161  collision_in_tool               \n",
       "162  collision_in_tool               \n",
       "163  collision_in_tool               \n",
       "\n",
       "[164 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ -2.,  -1.,  81.,   0.,  -5.,   0.],\n",
       "        [ -2.,  -1.,  79.,   0.,  -4.,   0.],\n",
       "        [ -2.,  -1.,  79.,   0.,  -4.,   0.],\n",
       "        ...,\n",
       "        [ -2.,  -1.,  78.,   0.,  -5.,   0.],\n",
       "        [ -3.,  -1.,  80.,   1.,  -4.,   1.],\n",
       "        [ -2.,  -1.,  79.,   0.,  -4.,   0.]],\n",
       "\n",
       "       [[  6.,  -1.,  79.,  -2.,   4.,  -3.],\n",
       "        [ 42.,  -3.,  80.,   5.,  53.,   3.],\n",
       "        [ -5.,   4.,  74., -15., -10.,  -1.],\n",
       "        ...,\n",
       "        [ -1.,  -5.,  80.,   6.,  -6.,   0.],\n",
       "        [ -4.,   5.,  78., -14.,  -9.,  -4.],\n",
       "        [ -4.,   1.,  80.,  -3., -12.,   5.]],\n",
       "\n",
       "       [[ -2.,  -6.,  85.,  14.,  -5.,   2.],\n",
       "        [  0.,   2.,  74.,  -7.,   1.,   0.],\n",
       "        [ -4.,  -5.,  76.,   7., -11.,   4.],\n",
       "        ...,\n",
       "        [  0.,  -9.,  87.,  13.,  -5.,   2.],\n",
       "        [ -5.,   5.,  67., -17., -16.,   7.],\n",
       "        [ -6., -10.,  86.,  16., -14.,  -1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-12.,  17.,   3., -19., -10.,  -4.],\n",
       "        [-12.,  12.,  11., -13., -16.,  -4.],\n",
       "        [ -8.,   3.,   6.,   2., -11.,  -4.],\n",
       "        ...,\n",
       "        [  0.,   1.,   3.,   1.,   1.,  -3.],\n",
       "        [  1.,   2.,   9.,   0.,   3.,  -4.],\n",
       "        [  1.,   2.,   9.,   0.,   3.,  -4.]],\n",
       "\n",
       "       [[-41.,  21.,  -5., -23., -59.,  -4.],\n",
       "        [-32.,  21.,  -6., -25., -45.,  -4.],\n",
       "        [-21.,  12.,  -6., -14., -31.,  -4.],\n",
       "        ...,\n",
       "        [ -4.,   4.,   3.,   0.,  -1.,  -3.],\n",
       "        [ -4.,   3.,   2.,   2.,  -3.,  -3.],\n",
       "        [ -2.,   3.,   5.,   0.,  -2.,  -3.]],\n",
       "\n",
       "       [[  9., -10., -11.,  17.,   7.,  -4.],\n",
       "        [  5.,   0.,   4.,   0.,   7.,  -4.],\n",
       "        [ -3.,   6.,  -2.,  -8.,  -8.,  -4.],\n",
       "        ...,\n",
       "        [ -1.,   1.,  -3.,  -3.,  -2.,  -3.],\n",
       "        [  0.,  -1.,  -5.,  -1.,   1.,  -3.],\n",
       "        [ -1.,   1.,   4.,   0.,  -1.,  -3.]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.3f}\".format\n",
    "\n",
    "data = np.loadtxt(\"lp5.csv\", delimiter=\",\", dtype=str)\n",
    "data_copy = data #va de 0 a 2623 o sea 2624 datos\n",
    "cont = 16\n",
    "labels = []\n",
    "numbers = []\n",
    "features= np.zeros((164,15,6))\n",
    "\n",
    "for i in range(len(data_copy)):\n",
    "    if(cont == 16):\n",
    "        labels.append(data_copy[i])\n",
    "    if(cont<16):\n",
    "        numbers.append(data_copy[i])\n",
    "    cont -= 1\n",
    "    if(cont == 0):\n",
    "        cont = 16\n",
    "        \n",
    "cont = 0\n",
    "for i in range(164):\n",
    "    for j in range(15):\n",
    "        for z in range(6):\n",
    "            features[i][j][z] = numbers[cont][z]\n",
    "        cont += 1\n",
    "        \n",
    "labels_df = pd.DataFrame(labels, columns=['labels','1','2','3','4','5'])\n",
    "#labels_df = labels_df.loc[:,['labels']]\n",
    "display(labels_df)\n",
    "display(features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manejo de Datos y Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dave4\\AppData\\Local\\Temp\\ipykernel_19480\\1747463221.py:8: FutureWarning:\n",
      "\n",
      "In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.77660263, 0.78156702, 0.79084826, 0.77055903, 0.7684006 ,\n",
       "         0.77422836],\n",
       "        [0.77466005, 0.78739478, 0.78717893, 0.75156486, 0.7638679 ,\n",
       "         0.77206993],\n",
       "        [0.77401252, 0.78804231, 0.79041658, 0.74465789, 0.76106195,\n",
       "         0.77012735],\n",
       "        ...,\n",
       "        [0.78005612, 0.7804878 , 0.78437298, 0.78135118, 0.78113533,\n",
       "         0.78027196],\n",
       "        [0.78027196, 0.78091949, 0.77962443, 0.78091949, 0.78027196,\n",
       "         0.78027196],\n",
       "        [0.78027196, 0.78070365, 0.78286208, 0.78156702, 0.78091949,\n",
       "         0.78005612]],\n",
       "\n",
       "       [[0.78027196, 0.77940859, 0.79905029, 0.78372545, 0.77962443,\n",
       "         0.78113533],\n",
       "        [0.78070365, 0.78113533, 0.79667602, 0.77919275, 0.78091949,\n",
       "         0.78070365],\n",
       "        [0.77984028, 0.77962443, 0.79710771, 0.78221455, 0.77832938,\n",
       "         0.78156702],\n",
       "        ...,\n",
       "        [0.78070365, 0.77876106, 0.79948198, 0.78350961, 0.77962443,\n",
       "         0.78113533],\n",
       "        [0.77962443, 0.78178286, 0.79516512, 0.77703432, 0.77725016,\n",
       "         0.78221455],\n",
       "        [0.77940859, 0.77854522, 0.79926613, 0.78415713, 0.77768185,\n",
       "         0.7804878 ]],\n",
       "\n",
       "       [[0.78502051, 0.77725016, 0.78264623, 0.78631556, 0.78696309,\n",
       "         0.7804878 ],\n",
       "        [0.78588388, 0.78005612, 0.78717893, 0.78243039, 0.78890568,\n",
       "         0.77984028],\n",
       "        [0.78653141, 0.77876106, 0.78027196, 0.78480466, 0.78933736,\n",
       "         0.78027196],\n",
       "        ...,\n",
       "        [0.78005612, 0.78135118, 0.77962443, 0.7804878 , 0.77962443,\n",
       "         0.77962443],\n",
       "        [0.78005612, 0.78091949, 0.7789769 , 0.78091949, 0.78027196,\n",
       "         0.77962443],\n",
       "        [0.7804878 , 0.78091949, 0.78005612, 0.78156702, 0.78070365,\n",
       "         0.77962443]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.78005612, 0.78070365, 0.79689186, 0.77919275, 0.77660263,\n",
       "         0.77984028],\n",
       "        [0.77962443, 0.77832938, 0.79883445, 0.78307792, 0.77617095,\n",
       "         0.77811353],\n",
       "        [0.78005612, 0.78264623, 0.79516512, 0.77487589, 0.77681848,\n",
       "         0.7789769 ],\n",
       "        ...,\n",
       "        [0.7804878 , 0.77832938, 0.79969782, 0.78307792, 0.777466  ,\n",
       "         0.77854522],\n",
       "        [0.77940859, 0.77984028, 0.79710771, 0.77962443, 0.77573926,\n",
       "         0.77984028],\n",
       "        [0.77940859, 0.78243039, 0.79710771, 0.77530758, 0.77617095,\n",
       "         0.77940859]],\n",
       "\n",
       "       [[0.77077488, 0.79494928, 0.77617095, 0.75588172, 0.77832938,\n",
       "         0.7804878 ],\n",
       "        [0.7653788 , 0.78135118, 0.78243039, 0.777466  , 0.7638679 ,\n",
       "         0.78027196],\n",
       "        [0.76365206, 0.78264623, 0.78350961, 0.78005612, 0.75674509,\n",
       "         0.78027196],\n",
       "        ...,\n",
       "        [0.77984028, 0.78113533, 0.78243039, 0.78113533, 0.78005612,\n",
       "         0.78005612],\n",
       "        [0.78027196, 0.78113533, 0.78350961, 0.78091949, 0.78070365,\n",
       "         0.78027196],\n",
       "        [0.7804878 , 0.78091949, 0.78480466, 0.78221455, 0.78156702,\n",
       "         0.78005612]],\n",
       "\n",
       "       [[0.77789769, 0.77789769, 0.7744442 , 0.78696309, 0.78653141,\n",
       "         0.77789769],\n",
       "        [0.77832938, 0.77811353, 0.77422836, 0.78631556, 0.78868983,\n",
       "         0.7789769 ],\n",
       "        [0.77832938, 0.77789769, 0.77466005, 0.78696309, 0.78804231,\n",
       "         0.77768185],\n",
       "        ...,\n",
       "        [0.67882581, 0.70472696, 0.09842435, 0.7759551 , 0.85732787,\n",
       "         0.74940643],\n",
       "        [0.69933089, 0.73796676, 0.34448521, 0.77099072, 0.81674941,\n",
       "         0.77530758],\n",
       "        [0.72199439, 0.73170732, 0.7155191 , 0.81804446, 0.69156054,\n",
       "         0.77012735]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1  2  3  4  5\n",
       "84   0  0  1  0  0\n",
       "2    1  0  0  0  0\n",
       "94   0  1  0  0  0\n",
       "45   0  0  1  0  0\n",
       "42   0  0  1  0  0\n",
       "..  .. .. .. .. ..\n",
       "71   0  0  1  0  0\n",
       "106  0  0  0  1  0\n",
       "14   1  0  0  0  0\n",
       "92   0  1  0  0  0\n",
       "102  0  0  0  0  1\n",
       "\n",
       "[131 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes_dict = {'normal':'1 0 0 0 0', #1\n",
    "        'collision_in_tool':'0 1 0 0 0', #2\n",
    "        'collision_in_part':'0 0 1 0 0', #3\n",
    "        'bottom_collision':'0 0 0 1 0', #4\n",
    "        'bottom_obstruction':'0 0 0 0 1'} #5\n",
    "\n",
    "labels_df = labels_df.replace({'labels':classes_dict})\n",
    "labels_df[['1', '2', '3', '4', '5']] = labels_df['labels'].str.split(' ', 4, expand= True)\n",
    "labels_df = labels_df.loc[:,['1', '2', '3', '4', '5']]\n",
    "labels_df[['1', '2', '3', '4', '5']]=labels_df[['1', '2', '3', '4', '5']].astype(str).astype(int)\n",
    "\n",
    "def norm(x):\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "    range = x_max - x_min  #min max entre 0 y 1\n",
    "    return((x-x_min)/(range))\n",
    "\n",
    "norm_features = norm(features)\n",
    "#split para entrenamiento y validacion\n",
    "train_features, test_features, train_labels, test_labels  = train_test_split(norm_features, labels_df, test_size=0.2, random_state= 42)\n",
    "display(train_features)\n",
    "display(train_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(learning_rate):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(8, (3,3), activation='relu', input_shape = (15,6,1), padding='same'), # #de feature maps, dimensiones del kernel, kernel entre más pequeño mejor y se prefiere un número impar\n",
    "        tf.keras.layers.Conv2D(8, (3,3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D((2,2), strides= 2, padding= 'valid'), # dimensiones del pooling\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Conv2D(16, (3,3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.Conv2D(16, (3,3), activation='relu', padding='same'),# #de feature maps, dimensiones del kernel, kernel entre más pequeño mejor y se prefiere un número impar\n",
    "        tf.keras.layers.MaxPooling2D((2,2), strides= 2, padding='valid'), # dimensiones del pooling\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(24, activation='relu'),\n",
    "        tf.keras.layers.Dense(5, activation='softmax')])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "# input shape es 15 filas, 6 columnas y 1 canal de color\n",
    "#model = model = my_model(0.01)\n",
    "#model.summary()\n",
    "\n",
    "# initialize tuner to run the model.\n",
    "# using the Hyperband search algorithm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, features, labels, epochs, batch_size):\n",
    "    history = model.fit(\n",
    "        x = features,\n",
    "        y = labels,\n",
    "        epochs= epochs,\n",
    "        batch_size= batch_size,\n",
    "        validation_split= 0.25\n",
    "    )\n",
    "    hist= pd.DataFrame(history.history) #se guardan los valores de errores y metricas en un diccionario\n",
    "    hist['epoch'] = history.epoch #los epochs se deben añadir aparte\n",
    "    return hist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizaciones\n",
    "### Pasos para visualizar los filtros:\n",
    "##### 1. Iterar por todas las capas del modelo usando model.layers\n",
    "##### 2. Si la capa actual es de convolucion se extraen los pesos y sesgos usando get_weights()\n",
    "##### 3. Se normalizan los pesos de los filtros entre 0 y 1\n",
    "##### 4. Se plotean los filtros para cada capa convolutional y todos los canales de color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_curves(history):\n",
    "    hist = history\n",
    "    labels = {\"loss\":\"Training Loss\", \"val_loss\":\"Validation Loss\"}\n",
    "    hist.rename(columns = labels, inplace = True)\n",
    "    \n",
    "    fig = px.line(hist, x='epoch', y=['Training Loss', 'Validation Loss'],\n",
    "                title='Gráficas de Pérdida de Entrenamiento y Evaluación',\n",
    "                labels={\"epoch\": \"Epoch\", \"value\":\"Binary Cross Entropy\", \"variable\":\"Curvas de Pérdida\"},\n",
    "                color_discrete_map={ # replaces default color mapping by value\n",
    "                \"Training Loss\": \"#46039f\", \"Validation Loss\": \"#fb9f3a\"})\n",
    "    fig.update_layout(template='plotly_white')\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se corren las funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dave4\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\backend.py:5703: UserWarning:\n",
      "\n",
      "\"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 1s 3ms/step - loss: 0.5529 - accuracy: 0.1429 - val_loss: 0.4953 - val_accuracy: 0.2727\n",
      "Epoch 2/250\n",
      "36/98 [==========>...................] - ETA: 0s - loss: 0.5043 - accuracy: 0.3333"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dave4\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\backend.py:5703: UserWarning:\n",
      "\n",
      "\"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 0s 2ms/step - loss: 0.5025 - accuracy: 0.2959 - val_loss: 0.4933 - val_accuracy: 0.2727\n",
      "Epoch 3/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.2755 - val_loss: 0.4966 - val_accuracy: 0.2727\n",
      "Epoch 4/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4979 - accuracy: 0.3163 - val_loss: 0.4927 - val_accuracy: 0.2727\n",
      "Epoch 5/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4938 - accuracy: 0.3163 - val_loss: 0.4943 - val_accuracy: 0.2727\n",
      "Epoch 6/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4922 - accuracy: 0.3163 - val_loss: 0.4942 - val_accuracy: 0.2727\n",
      "Epoch 7/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.3163 - val_loss: 0.4931 - val_accuracy: 0.2727\n",
      "Epoch 8/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.3163 - val_loss: 0.4943 - val_accuracy: 0.2727\n",
      "Epoch 9/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4923 - accuracy: 0.3163 - val_loss: 0.4927 - val_accuracy: 0.2727\n",
      "Epoch 10/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4945 - accuracy: 0.3163 - val_loss: 0.4924 - val_accuracy: 0.2727\n",
      "Epoch 11/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4935 - accuracy: 0.3163 - val_loss: 0.4952 - val_accuracy: 0.2727\n",
      "Epoch 12/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4908 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 13/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4907 - accuracy: 0.3163 - val_loss: 0.4933 - val_accuracy: 0.2727\n",
      "Epoch 14/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4909 - accuracy: 0.3163 - val_loss: 0.4912 - val_accuracy: 0.2727\n",
      "Epoch 15/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4912 - accuracy: 0.3163 - val_loss: 0.4904 - val_accuracy: 0.2727\n",
      "Epoch 16/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4899 - accuracy: 0.3163 - val_loss: 0.4920 - val_accuracy: 0.2727\n",
      "Epoch 17/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4911 - accuracy: 0.2449 - val_loss: 0.4937 - val_accuracy: 0.2727\n",
      "Epoch 18/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4909 - accuracy: 0.3163 - val_loss: 0.4940 - val_accuracy: 0.2727\n",
      "Epoch 19/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4908 - accuracy: 0.3163 - val_loss: 0.4948 - val_accuracy: 0.2727\n",
      "Epoch 20/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4909 - accuracy: 0.3163 - val_loss: 0.4953 - val_accuracy: 0.2727\n",
      "Epoch 21/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4894 - accuracy: 0.3163 - val_loss: 0.4931 - val_accuracy: 0.2727\n",
      "Epoch 22/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4895 - accuracy: 0.3163 - val_loss: 0.4901 - val_accuracy: 0.2727\n",
      "Epoch 23/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4919 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 24/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 25/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4901 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 26/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4917 - accuracy: 0.3163 - val_loss: 0.4952 - val_accuracy: 0.2727\n",
      "Epoch 27/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4907 - accuracy: 0.3163 - val_loss: 0.4949 - val_accuracy: 0.2727\n",
      "Epoch 28/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4901 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 29/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4898 - accuracy: 0.3163 - val_loss: 0.4907 - val_accuracy: 0.2727\n",
      "Epoch 30/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4903 - accuracy: 0.3163 - val_loss: 0.4933 - val_accuracy: 0.2727\n",
      "Epoch 31/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4901 - accuracy: 0.3163 - val_loss: 0.4945 - val_accuracy: 0.2727\n",
      "Epoch 32/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.3163 - val_loss: 0.4924 - val_accuracy: 0.2727\n",
      "Epoch 33/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.3163 - val_loss: 0.4939 - val_accuracy: 0.2727\n",
      "Epoch 34/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4904 - accuracy: 0.3163 - val_loss: 0.4941 - val_accuracy: 0.2727\n",
      "Epoch 35/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4897 - accuracy: 0.3163 - val_loss: 0.4913 - val_accuracy: 0.2727\n",
      "Epoch 36/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4903 - accuracy: 0.3163 - val_loss: 0.4908 - val_accuracy: 0.2727\n",
      "Epoch 37/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4902 - accuracy: 0.3163 - val_loss: 0.4932 - val_accuracy: 0.2727\n",
      "Epoch 38/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4896 - accuracy: 0.3163 - val_loss: 0.4915 - val_accuracy: 0.2727\n",
      "Epoch 39/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4893 - accuracy: 0.3163 - val_loss: 0.4920 - val_accuracy: 0.2727\n",
      "Epoch 40/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4902 - accuracy: 0.3163 - val_loss: 0.4911 - val_accuracy: 0.2727\n",
      "Epoch 41/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4896 - accuracy: 0.3163 - val_loss: 0.4913 - val_accuracy: 0.2727\n",
      "Epoch 42/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4900 - accuracy: 0.3163 - val_loss: 0.4940 - val_accuracy: 0.2727\n",
      "Epoch 43/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4900 - accuracy: 0.3163 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 44/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.3163 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 45/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 46/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4891 - accuracy: 0.3163 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 47/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4901 - accuracy: 0.3163 - val_loss: 0.4913 - val_accuracy: 0.2727\n",
      "Epoch 48/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4910 - accuracy: 0.3163 - val_loss: 0.4920 - val_accuracy: 0.2727\n",
      "Epoch 49/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4894 - accuracy: 0.3163 - val_loss: 0.4933 - val_accuracy: 0.2727\n",
      "Epoch 50/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4900 - accuracy: 0.3163 - val_loss: 0.4941 - val_accuracy: 0.2727\n",
      "Epoch 51/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 52/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4924 - val_accuracy: 0.2727\n",
      "Epoch 53/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.3163 - val_loss: 0.4927 - val_accuracy: 0.2727\n",
      "Epoch 54/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4902 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 55/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4893 - accuracy: 0.3163 - val_loss: 0.4934 - val_accuracy: 0.2727\n",
      "Epoch 56/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4900 - accuracy: 0.3163 - val_loss: 0.4927 - val_accuracy: 0.2727\n",
      "Epoch 57/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4888 - accuracy: 0.3163 - val_loss: 0.4917 - val_accuracy: 0.2727\n",
      "Epoch 58/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.3163 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 59/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4890 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 60/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.3163 - val_loss: 0.4942 - val_accuracy: 0.2727\n",
      "Epoch 61/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.3163 - val_loss: 0.4936 - val_accuracy: 0.2727\n",
      "Epoch 62/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4922 - val_accuracy: 0.2727\n",
      "Epoch 63/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4903 - accuracy: 0.3163 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 64/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4895 - accuracy: 0.3163 - val_loss: 0.4921 - val_accuracy: 0.2727\n",
      "Epoch 65/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4886 - accuracy: 0.3163 - val_loss: 0.4932 - val_accuracy: 0.2727\n",
      "Epoch 66/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4933 - val_accuracy: 0.2727\n",
      "Epoch 67/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.3163 - val_loss: 0.4927 - val_accuracy: 0.2727\n",
      "Epoch 68/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.3163 - val_loss: 0.4944 - val_accuracy: 0.2727\n",
      "Epoch 69/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4891 - accuracy: 0.3163 - val_loss: 0.4943 - val_accuracy: 0.2727\n",
      "Epoch 70/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.3163 - val_loss: 0.4938 - val_accuracy: 0.2727\n",
      "Epoch 71/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.3163 - val_loss: 0.4932 - val_accuracy: 0.2727\n",
      "Epoch 72/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4887 - accuracy: 0.3163 - val_loss: 0.4910 - val_accuracy: 0.2727\n",
      "Epoch 73/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.3163 - val_loss: 0.4915 - val_accuracy: 0.2727\n",
      "Epoch 74/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4886 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 75/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4886 - accuracy: 0.3163 - val_loss: 0.4917 - val_accuracy: 0.2727\n",
      "Epoch 76/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.3163 - val_loss: 0.4929 - val_accuracy: 0.2727\n",
      "Epoch 77/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 78/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4890 - accuracy: 0.3163 - val_loss: 0.4943 - val_accuracy: 0.2727\n",
      "Epoch 79/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4888 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 80/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4893 - accuracy: 0.3163 - val_loss: 0.4933 - val_accuracy: 0.2727\n",
      "Epoch 81/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 82/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4886 - accuracy: 0.3163 - val_loss: 0.4916 - val_accuracy: 0.2727\n",
      "Epoch 83/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4886 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 84/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 85/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.3163 - val_loss: 0.4921 - val_accuracy: 0.2727\n",
      "Epoch 86/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4927 - val_accuracy: 0.2727\n",
      "Epoch 87/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.3163 - val_loss: 0.4931 - val_accuracy: 0.2727\n",
      "Epoch 88/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 89/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4896 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 90/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.3163 - val_loss: 0.4924 - val_accuracy: 0.2727\n",
      "Epoch 91/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4886 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 92/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4888 - accuracy: 0.3163 - val_loss: 0.4941 - val_accuracy: 0.2727\n",
      "Epoch 93/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.3163 - val_loss: 0.4917 - val_accuracy: 0.2727\n",
      "Epoch 94/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.3163 - val_loss: 0.4929 - val_accuracy: 0.2727\n",
      "Epoch 95/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.3163 - val_loss: 0.4932 - val_accuracy: 0.2727\n",
      "Epoch 96/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4932 - val_accuracy: 0.2727\n",
      "Epoch 97/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 98/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.3163 - val_loss: 0.4932 - val_accuracy: 0.2727\n",
      "Epoch 99/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 100/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4924 - val_accuracy: 0.2727\n",
      "Epoch 101/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 102/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.3163 - val_loss: 0.4924 - val_accuracy: 0.2727\n",
      "Epoch 103/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 104/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.3163 - val_loss: 0.4920 - val_accuracy: 0.2727\n",
      "Epoch 105/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.3163 - val_loss: 0.4938 - val_accuracy: 0.2727\n",
      "Epoch 106/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4888 - accuracy: 0.3163 - val_loss: 0.4933 - val_accuracy: 0.2727\n",
      "Epoch 107/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.3163 - val_loss: 0.4932 - val_accuracy: 0.2727\n",
      "Epoch 108/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 109/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4872 - accuracy: 0.3163 - val_loss: 0.4927 - val_accuracy: 0.2727\n",
      "Epoch 110/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 111/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 112/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4887 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 113/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 114/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.3163 - val_loss: 0.4929 - val_accuracy: 0.2727\n",
      "Epoch 115/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4912 - val_accuracy: 0.2727\n",
      "Epoch 116/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 117/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.3163 - val_loss: 0.4903 - val_accuracy: 0.2727\n",
      "Epoch 118/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4927 - val_accuracy: 0.2727\n",
      "Epoch 119/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 120/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 121/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4888 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 122/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4901 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 123/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.3163 - val_loss: 0.4921 - val_accuracy: 0.2727\n",
      "Epoch 124/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.3163 - val_loss: 0.4929 - val_accuracy: 0.2727\n",
      "Epoch 125/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 126/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.3163 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 127/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4920 - val_accuracy: 0.2727\n",
      "Epoch 128/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 129/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4931 - val_accuracy: 0.2727\n",
      "Epoch 130/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4933 - val_accuracy: 0.2727\n",
      "Epoch 131/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 132/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 133/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4927 - val_accuracy: 0.2727\n",
      "Epoch 134/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4922 - val_accuracy: 0.2727\n",
      "Epoch 135/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 136/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.3163 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 137/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4890 - accuracy: 0.3163 - val_loss: 0.4921 - val_accuracy: 0.2727\n",
      "Epoch 138/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4910 - val_accuracy: 0.2727\n",
      "Epoch 139/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4875 - accuracy: 0.3163 - val_loss: 0.4922 - val_accuracy: 0.2727\n",
      "Epoch 140/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 141/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4931 - val_accuracy: 0.2727\n",
      "Epoch 142/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.3163 - val_loss: 0.4916 - val_accuracy: 0.2727\n",
      "Epoch 143/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 144/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.3163 - val_loss: 0.4927 - val_accuracy: 0.2727\n",
      "Epoch 145/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4933 - val_accuracy: 0.2727\n",
      "Epoch 146/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4912 - val_accuracy: 0.2727\n",
      "Epoch 147/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.3163 - val_loss: 0.4932 - val_accuracy: 0.2727\n",
      "Epoch 148/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.3163 - val_loss: 0.4929 - val_accuracy: 0.2727\n",
      "Epoch 149/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4875 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 150/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.3163 - val_loss: 0.4931 - val_accuracy: 0.2727\n",
      "Epoch 151/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 152/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 153/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.3163 - val_loss: 0.4914 - val_accuracy: 0.2727\n",
      "Epoch 154/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 155/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.3163 - val_loss: 0.4924 - val_accuracy: 0.2727\n",
      "Epoch 156/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 157/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 158/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4924 - val_accuracy: 0.2727\n",
      "Epoch 159/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.3163 - val_loss: 0.4933 - val_accuracy: 0.2727\n",
      "Epoch 160/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.3163 - val_loss: 0.4913 - val_accuracy: 0.2727\n",
      "Epoch 161/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.3163 - val_loss: 0.4921 - val_accuracy: 0.2727\n",
      "Epoch 162/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 163/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.3163 - val_loss: 0.4922 - val_accuracy: 0.2727\n",
      "Epoch 164/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 165/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4872 - accuracy: 0.3163 - val_loss: 0.4935 - val_accuracy: 0.2727\n",
      "Epoch 166/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.3163 - val_loss: 0.4932 - val_accuracy: 0.2727\n",
      "Epoch 167/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 168/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.3163 - val_loss: 0.4921 - val_accuracy: 0.2727\n",
      "Epoch 169/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4932 - val_accuracy: 0.2727\n",
      "Epoch 170/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4910 - val_accuracy: 0.2727\n",
      "Epoch 171/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 172/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.3163 - val_loss: 0.4924 - val_accuracy: 0.2727\n",
      "Epoch 173/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 174/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4913 - val_accuracy: 0.2727\n",
      "Epoch 175/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4917 - val_accuracy: 0.2727\n",
      "Epoch 176/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 177/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.3163 - val_loss: 0.4921 - val_accuracy: 0.2727\n",
      "Epoch 178/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 179/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4922 - val_accuracy: 0.2727\n",
      "Epoch 180/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.3163 - val_loss: 0.4931 - val_accuracy: 0.2727\n",
      "Epoch 181/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 182/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4871 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 183/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 184/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 185/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4929 - val_accuracy: 0.2727\n",
      "Epoch 186/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 187/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.3163 - val_loss: 0.4914 - val_accuracy: 0.2727\n",
      "Epoch 188/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.3163 - val_loss: 0.4920 - val_accuracy: 0.2727\n",
      "Epoch 189/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 190/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4875 - accuracy: 0.3163 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 191/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4872 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 192/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 193/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4871 - accuracy: 0.3163 - val_loss: 0.4934 - val_accuracy: 0.2727\n",
      "Epoch 194/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 195/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.3163 - val_loss: 0.4927 - val_accuracy: 0.2727\n",
      "Epoch 196/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4871 - accuracy: 0.3163 - val_loss: 0.4931 - val_accuracy: 0.2727\n",
      "Epoch 197/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 198/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4875 - accuracy: 0.3163 - val_loss: 0.4932 - val_accuracy: 0.2727\n",
      "Epoch 199/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 200/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.3163 - val_loss: 0.4929 - val_accuracy: 0.2727\n",
      "Epoch 201/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 202/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4875 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 203/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4875 - accuracy: 0.3163 - val_loss: 0.4935 - val_accuracy: 0.2727\n",
      "Epoch 204/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.3163 - val_loss: 0.4929 - val_accuracy: 0.2727\n",
      "Epoch 205/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 206/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.3163 - val_loss: 0.4933 - val_accuracy: 0.2727\n",
      "Epoch 207/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4924 - val_accuracy: 0.2727\n",
      "Epoch 208/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4924 - val_accuracy: 0.2727\n",
      "Epoch 209/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.3163 - val_loss: 0.4922 - val_accuracy: 0.2727\n",
      "Epoch 210/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 211/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 212/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4870 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 213/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.3163 - val_loss: 0.4923 - val_accuracy: 0.2727\n",
      "Epoch 214/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4920 - val_accuracy: 0.2727\n",
      "Epoch 215/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4921 - val_accuracy: 0.2727\n",
      "Epoch 216/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4914 - val_accuracy: 0.2727\n",
      "Epoch 217/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4927 - val_accuracy: 0.2727\n",
      "Epoch 218/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.3163 - val_loss: 0.4924 - val_accuracy: 0.2727\n",
      "Epoch 219/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4872 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 220/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 221/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4872 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 222/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.3163 - val_loss: 0.4916 - val_accuracy: 0.2727\n",
      "Epoch 223/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4922 - val_accuracy: 0.2727\n",
      "Epoch 224/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 225/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4872 - accuracy: 0.3163 - val_loss: 0.4922 - val_accuracy: 0.2727\n",
      "Epoch 226/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 227/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.3163 - val_loss: 0.4919 - val_accuracy: 0.2727\n",
      "Epoch 228/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4869 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 229/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 230/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4926 - val_accuracy: 0.2727\n",
      "Epoch 231/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4872 - accuracy: 0.3163 - val_loss: 0.4917 - val_accuracy: 0.2727\n",
      "Epoch 232/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.3163 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 233/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.3163 - val_loss: 0.4920 - val_accuracy: 0.2727\n",
      "Epoch 234/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.3163 - val_loss: 0.4927 - val_accuracy: 0.2727\n",
      "Epoch 235/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 236/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.3163 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 237/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.3163 - val_loss: 0.4930 - val_accuracy: 0.2727\n",
      "Epoch 238/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4875 - accuracy: 0.3163 - val_loss: 0.4932 - val_accuracy: 0.2727\n",
      "Epoch 239/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.3163 - val_loss: 0.4932 - val_accuracy: 0.2727\n",
      "Epoch 240/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.3163 - val_loss: 0.4922 - val_accuracy: 0.2727\n",
      "Epoch 241/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.3163 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 242/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.3163 - val_loss: 0.4928 - val_accuracy: 0.2727\n",
      "Epoch 243/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.3163 - val_loss: 0.4920 - val_accuracy: 0.2727\n",
      "Epoch 244/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.3163 - val_loss: 0.4920 - val_accuracy: 0.2727\n",
      "Epoch 245/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.3163 - val_loss: 0.4924 - val_accuracy: 0.2727\n",
      "Epoch 246/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4875 - accuracy: 0.3163 - val_loss: 0.4917 - val_accuracy: 0.2727\n",
      "Epoch 247/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 248/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4872 - accuracy: 0.3163 - val_loss: 0.4925 - val_accuracy: 0.2727\n",
      "Epoch 249/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4872 - accuracy: 0.3163 - val_loss: 0.4918 - val_accuracy: 0.2727\n",
      "Epoch 250/250\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 0.4875 - accuracy: 0.3163 - val_loss: 0.4920 - val_accuracy: 0.2727\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.553</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.502</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.501</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.273</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.498</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.273</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.494</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.273</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.487</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.273</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.487</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.273</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.487</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.273</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.487</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.273</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.487</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.273</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     loss  accuracy  val_loss  val_accuracy  epoch\n",
       "0   0.553     0.143     0.495         0.273      0\n",
       "1   0.502     0.296     0.493         0.273      1\n",
       "2   0.501     0.276     0.497         0.273      2\n",
       "3   0.498     0.316     0.493         0.273      3\n",
       "4   0.494     0.316     0.494         0.273      4\n",
       "..    ...       ...       ...           ...    ...\n",
       "245 0.487     0.316     0.492         0.273    245\n",
       "246 0.487     0.316     0.492         0.273    246\n",
       "247 0.487     0.316     0.492         0.273    247\n",
       "248 0.487     0.316     0.492         0.273    248\n",
       "249 0.487     0.316     0.492         0.273    249\n",
       "\n",
       "[250 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Curvas de Pérdida=Training Loss<br>Epoch=%{x}<br>Binary Cross Entropy=%{y}<extra></extra>",
         "legendgroup": "Training Loss",
         "line": {
          "color": "#46039f",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Training Loss",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249
         ],
         "xaxis": "x",
         "y": [
          0.552908718585968,
          0.5024504661560059,
          0.501410722732544,
          0.49794650077819824,
          0.49383237957954407,
          0.4921668767929077,
          0.49312588572502136,
          0.48886197805404663,
          0.49229785799980164,
          0.4945206046104431,
          0.4935207664966583,
          0.49083250761032104,
          0.49066704511642456,
          0.490936815738678,
          0.4912031590938568,
          0.4898938834667206,
          0.4911012351512909,
          0.4909103214740753,
          0.49079814553260803,
          0.49092230200767517,
          0.4894385039806366,
          0.4895266592502594,
          0.49190738797187805,
          0.4892164468765259,
          0.4901004731655121,
          0.491693377494812,
          0.49065259099006653,
          0.4900856018066406,
          0.48980361223220825,
          0.4903204143047333,
          0.4901283085346222,
          0.48891210556030273,
          0.4889031946659088,
          0.4904220998287201,
          0.48965898156166077,
          0.4902719259262085,
          0.4902343451976776,
          0.48963212966918945,
          0.48930278420448303,
          0.49015605449676514,
          0.48960965871810913,
          0.49001404643058777,
          0.48998796939849854,
          0.4885312616825104,
          0.4891703426837921,
          0.4890982210636139,
          0.4901450276374817,
          0.49099019169807434,
          0.48942360281944275,
          0.4900333881378174,
          0.4892182946205139,
          0.4881851375102997,
          0.4889451563358307,
          0.49021199345588684,
          0.48934388160705566,
          0.49002721905708313,
          0.48878419399261475,
          0.48919475078582764,
          0.4889698326587677,
          0.48772937059402466,
          0.4885413646697998,
          0.4879687428474426,
          0.49030646681785583,
          0.48951610922813416,
          0.48855388164520264,
          0.48796918988227844,
          0.4885198175907135,
          0.4892016351222992,
          0.48906412720680237,
          0.4889346659183502,
          0.4884614646434784,
          0.4886731207370758,
          0.48835739493370056,
          0.4886341094970703,
          0.48864415287971497,
          0.4889126121997833,
          0.4882412850856781,
          0.48897072672843933,
          0.48881611227989197,
          0.4893335998058319,
          0.4878029227256775,
          0.48855578899383545,
          0.48862096667289734,
          0.48854443430900574,
          0.48844900727272034,
          0.48824816942214966,
          0.4883979558944702,
          0.4877171814441681,
          0.48958972096443176,
          0.4891836643218994,
          0.488557904958725,
          0.4887610971927643,
          0.4884600043296814,
          0.48850443959236145,
          0.48829221725463867,
          0.48781469464302063,
          0.4880881905555725,
          0.48885855078697205,
          0.48806026577949524,
          0.48823824524879456,
          0.4885294437408447,
          0.48846742510795593,
          0.488450288772583,
          0.4883582890033722,
          0.4885326027870178,
          0.48880231380462646,
          0.4878908395767212,
          0.488262414932251,
          0.48723021149635315,
          0.4891708195209503,
          0.48811566829681396,
          0.4887310862541199,
          0.4877810478210449,
          0.4885433614253998,
          0.48775848746299744,
          0.4884638488292694,
          0.48787665367126465,
          0.48778510093688965,
          0.48824846744537354,
          0.48823875188827515,
          0.48881399631500244,
          0.49009034037590027,
          0.4880810081958771,
          0.4881390631198883,
          0.48797789216041565,
          0.48923259973526,
          0.4877718389034271,
          0.48838236927986145,
          0.48824796080589294,
          0.4878140985965729,
          0.4877842962741852,
          0.4889279901981354,
          0.48820599913597107,
          0.48802539706230164,
          0.4879440367221832,
          0.4881252646446228,
          0.4889817535877228,
          0.4880393147468567,
          0.4874556064605713,
          0.4873705804347992,
          0.48796382546424866,
          0.4884236752986908,
          0.4882122278213501,
          0.4885064363479614,
          0.4879773259162903,
          0.48822125792503357,
          0.4879082143306732,
          0.4883442223072052,
          0.4875437617301941,
          0.48834696412086487,
          0.4877634644508362,
          0.4880232810974121,
          0.4880802631378174,
          0.4882644712924957,
          0.4884655177593231,
          0.4878353178501129,
          0.4879438579082489,
          0.487985759973526,
          0.48770520091056824,
          0.48787662386894226,
          0.4874216318130493,
          0.48733824491500854,
          0.4883928596973419,
          0.48816409707069397,
          0.48718270659446716,
          0.48770463466644287,
          0.48775261640548706,
          0.4877464175224304,
          0.4877508878707886,
          0.488007128238678,
          0.48808541893959045,
          0.4877495765686035,
          0.4880470633506775,
          0.48764681816101074,
          0.4878373444080353,
          0.48762673139572144,
          0.4883268177509308,
          0.4872879087924957,
          0.4877956807613373,
          0.48773127794265747,
          0.4879668354988098,
          0.4870988130569458,
          0.4877667725086212,
          0.4885048270225525,
          0.4879572093486786,
          0.48799824714660645,
          0.48774439096450806,
          0.4880712330341339,
          0.48768097162246704,
          0.4874766170978546,
          0.4872273802757263,
          0.48732665181159973,
          0.4870785176753998,
          0.48755162954330444,
          0.48742979764938354,
          0.487125426530838,
          0.4876247048377991,
          0.487496942281723,
          0.4876088500022888,
          0.48728764057159424,
          0.48735058307647705,
          0.48747751116752625,
          0.48750773072242737,
          0.48742297291755676,
          0.4872879981994629,
          0.48732417821884155,
          0.4879797697067261,
          0.48760321736335754,
          0.4872853755950928,
          0.48738643527030945,
          0.487377405166626,
          0.48703041672706604,
          0.4873693287372589,
          0.48762306571006775,
          0.4880122244358063,
          0.4879928529262543,
          0.488031804561615,
          0.4873659908771515,
          0.48720625042915344,
          0.48788005113601685,
          0.48720216751098633,
          0.48767510056495667,
          0.4876410961151123,
          0.48763594031333923,
          0.48717254400253296,
          0.4878506660461426,
          0.48770958185195923,
          0.4869106709957123,
          0.4876382052898407,
          0.48758623003959656,
          0.4872302711009979,
          0.48744839429855347,
          0.48796436190605164,
          0.4872596263885498,
          0.487560898065567,
          0.48742419481277466,
          0.48735761642456055,
          0.48752081394195557,
          0.4872869849205017,
          0.4873826801776886,
          0.48819413781166077,
          0.4873282313346863,
          0.4873376488685608,
          0.48726463317871094,
          0.4875909090042114,
          0.48748382925987244,
          0.4874245524406433,
          0.48724034428596497,
          0.48716050386428833,
          0.4874797463417053
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Curvas de Pérdida=Validation Loss<br>Epoch=%{x}<br>Binary Cross Entropy=%{y}<extra></extra>",
         "legendgroup": "Validation Loss",
         "line": {
          "color": "#fb9f3a",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Validation Loss",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249
         ],
         "xaxis": "x",
         "y": [
          0.4952951967716217,
          0.4933135211467743,
          0.49659156799316406,
          0.49267399311065674,
          0.4943469762802124,
          0.4941781461238861,
          0.4930518865585327,
          0.494313508272171,
          0.4926668107509613,
          0.4923591613769531,
          0.4951598346233368,
          0.49255284667015076,
          0.49328354001045227,
          0.49119076132774353,
          0.49038732051849365,
          0.49199724197387695,
          0.49366047978401184,
          0.49395427107810974,
          0.4948221743106842,
          0.49529537558555603,
          0.49309736490249634,
          0.4900997579097748,
          0.49192944169044495,
          0.49301689863204956,
          0.49295565485954285,
          0.495157927274704,
          0.4948963522911072,
          0.49249041080474854,
          0.4906696677207947,
          0.493259072303772,
          0.49450168013572693,
          0.4924461543560028,
          0.49387919902801514,
          0.49413883686065674,
          0.49133577942848206,
          0.49083825945854187,
          0.49324920773506165,
          0.49148109555244446,
          0.4920456111431122,
          0.49110546708106995,
          0.4913267195224762,
          0.49404266476631165,
          0.4928424656391144,
          0.49283358454704285,
          0.49229222536087036,
          0.4927550256252289,
          0.49126964807510376,
          0.4920201301574707,
          0.4932701289653778,
          0.4941222369670868,
          0.4929847717285156,
          0.492438405752182,
          0.4926624894142151,
          0.4926004707813263,
          0.49341022968292236,
          0.4927027225494385,
          0.49166372418403625,
          0.4927656054496765,
          0.49246886372566223,
          0.4941806197166443,
          0.49356159567832947,
          0.492179811000824,
          0.49276110529899597,
          0.49207502603530884,
          0.49324601888656616,
          0.4933018684387207,
          0.49273768067359924,
          0.4943743646144867,
          0.49432072043418884,
          0.4938417971134186,
          0.49320048093795776,
          0.49097874760627747,
          0.4914698302745819,
          0.4929564595222473,
          0.4916945993900299,
          0.4929123520851135,
          0.49187907576560974,
          0.4942935109138489,
          0.49183717370033264,
          0.4933317303657532,
          0.4930337965488434,
          0.4915977716445923,
          0.4922533333301544,
          0.49304279685020447,
          0.4921301007270813,
          0.49267566204071045,
          0.4931308925151825,
          0.49246886372566223,
          0.49233701825141907,
          0.4923523962497711,
          0.4929628074169159,
          0.4940876364707947,
          0.4916578233242035,
          0.49294355511665344,
          0.4931655526161194,
          0.4931860864162445,
          0.4929695725440979,
          0.49321088194847107,
          0.4926290214061737,
          0.49236610531806946,
          0.49175581336021423,
          0.4924180507659912,
          0.4930103123188019,
          0.4920049011707306,
          0.4937755763530731,
          0.49330103397369385,
          0.49321943521499634,
          0.492588609457016,
          0.4927489459514618,
          0.4918329119682312,
          0.4917709529399872,
          0.4925706386566162,
          0.4929635524749756,
          0.49285200238227844,
          0.49118539690971375,
          0.4919256269931793,
          0.4903251826763153,
          0.49266502261161804,
          0.49258318543434143,
          0.491865336894989,
          0.4925152063369751,
          0.4918915629386902,
          0.4920630156993866,
          0.4929201006889343,
          0.49231886863708496,
          0.49276304244995117,
          0.4919659495353699,
          0.4925468862056732,
          0.4930686950683594,
          0.4933410882949829,
          0.49284738302230835,
          0.4918319284915924,
          0.49273398518562317,
          0.4921609163284302,
          0.49185439944267273,
          0.4927666485309601,
          0.4920734167098999,
          0.49095216393470764,
          0.49219322204589844,
          0.4917573034763336,
          0.49310049414634705,
          0.4916228652000427,
          0.4918113350868225,
          0.49266883730888367,
          0.49328115582466125,
          0.49121665954589844,
          0.49324849247932434,
          0.49289143085479736,
          0.4918230175971985,
          0.49312683939933777,
          0.4924657344818115,
          0.4927521347999573,
          0.491351842880249,
          0.4919462502002716,
          0.49238696694374084,
          0.4925781488418579,
          0.49177902936935425,
          0.4924144744873047,
          0.49332723021507263,
          0.491345077753067,
          0.4921185374259949,
          0.49261221289634705,
          0.49217489361763,
          0.49187740683555603,
          0.4934673309326172,
          0.4931994378566742,
          0.4929971992969513,
          0.49211427569389343,
          0.493152379989624,
          0.49104055762290955,
          0.49297744035720825,
          0.49235373735427856,
          0.4925304651260376,
          0.4913117289543152,
          0.49170586466789246,
          0.4922672510147095,
          0.4920893609523773,
          0.49264734983444214,
          0.49219807982444763,
          0.49314042925834656,
          0.49259307980537415,
          0.49233725666999817,
          0.4925442934036255,
          0.49254804849624634,
          0.4929141402244568,
          0.4925905168056488,
          0.4914105236530304,
          0.4920368790626526,
          0.4922764003276825,
          0.4928325414657593,
          0.4918747544288635,
          0.492611289024353,
          0.49342283606529236,
          0.4922599196434021,
          0.4926642179489136,
          0.49313291907310486,
          0.49180129170417786,
          0.49320822954177856,
          0.49227750301361084,
          0.49288928508758545,
          0.4930080771446228,
          0.49248579144477844,
          0.4935051202774048,
          0.4929465651512146,
          0.4917553961277008,
          0.4933432936668396,
          0.49243390560150146,
          0.4923847019672394,
          0.4921903908252716,
          0.49246540665626526,
          0.4929963946342468,
          0.49178677797317505,
          0.492343544960022,
          0.49200138449668884,
          0.49210596084594727,
          0.49137064814567566,
          0.4926862418651581,
          0.4924376606941223,
          0.49247342348098755,
          0.49259302020072937,
          0.4918065071105957,
          0.49162355065345764,
          0.49224621057510376,
          0.49254730343818665,
          0.4921751916408539,
          0.49262821674346924,
          0.4919450283050537,
          0.49177995324134827,
          0.4925171136856079,
          0.49257180094718933,
          0.49165499210357666,
          0.49284419417381287,
          0.4919665455818176,
          0.4926702380180359,
          0.49254971742630005,
          0.49279871582984924,
          0.49295946955680847,
          0.4932355582714081,
          0.49318552017211914,
          0.49223634600639343,
          0.49276119470596313,
          0.4927787780761719,
          0.4919542968273163,
          0.4920267164707184,
          0.4924302101135254,
          0.49165067076683044,
          0.4918295443058014,
          0.49247026443481445,
          0.4917791485786438,
          0.4920175075531006
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "Curvas de Pérdida"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Gráficas de Pérdida de Entrenamiento y Evaluación"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Binary Cross Entropy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.553</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.502</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.501</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.273</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.498</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.273</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.494</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.273</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.487</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.273</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.487</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.273</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.487</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.273</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.487</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.273</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.487</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.273</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Training Loss  accuracy  Validation Loss  val_accuracy  epoch\n",
       "0            0.553     0.143            0.495         0.273      0\n",
       "1            0.502     0.296            0.493         0.273      1\n",
       "2            0.501     0.276            0.497         0.273      2\n",
       "3            0.498     0.316            0.493         0.273      3\n",
       "4            0.494     0.316            0.494         0.273      4\n",
       "..             ...       ...              ...           ...    ...\n",
       "245          0.487     0.316            0.492         0.273    245\n",
       "246          0.487     0.316            0.492         0.273    246\n",
       "247          0.487     0.316            0.492         0.273    247\n",
       "248          0.487     0.316            0.492         0.273    248\n",
       "249          0.487     0.316            0.492         0.273    249\n",
       "\n",
       "[250 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 250\n",
    "batch_size = 1\n",
    "model = my_model(learning_rate)\n",
    "history= train_model(model, train_features, train_labels, epochs, batch_size)\n",
    "display(history)\n",
    "loss_curves(history)\n",
    "#view_filters(model)\n",
    "display(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 0s/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normal</th>\n",
       "      <th>collision_in_tool</th>\n",
       "      <th>collision_in_part</th>\n",
       "      <th>bottom_collision</th>\n",
       "      <th>bottom_obstruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.239</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.239</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.239</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.239</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.239</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.239</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.239</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.239</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.239</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.239</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    normal  collision_in_tool  collision_in_part  bottom_collision  \\\n",
       "0    0.239              0.129              0.358             0.133   \n",
       "1    0.239              0.129              0.358             0.133   \n",
       "2    0.239              0.129              0.358             0.133   \n",
       "3    0.239              0.129              0.358             0.133   \n",
       "4    0.239              0.129              0.358             0.133   \n",
       "..     ...                ...                ...               ...   \n",
       "28   0.239              0.129              0.358             0.133   \n",
       "29   0.239              0.129              0.358             0.133   \n",
       "30   0.239              0.129              0.358             0.133   \n",
       "31   0.239              0.129              0.358             0.133   \n",
       "32   0.239              0.129              0.358             0.133   \n",
       "\n",
       "    bottom_obstruction  \n",
       "0                0.141  \n",
       "1                0.141  \n",
       "2                0.141  \n",
       "3                0.141  \n",
       "4                0.141  \n",
       "..                 ...  \n",
       "28               0.141  \n",
       "29               0.141  \n",
       "30               0.141  \n",
       "31               0.141  \n",
       "32               0.141  \n",
       "\n",
       "[33 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1  2  3  4  5\n",
       "135  0  0  0  1  0\n",
       "115  0  0  0  0  1\n",
       "131  0  0  0  0  1\n",
       "55   1  0  0  0  0\n",
       "95   0  1  0  0  0\n",
       "..  .. .. .. .. ..\n",
       "134  0  0  0  1  0\n",
       "160  0  1  0  0  0\n",
       "139  0  0  0  1  0\n",
       "78   0  0  1  0  0\n",
       "60   1  0  0  0  0\n",
       "\n",
       "[33 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = model.predict(test_features)\n",
    "predictions_df = pd.DataFrame(predictions, columns=['normal', 'collision_in_tool', 'collision_in_part', 'bottom_collision', 'bottom_obstruction'])\n",
    "display(predictions_df)\n",
    "display(test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
